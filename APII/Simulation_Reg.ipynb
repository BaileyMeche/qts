{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8608e41a",
   "metadata": {},
   "source": [
    "**Work completed:** Implemented missing functions in `Reg.py` and `auxiliary_func.py` (proximal-gradient solvers, Huber gradient/loss, PCR/PLS, OLS/OLSH, PCAR/PLSR, oracle regression, and VIP utilities)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Set up environment\n",
    "\n",
    "Run the following import statements and ensure there are no errors before continuing.\n",
    "If you are missing packages, use `! pip install %` to install package `%` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install scipy\n",
    "# !pip install sklearn\n",
    "# !pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate firm-characteristics and returns \n",
    "Run the following cell to generate the dataset. After running it, you should see a new folder called `Simu` and one subfolder called `SimuData_` plus **a data tag**, i.e. `SimuData_100`. In this folder, you will get the charactistics and returns of the linear and nonlinear models. \n",
    "\n",
    "Remember you don't need to change anything in `DGP.py`, but make sure to read through it and understand what each csv file represents. You should change the data number parameter *P* and horizon parameter *hh* which allows you to have different return resolutions ie monthly, quarterly, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DGP import dgp\n",
    "P     =   100\n",
    "hh    =   [1,3]\n",
    "dgp(P,hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Preparation functions\n",
    "In this section, you will first implement some preparation functions for creating the models. For example, to update the coefficient matrix, we should do some gradient descent on the original matrix, so we introduce Accelarated Proximal Gradient Algorithm to update the matrix. \n",
    "\n",
    "This section is cited of paper [Empirical Asset Pricing via Machine Learning](https://dachxiu.chicagobooth.edu/download/ML.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open the file `auxiliary_func.py`. In this file, we have already provided several functions that will be used later. Read through the functions: `fw2()`, `fw1()` and `sq()`. Run the next cell to show what each function really does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliary_func import fw2,fw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1,3,2],[9,3,6],[2,4,6]])\n",
    "index = fw2(x)\n",
    "print(index)      # you should get the result of [1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,7,4,3,5])\n",
    "index = fw1(x)\n",
    "print(index)    # you should get the reuslt of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `sq(a,b,step)` should have the same functionality as `np.arange(a,b+1e-10,step)`. Note that we add a small value to b since np.arange() will not include the end in the array, but our function should include it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliary_func import sq\n",
    "a = 1.0\n",
    "b = 5.0\n",
    "step = 1\n",
    "x = sq(a,b,step)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized linear\n",
    "Linear models are popular in practice, however, the “true” model is sometimes complex and nonlinear, so if we restrict the functional form to be linear it may introduce approximation error due to model misspecification. Thus, we introduce a nonparametric approach: the generalized linear model. You can find more details in Section 1.5 in paper [Empirical Asset Pricing via Machine Learning](https://dachxiu.chicagobooth.edu/download/ML.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the generalized linear model, we implement a vector of basic functions. To simplify the function, we first compute these basic functions. Open the file `Reg.py`.  Follow the function description in the paper to implement the function `cut_knots_degree2(x,n,th)` and test your code. You should get error of magnitude `e-4` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import cut_knots_degree2\n",
    "N = 3\n",
    "n = 4\n",
    "T = 2\n",
    "\n",
    "x      = np.linspace(-0.2, 0.4, num=N*T).reshape(N, T)\n",
    "th     = np.linspace(0.3,1.6,num=n*T).reshape(n,T)\n",
    "result = cut_knots_degree2(x,n,th)\n",
    "expect_result = np.array([\n",
    "    [-0.24,0.144,0,0,0,-0.24,0.175542857,0,0,0],\n",
    "    [0,-0.0384,0,0,0,5.55111512e-17,-0.0384,0,0,0],\n",
    "    [0.24,-0.1056,0,0,0,0.24,-0.13712857,0,0,0]\n",
    "])\n",
    "print(rel_error(result,expect_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square \n",
    "We begin our model practice with the least complex method, the simple linear predictive regression model estimated via ordinary least squares (OLS). Although we expect this to perform poorly in our high dimensional problem, we use it as a benchmark to emphasize the efficacy of more complex models.\n",
    "Our baseline estimation of the simple linear model uses a standard least squares, or “*l2*” objective function:\n",
    "\n",
    "$$L(\\theta) = \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T}\n",
    "(r_{i,t+1}-g(z_{i,t};\\theta))^{2}$$\n",
    "\n",
    "Open file `Reg.py` and implement the function `loss(y,yhat)`. Test your code here and you should see errors of magnitude `e-9` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import loss\n",
    "y           = np.linspace(-0.2, 0.4, num = N*T)\n",
    "yhat        = np.linspace(-0.1,0.5, num = N*T)\n",
    "diff        = loss(y,yhat)\n",
    "expect_diff = 0.0099999999\n",
    "print(rel_error(diff,expect_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Square and Huber Function\n",
    "Heavy tails are a well-known attribute of financial returns and stock-level predictor variables. Convexity of the least squares loss places extreme emphasis on large errors, thus outliers can undermine the stablity of OLS-based prediction.\n",
    "\n",
    "In the machine learning literature, a common choice for counteracting the deleterious effect of heavy-tailed observations is the Huber robust objective function:\n",
    "$$L_{H}(\\theta)=\\frac{1}{NT}\\sum_{i=1}^{N}\\sum_{t=1}^{T}\n",
    "H\\left(r_{i,t+1}-g(z_{i,t};\\theta),\\xi\\right)$$\n",
    "\n",
    "\n",
    "$$H(x;\\xi)=\n",
    "\\begin{cases}x^2, &if\\quad|x|\\leq\\xi, \\\\\n",
    "2\\xi|x|-\\xi^2, &if\\quad|x|>\\xi\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Open file `Reg.py` and implement the function `lossh(y,yhat,mu)`, test your code here and you should see errors of magnitude`e-16` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import lossh\n",
    "y    = np.linspace(-0.2, 0.4, num = N*T)\n",
    "yhat = np.linspace(-0.1,0.5, num = N*T)\n",
    "mu   = 0.001\n",
    "diff = lossh(y,yhat,mu)\n",
    "expect_diff = 0.000199\n",
    "print(rel_error(diff,expect_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension**: Robust objective functions. In some cases, replacing oridinary squared loss equation with a weighted least squares objective, such as\n",
    "\n",
    "$$\\mathcal{L}_{\\omega}(\\theta) = \\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T}\n",
    "\\omega_{i,t} \\; (r_{i,t+1}\\;) - g(z_{i,t};\\theta))^{2}$$\n",
    "\n",
    "can possibly improve the predictive performance. This weighted least squares\n",
    "objective allows the econometrician to tilt estimates toward observations that\n",
    "are more statistically or economically informative.\n",
    "\n",
    "Open the file `Reg.py` and implement the functions `lossw()` and `losshw()`. These two functions should be very similar to `loss()` and `lossh()`. Test your code here, you should get errors of magnitude `e-10` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import lossw,losshw\n",
    "np.random.seed(1)\n",
    "y    = np.random.randn(10)\n",
    "yhat = np.random.randn(10)\n",
    "w    = np.random.randn(10)\n",
    "mu   = 0.1\n",
    "lw   = lossw(y,yhat,w)\n",
    "expect_lw  = 1.85130397827934\n",
    "lhw  = losshw(y,yhat,w,mu)\n",
    "expect_lhw = 0.32452079903016\n",
    "print(rel_error(lw,expect_lw))\n",
    "print(rel_error(lhw,expect_lhw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still in `Reg.py`, implement the function `f_grad()` and `f_gradh()`, test your code here. You should see errors on the order of `e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import f_grad\n",
    "np.random.seed(1)\n",
    "X    = np.random.randn(3,5)\n",
    "Y    = np.random.randn(3,1)\n",
    "w    = np.random.randn(5,1)\n",
    "pred = f_grad(X,Y,w)\n",
    "expect_pred = np.array([[0.94522685],[2.07035499],[0.67631656]])\n",
    "print(rel_error(pred,expect_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import f_gradh\n",
    "mu          = 1.0\n",
    "w_grad      = f_gradh(w,X,Y,mu)\n",
    "expect_grad = np.array([[0.22268398],[-0.22674411],[-1.47850512],[-0.95490198],[1.33542321]])\n",
    "print(rel_error(w_grad,expect_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the file `Reg.py` and read through the following functions: `soft_thresholdl()`, `soft_thresholdr()`, `soft_thresholde()`, `soft_thresholda()`, `soft_thresholdg()`. These functions are differet kinds of soft thresold functions for various models. Make sure you understand what each function really does, keeping in mind we'll use them later. You can also test their outputs in the following cells.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import soft_thresholdl,soft_thresholdr,soft_thresholde,soft_thresholda,soft_thresholdg\n",
    "np.random.seed(3)\n",
    "w  = np.random.randn(8,1) * 5\n",
    "mu = 5\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Soft_thresholdl:\")\n",
    "print(soft_thresholdl(w,mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Soft_thresholdr:\")\n",
    "print(soft_thresholdr(w,mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Soft_thresholde:\")\n",
    "print(soft_thresholde(w,mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "print(\"Soft_thresholda:\")\n",
    "print(soft_thresholda(w,alpha,mu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Soft_thresholdg:\")\n",
    "print(soft_thresholdg(w,mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelarated Proximal Gradient Algorithm\n",
    "\n",
    "Open the file `Reg.py` and implement the function `proximal()` and `proximalH()` which will output the coefficient matrix for the simple linear regression and the regression with huber function separately. You should see an error of magnitude `e-6` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import proximal,soft_thresholdl\n",
    "np.random.seed(2)\n",
    "XX  = np.random.randn(6,6) * 0.01\n",
    "XY  = np.random.randn(6,1) * 0.01\n",
    "tol = 1e-5\n",
    "L   = 10000\n",
    "l1  = 1e-4\n",
    "w   = proximal(XX,XY,tol,L,l1,soft_thresholdl)\n",
    "expect_w = np.array([[0.02564119,-0.0468463,-0.00101769,\n",
    "                      0.02964973,-0.01591977,0.04160531]])\n",
    "print(rel_error(w,expect_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import proximalH,soft_thresholdr\n",
    "np.random.seed(3)\n",
    "X   = np.random.randn(6,5) * 0.01\n",
    "y   = np.random.randn(6,1) * 0.01\n",
    "w   = np.random.randn(5) * 0.01\n",
    "tol = 1e-5\n",
    "L   = 10000\n",
    "l1  = 1e-4\n",
    "mu  = 0.001\n",
    "wh  = proximalH(w,X,y,mu,tol,L,l1,soft_thresholdr)\n",
    "expect_wh = np.array([-0.00923792,-0.01023876,0.01123978,-0.00131915,-0.01623285])\n",
    "\n",
    "print(rel_error(wh,expect_wh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCR and PLS\n",
    "Penalized linear models use shrinkage and variable selection to manage high dimensionality by forcing the coefficients on most regressors near or exactly to zero. This can produce suboptimal forecasts when predictors are highly correlated.\n",
    "\n",
    "The idea of predictor averaging, as opposed to predictor selection, is the essence of dimension reduction. Forming linear combinations of predictors helps reduce noise to better isolate the signal in predictors and helps decorrelate otherwise highly dependent predictors. Two classic dimension reduction techniques are principal components regression (PCR) and partial least squares (PLS).\n",
    "\n",
    "PCR consists of a two-step procedure. In the first step, principal components analysis (PCA) combines regressors into a small set of linear combinations that best preserve the covariance structure among the predictors. In the second step, a few leading components are used in standard predictive regression. That is, PCR regularizes the prediction problem by zeroing out coefficients on low variance components.\n",
    "\n",
    "Partial Least Squares performs dimension reduction by directly exploiting covariation of predictors with the forecast target. PLS regression proceeds as follows. For each predictor $j$ , estimate its univariate return prediction coefficient via OLS. This coefficient, denoted $\\varphi_{j}$ , reflects the “partial” sensitivity of returns to each predictor $j$ . Next, average all predictors into a single aggregate component with weights proportional to $\\varphi_{j}$ , placing the highest weight on the strongest univariate predictors, and the least weight on the weakest. In this way, PLS performs its dimension reduction with the ultimate forecasting objective in mind. To form more than one predictive component, the target and all predictors are orthogonalized with respect to previously constructed components, and the above procedure is repeated on the orthogonalized data set. This is iterated until the desired number of PLS components is reached.\n",
    "\n",
    "You can find more details in Section 1.4 in paper [Empirical Asset Pricing via Machine Learning](https://dachxiu.chicagobooth.edu/download/ML.pdf).\n",
    "\n",
    "Open the file `Reg.py` and implement the functions `PCR()` and `PLS()`. Check your code here, you should get an error of magnitude `e-8` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import PCR\n",
    "N = 6\n",
    "p = 4\n",
    "A = 3\n",
    "np.random.seed(3)\n",
    "X = np.random.randn(N,p)\n",
    "y = np.random.randn(N,1)\n",
    "B = PCR(X,y,A)\n",
    "expect_B = np.array([[0,0.5751487,0.46676701],\n",
    "                    [0,-0.28096611,-0.36453058],\n",
    "                    [0,0.37704483,0.40708078],\n",
    "                    [0,0.44675735,0.50838344]])\n",
    "print(rel_error(B,expect_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import pls\n",
    "B_pls        = pls(X,y,A)\n",
    "expect_B_pls = np.array([[0,-0.12371227,-0.07619591],\n",
    "                        [0,0.05231579,0.09294739],\n",
    "                        [0,-0.21909422,-0.22530672],\n",
    "                        [0,0.41948184,0.43975808]])\n",
    "print(rel_error(B_pls,expect_B_pls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance\n",
    "After we generate the coefficient matrix, we want to know which coefficients are important, meaning the coefficient has a strong importance on return prediction. We define importance as the difference of in-sample $R^{2}$. Open `auxiliary_func.py` and implement function `vip()`, test your code here. You should get an error of magnitude `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from auxiliary_func import vip\n",
    "np.random.seed(4)\n",
    "N = 10\n",
    "p = 8\n",
    "x = np.random.randn(N,p) * 0.01\n",
    "y = np.random.randn(N,1) * 0.01\n",
    "m = np.mean(y) \n",
    "y = y - m    # demeaned\n",
    "b = np.random.randn(p,1)\n",
    "v = vip(b,x,y,m)\n",
    "expect_v = np.array([[0.00602217,0.04295142,-0.1589286,-0.01933711, \n",
    "                      -0.94963987,0.3282685,-0.13133609,-0.09904148]])\n",
    "print(rel_error(v,expect_v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simulation Regression Models\n",
    "We follow the most common approach in the literature and select tuning parameters adaptively from the data in a validation sample. In particular, we divide our sample into three disjoint time periods that maintain the temporal ordering of the data. The first, or “training,” subsample is used to estimate the model subject to a specific set of tuning parameter values.\n",
    "\n",
    "The second, or “validation,” sample is used for tuning the hyperparameters. We construct forecasts for data points in the validation sample based on the estimated model from the training sample. Next, we calculate the objective function based on forecast errors from the validation sample, and iteratively search for hyperparameters that optimize the validation objective (at each step reestimating the model from the training data subject to the prevailing hyperparameter values).\n",
    "\n",
    "Tuning parameters are chosen from the validation sample taking into account estimated parameters, but the parameters are estimated from the training data alone. The idea of validation is to simulate an out-of-sample test of the model. Hyperparameter tuning amounts to searching for a degree of model complexity that tends to produce reliable out-of-sample performance. The validation sample fits are of course not truly out of sample, because they are used for tuning, which is in turn an input to the estimation. Thus, the third, or “testing,” subsample, which is used for neither estimation nor tuning, is truly out of sample and thus is used to evaluate a method’s predictive performance.\n",
    "\n",
    "In this section, you will learn some regression models and try to implement them. Here's the regression models you will be familiar with: **OLS, OLS+H, PCR, PLS, Lasso, Lasso+H, Ridge, Ridge+H, ENet, ENet+H and Group Lasso, Group Lasso+H**, also including the **Oracle Regression Model**.\n",
    "\n",
    "\n",
    "This section is cited of paper [Empirical Asset Pricing via Machine Learning](https://dachxiu.chicagobooth.edu/download/ML.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS\n",
    "\n",
    "Now, we can create the actual model to approximate returns of characteristics. First, the simpliest model: OLS, short for ordinary least square. The simple linear model assumes that conditional expectation $g^{\\star}(·)$ can be approximated by a linear function of the raw predictor variables and the\n",
    "parameter vector, $\\theta$,\n",
    "$$g(z_{i,t};\\theta)=z_{i,t}'\\;\\theta$$\n",
    "\n",
    "This model imposes a simple regression specification and does not allow for nonlinear effects or interactions between predictors.\n",
    "\n",
    "Open the file `Reg.py` and implement the function `OLS()`. Test your code here and make sure your error is of magnitude `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import OLS\n",
    "np.random.seed(5)\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) * 0.01\n",
    "ytrain = np.random.randn(N,1) * 0.01\n",
    "mtrain = np.mean(ytrain)\n",
    "\n",
    "ytrain_demean    = ytrain - mtrain\n",
    "xoos   = np.random.randn(N,p) * 0.01\n",
    "yoos   = np.random.randn(N,1) * 0.01\n",
    "r2_oos,r2_is,b,v = OLS(xtrain,ytrain,ytrain_demean,mtrain,xoos,yoos)\n",
    "expect_roos = -2.62487715\n",
    "expect_ris  = 0.93824603\n",
    "expect_b    = np.array([[1.03246781,-0.57572949,0.30877116,2.22773706,\n",
    "                         0.53579763,0.34007811,-0.01709376,-0.47917419]])\n",
    "expect_v    = np.array([[8.41495322e-01,3.96755096e-01,1.74216766e-01,1.75101820e+00,\n",
    "                         4.26636411e-01,1.30379085e-01,3.80488058e-04,2.41441866e-01]])\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OLS+H\n",
    "After implementsing OLS, we can perform a simple prediction on training set. However, the convexity of the least squares loss places extreme emphasis on large errors, thus outliers can undermine the stablity of OLS-based prediction. The statistics literature, long aware of this problem, has developed modified least squares objective functions that tend to produce more stable forecasts than OLS in the presence of extreme observations. In the machine learning literature, a common choice for counteracting the deleterious effect of heavy-tailed observations is the Huber robust objective function. Open the file `Reg.py` and implement the function `OLSH()`. Test your code here and make sure your error is of magnitude `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import OLSH,soft_thresholdl\n",
    "np.random.seed(6)\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) * 0.01\n",
    "ytrain = np.random.randn(N,1) * 0.01\n",
    "mtrain = np.mean(ytrain)\n",
    "\n",
    "ytrain_demean = ytrain - mtrain\n",
    "xoos   = np.random.randn(N,p) * 0.01\n",
    "yoos   = np.random.randn(N,1) * 0.01\n",
    "mu     = 0.001\n",
    "tol    = 1e-5\n",
    "L      = 10000\n",
    "r2_oos,r2_is,b,v = OLSH(xtrain,ytrain,ytrain_demean,mtrain,xoos,yoos,\\\n",
    "                        mu,tol,L,soft_thresholdl)\n",
    "expect_roos = -10.46818178\n",
    "expect_ris  = 0.85998362\n",
    "expect_b    = np.array([[-1.10864565,-0.33660359,1.30256235,0.23021716,\n",
    "                         -0.00943861,-0.96309068,1.00235173,-0.20870945]])\n",
    "expect_v    = np.array([[1.41248027e+00,1.00154851e-01,1.97150088e+00,3.70812396e-02,\n",
    "                         1.46988180e-04,1.09638383e+00,6.60536012e-01,5.29987272e-02]])\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Regression and PLS Regrassion\n",
    "\n",
    "Our implementation of PCR and PLS begins from the vectorized version of\n",
    "the linear model. In particular, we reorganize the linear regression $r_{i,t+1} = z_{i,t}' \\theta + \\epsilon_{i,t+1}$ as\n",
    "$$ R = Z\\theta + E$$\n",
    "\n",
    "where $R$ is the $NT \\times 1$ vector of $r_{i,t+1}$, $Z$ is the $NT \\times P$ matrix of stacked predictors $z_{i,t}$, and $E$ is a $NT \\times 1$ vector of residuals $\\epsilon_{i,t+1}$.\n",
    "\n",
    "PCR and PLS take the same general approach to reducing the dimensionality. They both condense the set of predictors from dimension $P$ to a much smaller number of $K$ linear combinations of predictors. Thus, the forecasting model for both methods is written as\n",
    "\n",
    "$$R = (Z \\Omega_{K}) \\theta_{K} + \\tilde{E}$$\n",
    "\n",
    "$\\Omega_{K}$ is $P \\times K$ matrix with columns $\\omega_{1},\\omega_{2}, ..., \\omega_{K}$. Each $\\omega_{j}$ is the set of linear combination weights used to create the $j$ th predictive components, thus $Z \\Omega_{K}$ is the dimension-reduced version of the original predictor set. Likewise, the predictive coefficient $\\theta_{K}$ is now a $K\\times 1$ vector rather than $P\\times 1$. \n",
    "\n",
    "PCR chooses the combination weights $\\Omega_{K}$ recursively. The $j^{th}$ linear combination solves\n",
    "\n",
    "$$\\omega_{j} = \\text{arg} \\max_{\\omega} \\text{Var} (Z \\omega), \\text{s.t.} \\;\n",
    "\\omega'\\omega = 1, \\text{Cov}(Z \\omega,Z \\omega_{l}) = 0, l = 1,2,...,j-1.$$\n",
    "\n",
    "Intuitively, PCR seeks the $K$ linear combinations of $Z$ that most faithfully mimic the full predictor set. The objective illustrates that the choice of components is not based on the forecasting objective at all. Instead, the emphasis of PCR is on finding components that retain the most possible common variation within the predictor set. The well known solution for PCR computes $\\Omega_{K}$ via singular value decomposition of $Z$, and therefore the PCR algorithm is extremely efficient from a computational standpoint.\n",
    "In contrast to PCR, the PLS objective seeks $K$ linear combinations of $Z$ that have maximal predictive association with the forecast target. The weights used to construct the $j^{th}$ PLS component solve\n",
    "\n",
    "$$\\omega_{j} = \\text{arg} \\max_{\\omega} \\text{Cov}^{2} (R,Z \\omega), \\text{s.t.} \\;\n",
    "\\omega'\\omega = 1, \\text{Cov}(Z \\omega,Z \\omega_{l}) = 0, l = 1,2,...,j-1.$$\n",
    "\n",
    "This objective highlights the main distinction between PCR and PLS. PLS is willing to sacrifice how accurately $Z\\Omega_{K}$ approximates $Z$ in order to find components with more potent return predictability. The problem of PLS can be efficiently solved using a number of similar routines, the most prominent being the SIMPLS algorithm of de Jong (1993).\n",
    "\n",
    "Finally, given a solution for $\\Omega_{K}$, $\\theta_{K}$ is estimated in both PCR and PLS via OLS regression of $R$ on $Z\\Omega_{K}$. For both models, $K$ is a hyperparameter that can be determined adaptively from the validation sample.\n",
    "\n",
    "Then, we will use previous function `PCR()` and `pls()` to create PCA regression and PLS regression. Open the file `Reg.py`, follow the steps to implement function `PCAR()` and `PLAR()`. Test your code here, you should get error on the order of `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import PCAR\n",
    "np.random.seed(6)\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) * 0.01\n",
    "ytrain = np.random.randn(N,1) * 0.01\n",
    "mtrain = np.mean(ytrain)\n",
    "ytrain_demean = ytrain - mtrain\n",
    "\n",
    "xoos   = np.random.randn(N,p) * 0.01\n",
    "yoos   = np.random.randn(N,1) * 0.01\n",
    "\n",
    "xtest  = np.random.randn(N,p) * 0.01\n",
    "ytest  = np.random.randn(N,1) * 0.01\n",
    "\n",
    "ne     = 5\n",
    "\n",
    "\n",
    "r2_oos,r2_is,b,v = PCAR(xtrain,ytrain,ytrain_demean,mtrain,xoos,yoos,xtest,ytest,ne)\n",
    "\n",
    "expect_roos      = -0.30884144116077\n",
    "expect_ris       = 0.113068966716010\n",
    "expect_b         = np.array([[0.0802575,0.01376213,0.0884982,-0.03628063,\n",
    "                             -0.16151551,0.02924157,0.04834153,-0.0733434]])\n",
    "expect_v         = np.array([[-0.00605116,-0.01083863,0.01308499,0.0020043,\n",
    "                               0.03914248,-0.00527651,0.04044305,-0.00278299]])\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import PLSR\n",
    "np.random.seed(6)\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) * 0.01\n",
    "ytrain = np.random.randn(N,1) * 0.01\n",
    "mtrain = np.mean(ytrain)\n",
    "ytrain_demean = ytrain - mtrain\n",
    "\n",
    "xoos   = np.random.randn(N,p) * 0.01\n",
    "yoos   = np.random.randn(N,1) * 0.01\n",
    "\n",
    "xtest  = np.random.randn(N,p) * 0.01\n",
    "ytest  = np.random.randn(N,1) * 0.01\n",
    "\n",
    "ne     = 5\n",
    "\n",
    "r2_oos,r2_is,b,v = PLSR(xtrain,ytrain,ytrain_demean,mtrain,xoos,yoos,xtest,ytest,ne)\n",
    "\n",
    "expect_roos      = -2.64665594438756\n",
    "expect_ris       = 0.355202101604475\n",
    "expect_b         = np.array([[0.10038269,-0.38864287,0.22868396,-0.1077111,\n",
    "                             -0.37620204,-0.05061696,0.54888731,-0.10922797]])\n",
    "expect_v         = np.array([[-0.04917611,0.22886778,-0.0102955,-0.00796865,\n",
    "                              -0.21511732,0.01438165,0.16142618,-0.02715686]])\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso, Ridge and Enet\n",
    "The simple linear model is bound to fail in the presence of many predictors. When the number of predictors $P$ approaches the number of observations $T$ , the linear model becomes inefficient or even inconsistent. It begins to overfit noise rather than extracting signal. This is particularly troublesome for the problem of return prediction where the signal-to-noise ratio is notoriously low.\n",
    "\n",
    "Crucial for avoiding overfit is reducing the number of estimated parameters. The most common machine learning device for imposing parameter parsimony is to append a penalty to the objective function in order to favor more parsimonious specifications. This “regularization” of the estimation problem mechanically deteriorates a model’s in-sample performance in hopes that it improves its stability out of sample. This will be the case when penalization manages to reduce the model’s fit of noise, while preserving its signal fit.\n",
    "\n",
    "The statistical model for our penalized linear model is the same as the simple linear model. That is, it continues to consider only the baseline, untransformed predictors. Penalized methods differ by appending a penalty to the original loss function:\n",
    "\n",
    "$$\\mathcal{L} (\\theta;\\cdot) = \\mathcal{L}(\\theta) + \\phi(\\theta;\\cdot)$$\n",
    "\n",
    "There are several choices for the penalty function $\\phi(\\theta;\\cdot)$. We focus on the popular “elastic net” penalty, which takes the form:\n",
    "\n",
    "$$ \\phi(\\theta ; \\lambda, \\rho) = \\lambda ( 1 - \\rho) \n",
    "\\sum_{j=1}^{P} | \\theta_{j}| + \\frac{1}{2} \\lambda \\rho \n",
    "\\sum_{j=1}^{P} \\theta_{j}^{2}$$\n",
    "\n",
    "The elastic net involves two nonnegative hyperparameters, $\\lambda$ and $\\rho$, and includes two well-known regularizers as special cases. The $\\rho = 0$ case corresponds to the lasso and uses an absolute value, or “$l_{1}$,” parameter penalization. The fortunate geometry of the lasso sets coefficients on a subset of covariates to exactly zero. In this sense, the lasso imposes sparsity on the specification and can thus be thought of as a variable *selection* method. The $\\rho = 1$ case corresponds to ridge regression, which uses an $l_{2}$ parameter penalization, that draws all coefficient estimates closer to zero but does not impose exact zeros anywhere. In this sense, ridge is a shrinkage method that helps prevent coefficients from becoming unduly large in magnitude. For intermediate values of $\\rho$, the elastic net encourages simple models through both shrinkage and selection.\n",
    "\n",
    "We adaptively optimize the tuning parameters, $\\lambda$ and $\\rho$, using the validation sample. Our implementation of penalized regression uses the accelerated proximal gradient algorithm and accommodates both least squares and Huber objective functions.\n",
    "\n",
    "\n",
    "Now, you can implement `Lasso()`, `Ridge()` and `Enet()` to create **Lasso, LassoH, Ridge, RidgeH, Enet and Enet+H** model. Test your code here. You should get error on the order of `e-5` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import Lasso\n",
    "\n",
    "np.random.seed(8)\n",
    "\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) * 0.1\n",
    "ytrain = np.random.randn(N,1) * 0.1\n",
    "mtrain = np.mean(ytrain)\n",
    "ytrain_demean = ytrain - mtrain\n",
    "\n",
    "XX     = xtrain.T.dot(xtrain) \n",
    "XY     = xtrain.T.dot(ytrain_demean)\n",
    "\n",
    "xoos   = np.random.randn(N,p)  * 0.1\n",
    "yoos   = np.random.randn(N,1)  * 0.1\n",
    "\n",
    "xtest  = np.random.randn(N,p) * 0.1\n",
    "ytest  = np.random.randn(N,1) * 0.1 \n",
    "\n",
    "tol    = 1e-3\n",
    "L      = 1000\n",
    "alpha  = 1\n",
    "lamv   = [-6,-5,-4]\n",
    "mu     = 0.001\n",
    "\n",
    "\n",
    "r2_oos,r2_is,b,v,r2_oos_H,r2_is_H,b_H,v_H = Lasso(XX,XY,xtrain,ytrain,\\\n",
    "                                                      ytrain_demean,mtrain,\\\n",
    "                                                      xoos,yoos,xtest,ytest,\\\n",
    "                                                      mu,tol,L,alpha,lamv)\n",
    "expect_roos   =  0.017764443311936495\n",
    "expect_ris    =  0.11727752632364752\n",
    "expect_b      = [-0.01298148,0.01059046,-0.00955958,-0.02074591,-0.00275158,\n",
    "                 -0.01082944,0.01415878,0.00520742]\n",
    "expect_v      = [0.01598016,0.01074124,0.00847576,0.04256298,0.00084163,\n",
    "                 0.01224452,0.0194681,0.00231169]\n",
    "expect_roos_H =  0.01776444\n",
    "expect_ris_H  =  0.11727753\n",
    "expect_b_H    = [-0.01298148,0.01059046,-0.00955958,-0.02074591,-0.00275158,\n",
    "                 -0.01082944,0.01415878,0.00520742]\n",
    "expect_v_H    = [0.01598016,0.01074124,0.00847576,0.04256298,0.00084163,\n",
    "                 0.01224452,0.0194681,0.00231169]\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))\n",
    "print(rel_error(r2_oos_H,expect_roos_H))\n",
    "print(rel_error(r2_is_H,expect_ris_H))\n",
    "print(rel_error(b_H,expect_b_H))\n",
    "print(rel_error(v_H,expect_v_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import ridge\n",
    "\n",
    "np.random.seed(9)\n",
    "\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) \n",
    "ytrain = np.random.randn(N,1) \n",
    "mtrain = np.mean(ytrain)\n",
    "ytrain_demean = ytrain - mtrain\n",
    "\n",
    "XX     = xtrain.T.dot(xtrain) \n",
    "XY     = xtrain.T.dot(ytrain_demean)\n",
    "\n",
    "xoos   = np.random.randn(N,p)  \n",
    "yoos   = np.random.randn(N,1)  \n",
    "\n",
    "xtest  = np.random.randn(N,p) \n",
    "ytest  = np.random.randn(N,1)  \n",
    "\n",
    "tol    = 1e-3\n",
    "L      = 1000\n",
    "alpha  = 1\n",
    "lamv   = [-7,-6,-5]\n",
    "mu     = 0.01\n",
    "\n",
    "\n",
    "r2_oos,r2_is,b,v,r2_oos_H,r2_is_H,b_H,v_H = ridge(XX,XY,xtrain,ytrain,\\\n",
    "                                                      ytrain_demean,mtrain,\\\n",
    "                                                      xoos,yoos,xtest,ytest,\\\n",
    "                                                      mu,tol,L,alpha,lamv)\n",
    "\n",
    "expect_roos   = -0.13576377873651002\n",
    "expect_ris    = 0.767011957407248\n",
    "expect_b      = [0.00785283,0.50855187,-0.22865912,0.0178831,\n",
    "                -0.25993783,-0.00427208,0.20439858,0.24239826]\n",
    "expect_v      = [-4.88042803e-04,4.47277473e-01,  4.30400931e-02,3.24499783e-04,\n",
    "                  1.99495639e-02,6.20250496e-04,2.15964606e-01,1.12171943e-01]\n",
    "expect_roos_H = -0.13579605\n",
    "expect_ris_H  = 0.76702826\n",
    "expect_b_H    = [ 0.00784691,0.50858005,-0.22868178,0.01788824,\n",
    "                 -0.25994479,-0.00427235,0.20441274,0.24243719]\n",
    "expect_v_H    = [-4.87552052e-04,4.47312809e-01,4.30404119e-02,3.23845178e-04,\n",
    "                  1.99377721e-02,6.20430699e-04,2.15969606e-01,1.12177408e-01]\n",
    "\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))\n",
    "print(rel_error(r2_oos_H,expect_roos_H))\n",
    "print(rel_error(r2_is_H,expect_ris_H))\n",
    "print(rel_error(b_H,expect_b_H))\n",
    "print(rel_error(v_H,expect_v_H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import Enet\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) \n",
    "ytrain = np.random.randn(N,1) \n",
    "mtrain = np.mean(ytrain)\n",
    "ytrain_demean = ytrain - mtrain\n",
    "\n",
    "XX     = xtrain.T.dot(xtrain) \n",
    "XY     = xtrain.T.dot(ytrain_demean)\n",
    "\n",
    "xoos   = np.random.randn(N,p)  \n",
    "yoos   = np.random.randn(N,1)  \n",
    "\n",
    "xtest  = np.random.randn(N,p) \n",
    "ytest  = np.random.randn(N,1)  \n",
    "\n",
    "tol    = 1e-3\n",
    "L      = 1000\n",
    "alpha  = 1\n",
    "lamv   = [-7,-6,-5]\n",
    "mu     = 0.01\n",
    "\n",
    "\n",
    "r2_oos,r2_is,b,v,r2_oos_H,r2_is_H,b_H,v_H = Enet(XX,XY,xtrain,ytrain,\\\n",
    "                                                      ytrain_demean,mtrain,\\\n",
    "                                                      xoos,yoos,xtest,ytest,\\\n",
    "                                                      mu,tol,L,alpha,lamv)\n",
    "\n",
    "\n",
    "expect_roos   = -0.9102052662776483\n",
    "expect_ris    = 0.5164359379277887\n",
    "expect_b      = [0.29423895,-0.04053108,-0.10959231,0.05154946,\n",
    "                 0.60408725,0.31032992,0.53226383,0.17623848]\n",
    "expect_v      = [0.03960828,-0.00078205,0.00219732,0.00710816,\n",
    "                 0.202848,0.06113573,0.11937089,0.04743008]\n",
    "expect_roos_H = -0.9101137\n",
    "expect_ris_H  = 0.51644045\n",
    "expect_b_H    = [0.29419352,-0.04055566,-0.10958457,0.05150143,\n",
    "                 0.60408996,0.31030545,0.53225886,0.17629508]\n",
    "expect_v_H    = [0.03962512,-0.00078528,0.00220263,0.00710675,\n",
    "                 0.20286353,0.06115181,0.11936435,0.04742936]\n",
    "\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))\n",
    "print(rel_error(r2_oos_H,expect_roos_H))\n",
    "print(rel_error(r2_is_H,expect_ris_H))\n",
    "print(rel_error(b_H,expect_b_H))\n",
    "print(rel_error(v_H,expect_v_H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oracle Linear Model\n",
    "In the oracle linear model, we make a small change in convariant matrix. Open the file `Reg.py` and implement the function `Oracle()`. Test your code here. You should get error of magnitude `e-9` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import Oracle\n",
    "\n",
    "np.random.seed(11)\n",
    "\n",
    "N      = 10\n",
    "p      = 8\n",
    "xtrain = np.random.randn(N,p) \n",
    "ytrain = np.random.randn(N,1) \n",
    "mtrain = np.mean(ytrain)\n",
    "\n",
    "xoos   = np.random.randn(N,p)  \n",
    "yoos   = np.random.randn(N,1)\n",
    "\n",
    "nump   = 5\n",
    "\n",
    "mo     = 1\n",
    "r2_oos,r2_is = Oracle(mo,nump,xtrain,ytrain,mtrain,xoos,yoos)\n",
    "\n",
    "expect_roos  = -1.51856578\n",
    "expect_ris   = 0.30796397\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Lasso \n",
    "\n",
    "Linear models are popular in practice, in part because they can be thought of as a first-order approximation to the data generating process. When the “true” model is complex and nonlinear, restricting the functional form to be linear introduces approximation error due to model misspecification. Let $g^{\\star}(z_{i,t})$ denote the true model and $g(z_{i,t};\\theta)$ the functional form specified by the econometrician. And let $g(z_{i,t};\\hat{\\theta})$ and $\\hat{r}_{i,t+1}$ denote the fitted model and its\n",
    "ensuing return forecast. We can decompose a model’s forecast error as:\n",
    "\n",
    "\n",
    "$$ \\begin{matrix}\n",
    "r_{i,t+1} - \\hat{r}_{i,t+1} = &\\underbrace{g^{\\star}(z_{i,t})-g(z_{i,t};\\theta)} \\;+ &\\underbrace{g(z_{i,t};\\theta) - g(z_{i,t};\\hat{\\theta})} \\;+&\\underbrace{\\epsilon_{i,t}}\n",
    "\\\\ &\\text{approximation error} & \\text{estimation error} &\\text{intrinstic error} \n",
    "\\end{matrix}$$\n",
    "\n",
    "Intrinsic error is irreducible; it is the genuinely unpredictable component of returns associated with news arrival and other sources of randomness in financial markets. Estimation error, which arises due to sampling variation, is determined by the data. It is potentially reducible by adding new observations, though this may not be under the econometrician’s control. Approximation error is directly controlled by the econometrician and is potentially reducible by incorporating more flexible specifications that improve the model’s ability to approximate the true model. But additional flexibility raises the risk of overfitting and destabilizing the model out of sample. In this and the following subsections, we introduce nonparametric models of $g(\\cdot)$ with increasing degrees of flexibility, each complemented by regularization methods to mitigate overfit.\n",
    "\n",
    "The first and most straightforward nonparametric approach that we consider is the generalized linear model. It introduces nonlinear transformations of the original predictors as new additive terms in an otherwise linear model. Generalized linear models are thus the closest nonlinear counterparts to the linear approaches in previous section. \n",
    "\n",
    "The model we study adapts the simple linear form by adding a $K$-term spline series expansion of the predictors\n",
    "\n",
    "$$ g(z;\\theta,p(\\cdot)) = \\sum_{j=1}^{P} p(z_{j})' \\theta_{j}$$\n",
    "\n",
    "where $p(\\cdot)=(p_1(\\cdot),p_2(\\cdot),...,p_{K}(\\cdot))'$ is a vector of basis functions, and the parameters are now a $K\\times N$ matrix $\\theta =(\\theta_1,\\theta_2,...,\\theta_{N})$. There are many potential choices for spline functions. We adopt a spline series of order two: $\\left(1, z, (z-c_1)^{2}, (z-c_2)^{2}, ..., (z-c_{K-2}\\;)^{2}\\; \\right)$, where $c_1, c_2, …c_{K−2}$ are knots.\n",
    "\n",
    "Because higher-order terms enter additively, forecasting with the generalized linear model can be approached with the same estimation tools as in simple linear model. In particular, our analysis uses a least squares objective function, both with and without the Huber robustness modification. Because series expansion quickly multiplies the number of model parameters, we use penalization to control degrees of freedom. Our choice of penalization function is specialized for the spline expansion setting and is known as the group lasso. It takes the form\n",
    "\n",
    "$$\\phi(\\theta ; \\lambda,K) = \\lambda \\sum_{j=1}^{P} \\left(\n",
    "\\sum_{k=1}^{K} \\theta_{j,k}^{2}\n",
    "\\right)^{1/2}$$\n",
    "\n",
    "As its name suggests, the group lasso selects either all $K$ spline terms associated with a given characteristic or none of them. We embed this penalty in the general objective of penalized model. Group lasso accommodates either least squares or robust Huber objective, and it uses the same accelerated proximal gradient descent as the elastic net. It has two tuning parameters, $\\lambda$ and $K$.\n",
    "\n",
    "Open the file `Reg.py` and implement the function `Group_lasso()`. Test your code here and you should get an error of magnitude `e-7` or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Reg import Group_lasso\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "N      = 10\n",
    "p      = 2\n",
    "xtrain = np.random.randn(N,p) \n",
    "ytrain = np.random.randn(N,1) \n",
    "mtrain = np.mean(ytrain)\n",
    "ytrain_demean = ytrain - mtrain\n",
    "\n",
    "xoos   = np.random.randn(N,p)  \n",
    "yoos   = np.random.randn(N,1)\n",
    "\n",
    "xtest  = np.random.randn(N,p)  \n",
    "ytest  = np.random.randn(N,1)\n",
    "\n",
    "kn     = 3\n",
    "\n",
    "mu     = 0.001\n",
    "lamv   = [-1,-2]\n",
    "tol    = 1e-5\n",
    "\n",
    "r2_oos,r2_is,b,v,r2_oos_H,r2_is_H,b_H,v_H = Group_lasso(kn,xtrain,ytrain,\\\n",
    "                                                        ytrain_demean,mtrain,\\\n",
    "                                                        xoos,yoos,xtest,ytest,mu,\\\n",
    "                                                        tol,lamv)\n",
    "\n",
    "\n",
    "expect_roos   = -13.774661580655398\n",
    "expect_ris    = 0.7520909205110202\n",
    "expect_b      = [-1.3814082,5.49855818,-5.45226757,1.03838396,0.01786894,\n",
    "                  0.05652455,0.05413286,0.01199471]\n",
    "expect_v      = [1.55230372e+00,2.45922982e+01,2.41808615e+01,8.77280318e-01,\n",
    "                 3.18114214e-04,3.24397984e-03,2.96481549e-03,1.44667755e-04]\n",
    "expect_roos_H = -13.76682029\n",
    "expect_ris_H  = 0.75202276\n",
    "expect_b_H    = [-1.38108829,5.49731366,-5.45099348,1.03819251,\n",
    "                  0.017472,0.05524136,0.05289458,0.01169055]\n",
    "expect_v_H    = [1.55657585e+00,2.45640198e+01,2.41817551e+01,8.75582129e-01,\n",
    "                 3.88133765e-04,3.39374814e-03,3.11249465e-03,1.96395816e-04]\n",
    "\n",
    "print(rel_error(r2_oos,expect_roos))\n",
    "print(rel_error(r2_is,expect_ris))\n",
    "print(rel_error(b,expect_b))\n",
    "print(rel_error(v,expect_v))\n",
    "print(rel_error(r2_oos_H,expect_roos_H))\n",
    "print(rel_error(r2_is_H,expect_ris_H))\n",
    "print(rel_error(b_H,expect_b_H))\n",
    "print(rel_error(v_H,expect_v_H))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Start Monte Carlo Simulation\n",
    "\n",
    "Now, you are ready for the MC simulation. In this section, you only need to provide some parameters, such as the times for MC simulation, P, horizon and model type.\n",
    "\n",
    "Start your simulation now! \n",
    "\n",
    "This section is cited of paper [Empirical Asset Pricing via Machine Learning](https://dachxiu.chicagobooth.edu/download/ML.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from Reg import MC_simulation\n",
    "MC      = [1,10]\n",
    "datanum = 100\n",
    "horizon = [1]\n",
    "mo      = [1]\n",
    "MC_simulation(MC,datanum,horizon,mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
