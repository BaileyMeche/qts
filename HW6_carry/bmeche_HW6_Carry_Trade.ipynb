{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6 FX Carry \n",
    "Bailey Meche"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6ea72",
   "metadata": {},
   "source": [
    "## Submission notes  \n",
    "\n",
    "This notebook implements the HW6 specification exactly:\n",
    "- Weekly W-WED entry/exit; FX/OIS aligned within ±2 days.\n",
    "- Borrow in GBP at OIS+50bp on 4/5 notional (5x leverage); lend via a 5Y par bond in each EM currency.\n",
    "- Coupon fixed at entry 5Y swap rate; quarterly coupons; mark-to-market one week later using the new swap curve bootstrapped to zero-coupon discounts.\n",
    "- Trades only when the lending 5Y swap exceeds the funding 5Y swap by at least 50bp.\n",
    "- All cashflows converted to USD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a219f",
   "metadata": {},
   "source": [
    "#  Report\n",
    "\n",
    "## Spec implemented  \n",
    "- Weekly entry/exit on W-WED; nearest trading day within ±2 days for FX/OIS.\n",
    "- Funding GBP: borrow at OIS+50bp on 4/5 notional (5x leverage).\n",
    "- Lending: buy 5Y par bond, coupon fixed at entry 5Y swap rate, quarterly coupons.\n",
    "- Filter: trade if (lend 5Y swap − fund 5Y swap) ≥ 50bp.\n",
    "- MTM: reprice remaining CFs one week later on new curve (bootstrapped ZC from par curve).\n",
    "- All flows in USD; returns shown as equity return (P&L / $2MM) and notional-normalized.\n",
    "\n",
    "## Data + interpolation audit\n",
    "- Curves densified to business days via time interpolation (max gap 45 bdays).\n",
    "- Weekly curves sampled within ±7 days.\n",
    "- Enforced: curve must cover **1Y and 5Y** at entry and exit.\n",
    "- See: `curve_time_interpolation_audit.csv`, `alignment_staleness_summary.csv`, `entry_par_check.csv`.\n",
    "\n",
    "## Performance\n",
    "### Variable-capital portfolio (avg across active positions)\n",
    "| n_weeks | mean_w | vol_w | sharpe_w | mean_ann | vol_ann | sharpe_ann | terminal_wealth | max_drawdown |\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| 365 | -0.006287 | 0.118949 | -0.052851 | -0.326902 | 0.857751 | -0.381115 | 0.006521 | -0.998644 |\n",
    "\n",
    "### Fixed-allocation portfolio (inactive=0)\n",
    "| n_weeks | mean_w | vol_w | sharpe_w | mean_ann | vol_ann | sharpe_ann | terminal_wealth | max_drawdown |\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "| 365 | -0.001835 | 0.044655 | -0.041098 | -0.095433 | 0.322014 | -0.296365 | 0.355795 | -0.842076 |\n",
    "\n",
    "## Correlations, risk factors, contributions\n",
    "- Correlations: `currency_corr_all_weeks.csv` and `currency_corr_active_overlap.csv`\n",
    "- Factor regression (HAC): `market_factor_regs.csv`\n",
    "- Sharpe variance shares: `sharpe_variance_contrib_fixed.csv`\n",
    "- Worst-10-week drawdown contributors: `drawdown_contrib_worst10w.csv`\n",
    "![alt text](wealth_fixed.png)![alt text](drawdown_fixed.png)![alt text](cum_pnl_by_ccy.png)![alt text](corr_heatmap_all.png)\n",
    "- `figures/wealth_fixed.png`\n",
    "- `figures/drawdown_fixed.png`\n",
    "- `figures/cum_pnl_by_ccy.png`\n",
    "- `figures/corr_heatmap_all.png`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'timestamp_utc': '2026-02-18T05:21:06.987181+00:00', 'python': '3.13.9', 'pandas': '2.3.3', 'numpy': '2.3.3', 'files_chosen': {}, 'coverage': {}, 'weekly_obs': None, 'n_currencies': None, 'git_commit': '84c6f27'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import json, os, re, subprocess, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "BASE=Path('.').resolve()\n",
    "OUT=BASE/'outputs'\n",
    "FIG=OUT/'figures'\n",
    "DATA=BASE/'data'\n",
    "DATA_CLEAN=BASE/'data_clean'\n",
    "OUT.mkdir(parents=True,exist_ok=True)\n",
    "FIG.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "def backup_if_exists(path: Path):\n",
    "    if path.exists():\n",
    "        ts=datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        bak=path.with_name(path.name+f'.bak_{ts}')\n",
    "        path.rename(bak)\n",
    "        print('Backed up existing',path,'->',bak)\n",
    "\n",
    "def write_text_new(path: Path, text: str):\n",
    "    backup_if_exists(path)\n",
    "    path.write_text(text)\n",
    "\n",
    "def write_csv_new(df: pd.DataFrame, path: Path, index=False):\n",
    "    backup_if_exists(path)\n",
    "    df.to_csv(path, index=index)\n",
    "\n",
    "manifest={\n",
    " 'timestamp_utc':datetime.now(timezone.utc).isoformat(),\n",
    " 'python':platform.python_version(),'pandas':pd.__version__,'numpy':np.__version__,\n",
    " 'files_chosen':{},'coverage':{},'weekly_obs':None,'n_currencies':None\n",
    "}\n",
    "try:\n",
    "    manifest['git_commit']=subprocess.check_output(['git','rev-parse','--short','HEAD'],text=True).strip()\n",
    "except Exception:\n",
    "    manifest['git_commit']='unknown'\n",
    "print(manifest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed EM curves: | files_used 5 | rows 9792 | ccy_counts {'BRL': 3973, 'TRY': 2890, 'ZAR': 2317, 'PKR': 349, 'NGN': 263} | tenor_range (1.0, 10.0)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# LOAD + CANONICALIZE INPUTS (single source of truth)\n",
    "# Produces:\n",
    "#   ois_on  : pd.Series (daily, decimal), UK overnight OIS (IUDSOIA)\n",
    "#   fx_usd_per_ccy : pd.DataFrame (daily), columns include EM + GBP, values USD per 1 CCY\n",
    "#   curves_long : pd.DataFrame columns [date, ccy, tenor_y, par_swap] (decimal)\n",
    "#   gbp_s5 : pd.Series (daily, decimal), GBP 5Y par swap proxy used in filter\n",
    "# ============================================\n",
    "# ---- EM curves (Bloomberg exports; robust parser) ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "EM = [\"BRL\",\"NGN\",\"PKR\",\"TRY\",\"ZAR\"]\n",
    "\n",
    "EM_CURVE_DIRS = [DATA, DATA_CLEAN, BASE / \"_data\", BASE / \"data_raw\"]\n",
    "EM_CURVE_GLOBS = [\n",
    "    \"*Emerging*Mkt*YC*.csv\",\n",
    "    \"*Emerging*YC*.csv\",\n",
    "    \"*EM*YC*.csv\",\n",
    "    \"*YC*Emerging*.csv\",\n",
    "]\n",
    "\n",
    "def _read_csv_robust(fp: Path) -> pd.DataFrame:\n",
    "    # Try common Bloomberg layouts: single header, two-row header, messy preamble\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        if df.shape[1] >= 2:\n",
    "            return df\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Two-row header (e.g., security row + PX_LAST row)\n",
    "    try:\n",
    "        df2 = pd.read_csv(fp, header=[0, 1])\n",
    "        if isinstance(df2.columns, pd.MultiIndex) and df2.shape[1] >= 2:\n",
    "            # Collapse to first level (security names)\n",
    "            df2.columns = [str(c[0]) for c in df2.columns]\n",
    "            return df2\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Fallback: python engine + skip bad lines\n",
    "    df = pd.read_csv(fp, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    return df\n",
    "\n",
    "def _detect_header_row(fp: Path, max_rows: int = 30) -> int | None:\n",
    "    # Finds a row containing \"date\" / \"dates\" to use as header\n",
    "    raw = pd.read_csv(fp, header=None, nrows=max_rows, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    for i in range(min(max_rows, len(raw))):\n",
    "        row = raw.iloc[i].astype(str).str.strip().str.lower()\n",
    "        if row.isin([\"date\", \"dates\", \"as of date\", \"asof\", \"dt\"]).any():\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "def _normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    # drop fully empty cols\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    return df\n",
    "\n",
    "def _find_date_col(df: pd.DataFrame) -> str | None:\n",
    "    cols = list(df.columns)\n",
    "    # Prefer explicit date columns\n",
    "    for c in cols:\n",
    "        if re.search(r\"\\b(date|dates|asof|as of)\\b\", str(c), flags=re.I):\n",
    "            return c\n",
    "    # Else try first col if it parses well\n",
    "    c0 = cols[0] if cols else None\n",
    "    if c0 is None:\n",
    "        return None\n",
    "    dt = pd.to_datetime(df[c0], errors=\"coerce\")\n",
    "    if dt.notna().mean() > 0.7:\n",
    "        return c0\n",
    "    return None\n",
    "\n",
    "def _strip_px_last_row(df: pd.DataFrame, date_col: str) -> pd.DataFrame:\n",
    "    # Bloomberg sometimes puts a first “data” row with PX_LAST repeated\n",
    "    if df.shape[0] == 0:\n",
    "        return df\n",
    "    first = df.iloc[0].copy()\n",
    "    # ignore date cell\n",
    "    vals = first.drop(labels=[date_col], errors=\"ignore\").astype(str).str.upper().str.strip()\n",
    "    if vals.size and (vals.isin([\"PX_LAST\",\"PX_MID\",\"LAST_PRICE\",\"LAST\"]).mean() > 0.8):\n",
    "        return df.iloc[1:].copy()\n",
    "    return df\n",
    "\n",
    "def _infer_ccy_tenor_from_string(s: str):\n",
    "    s0 = str(s).upper()\n",
    "\n",
    "    # Preferred: GTBRL 5Y, GTBRL5Y, GTBRL 60M, etc.\n",
    "    m = re.search(r\"GT\\s*([A-Z]{3})\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(Y|YR|M|W|D)\\b\", s0)\n",
    "    if not m:\n",
    "        m = re.search(r\"\\bGT([A-Z]{3})\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(Y|YR|M|W|D)\\b\", s0)\n",
    "    if m:\n",
    "        ccy = m.group(1)\n",
    "        n = float(m.group(2))\n",
    "        u = m.group(3)\n",
    "        tenor_y = n if u in {\"Y\",\"YR\"} else (n/12.0 if u==\"M\" else (n/52.0 if u==\"W\" else n/365.0))\n",
    "        return ccy, tenor_y\n",
    "\n",
    "    # Alternative: contains CCY and tenor like \"BRL 5Y\", \"TRY5Y\"\n",
    "    m = re.search(r\"\\b(\" + \"|\".join(EM) + r\")\\b.*?([0-9]+(?:\\.[0-9]+)?)\\s*(Y|YR|M|W|D)\\b\", s0)\n",
    "    if m:\n",
    "        ccy = m.group(1)\n",
    "        n = float(m.group(2))\n",
    "        u = m.group(3)\n",
    "        tenor_y = n if u in {\"Y\",\"YR\"} else (n/12.0 if u==\"M\" else (n/52.0 if u==\"W\" else n/365.0))\n",
    "        return ccy, tenor_y\n",
    "\n",
    "    # Another alternative: tenor like \"5Y\" but CCY maybe in filename later\n",
    "    m = re.search(r\"\\b([0-9]+(?:\\.[0-9]+)?)\\s*(Y|YR|M|W|D)\\b\", s0)\n",
    "    if m:\n",
    "        n = float(m.group(1))\n",
    "        u = m.group(2)\n",
    "        tenor_y = n if u in {\"Y\",\"YR\"} else (n/12.0 if u==\"M\" else (n/52.0 if u==\"W\" else n/365.0))\n",
    "        return None, tenor_y\n",
    "\n",
    "    return None, np.nan\n",
    "\n",
    "def _parse_em_curve_file(fp: Path) -> pd.DataFrame:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    EM = [\"BRL\",\"NGN\",\"PKR\",\"TRY\",\"ZAR\"]\n",
    "\n",
    "    def _parse_name(name: str):\n",
    "        s = str(name).upper()\n",
    "        m = re.search(r\"\\bGT\\s*([A-Z]{3})\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(Y|YR|M|W|D)\\b\", s)\n",
    "        if not m:\n",
    "            m = re.search(r\"\\bGT([A-Z]{3})\\s*([0-9]+(?:\\.[0-9]+)?)\\s*(Y|YR|M|W|D)\\b\", s)\n",
    "        if not m:\n",
    "            return None, np.nan\n",
    "        ccy = m.group(1)\n",
    "        n = float(m.group(2))\n",
    "        u = m.group(3)\n",
    "        tenor_y = n if u in {\"Y\",\"YR\"} else (n/12.0 if u==\"M\" else (n/52.0 if u==\"W\" else n/365.0))\n",
    "        return ccy, tenor_y\n",
    "\n",
    "    def _build_long(dates, values, ccy, tenor_y):\n",
    "        out = pd.DataFrame({\"date\": dates, \"ccy\": ccy, \"tenor_y\": float(tenor_y), \"par_swap\": values})\n",
    "        out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\")\n",
    "        out[\"par_swap\"] = pd.to_numeric(out[\"par_swap\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"date\",\"par_swap\"])\n",
    "        out[\"date\"] = out[\"date\"].dt.tz_localize(None)\n",
    "        return out\n",
    "\n",
    "    # ---------- Attempt 1: header-based paired columns ----------\n",
    "    try:\n",
    "        df = pd.read_csv(fp)\n",
    "        cols = list(df.columns)\n",
    "        gt_idx = [i for i, c in enumerate(cols)\n",
    "                  if re.search(r\"\\bGT[A-Z]{3}\\s*[0-9]+(?:\\.[0-9]+)?\\s*(Y|YR|M|W|D)\\b\", str(c).upper())]\n",
    "        out = []\n",
    "        for i in gt_idx:\n",
    "            if i + 1 >= len(cols):\n",
    "                continue\n",
    "            ccy, ten = _parse_name(cols[i])\n",
    "            if ccy not in EM or not np.isfinite(ten):\n",
    "                continue\n",
    "            dates = df.iloc[:, i]\n",
    "            vals = df.iloc[:, i+1]\n",
    "            out.append(_build_long(dates, vals, ccy, ten))\n",
    "        if out:\n",
    "            return pd.concat(out, ignore_index=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # ---------- Attempt 2: header=None, first row contains series names ----------\n",
    "    raw = pd.read_csv(fp, header=None, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    if raw.shape[0] < 2 or raw.shape[1] < 2:\n",
    "        return pd.DataFrame(columns=[\"date\",\"ccy\",\"tenor_y\",\"par_swap\"])\n",
    "\n",
    "    names = raw.iloc[0].astype(str)\n",
    "    out = []\n",
    "    used = set()\n",
    "\n",
    "    for j in range(raw.shape[1] - 1):\n",
    "        if j in used:\n",
    "            continue\n",
    "        name = names.iloc[j]\n",
    "        if not re.search(r\"\\bGT[A-Z]{3}\\s*[0-9]+(?:\\.[0-9]+)?\\s*(Y|YR|M|W|D)\\b\", name.upper()):\n",
    "            continue\n",
    "\n",
    "        ccy, ten = _parse_name(name)\n",
    "        if ccy not in EM or not np.isfinite(ten):\n",
    "            continue\n",
    "\n",
    "        dates = raw.iloc[1:, j]\n",
    "        vals = raw.iloc[1:, j+1]\n",
    "        out.append(_build_long(dates, vals, ccy, ten))\n",
    "\n",
    "        used.add(j)\n",
    "        used.add(j+1)\n",
    "\n",
    "    if out:\n",
    "        return pd.concat(out, ignore_index=True)\n",
    "\n",
    "    return pd.DataFrame(columns=[\"date\",\"ccy\",\"tenor_y\",\"par_swap\"])\n",
    "\n",
    "# Collect files across likely directories\n",
    "em_files = []\n",
    "for d in EM_CURVE_DIRS:\n",
    "    if d.exists():\n",
    "        for g in EM_CURVE_GLOBS:\n",
    "            em_files += list(d.glob(g))\n",
    "em_files = sorted({p.resolve() for p in em_files if p.is_file()}, key=lambda p: (p.stat().st_mtime, p.name), reverse=True)\n",
    "\n",
    "if not em_files:\n",
    "    raise FileNotFoundError(f\"No EM curve CSVs found. Searched dirs={EM_CURVE_DIRS} globs={EM_CURVE_GLOBS}\")\n",
    "\n",
    "parts = []\n",
    "failed = []\n",
    "for fp in em_files:\n",
    "    tmp = _parse_em_curve_file(fp)\n",
    "    if tmp is None or tmp.empty:\n",
    "        failed.append(fp)\n",
    "        continue\n",
    "    parts.append(tmp)\n",
    "\n",
    "if not parts:\n",
    "    # Print one file sample to make debugging deterministic\n",
    "    samp = failed[0]\n",
    "    raw_preview = pd.read_csv(samp, header=None, nrows=12, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    raise RuntimeError(\n",
    "        \"Failed to parse any EM curve files.\\n\"\n",
    "        f\"First file tried: {samp}\\n\"\n",
    "        f\"Preview (first 12 rows):\\n{raw_preview}\"\n",
    "    )\n",
    "\n",
    "curves_long = pd.concat(parts, ignore_index=True)\n",
    "curves_long[\"date\"] = pd.to_datetime(curves_long[\"date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "curves_long[\"ccy\"] = curves_long[\"ccy\"].astype(str).str.upper().str.strip()\n",
    "curves_long[\"par_swap\"] = pd.to_numeric(curves_long[\"par_swap\"], errors=\"coerce\")\n",
    "curves_long = curves_long.dropna(subset=[\"date\",\"ccy\",\"tenor_y\",\"par_swap\"])\n",
    "curves_long = curves_long[curves_long[\"ccy\"].isin(EM)]\n",
    "curves_long = curves_long[(curves_long[\"tenor_y\"] > 0) & (curves_long[\"tenor_y\"] <= 30)]\n",
    "if curves_long[\"par_swap\"].median() > 1.0:\n",
    "    curves_long[\"par_swap\"] = curves_long[\"par_swap\"] / 100.0\n",
    "curves_long = curves_long.groupby([\"date\",\"ccy\",\"tenor_y\"], as_index=False)[\"par_swap\"].mean()\n",
    "\n",
    "print(\"Parsed EM curves:\",\n",
    "      \"| files_used\", len(parts),\n",
    "      \"| rows\", len(curves_long),\n",
    "      \"| ccy_counts\", curves_long[\"ccy\"].value_counts().to_dict(),\n",
    "      \"| tenor_range\", (float(curves_long[\"tenor_y\"].min()), float(curves_long[\"tenor_y\"].max())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e9910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP5 rebuilt from BoE canonical long: 2009-01-02 → 2026-02-12 | missing_frac_daily= 0.0 | min/median/max= 2.358735926661e-05 0.00482294687585399 0.056929643311827115\n"
     ]
    }
   ],
   "source": [
    "def _tenor_to_years(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)): \n",
    "        return np.nan\n",
    "    s = str(x).strip().upper()\n",
    "    m = re.match(r\"^(\\d+(\\.\\d+)?)(Y|YR|YEARS?)$\", s)\n",
    "    if m: return float(m.group(1))\n",
    "    m = re.match(r\"^(\\d+(\\.\\d+)?)(M|MO|MONTHS?)$\", s)\n",
    "    if m: return float(m.group(1))/12.0\n",
    "    m = re.match(r\"^(\\d+(\\.\\d+)?)(W|WK|WEEKS?)$\", s)\n",
    "    if m: return float(m.group(1))/52.0\n",
    "    m = re.match(r\"^(\\d+(\\.\\d+)?)(D|DAY|DAYS?)$\", s)\n",
    "    if m: return float(m.group(1))/365.0\n",
    "    try: \n",
    "        return float(s)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def boe_to_long(boe_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    b = boe_df.copy()\n",
    "\n",
    "    # Try identify date col\n",
    "    date_col = next((c for c in b.columns if str(c).lower() in {\"date\",\"dt\",\"asof\",\"obs_date\",\"observation_date\"}), None)\n",
    "    if date_col is not None:\n",
    "        b[date_col] = pd.to_datetime(b[date_col], errors=\"coerce\")\n",
    "    else:\n",
    "        # Sometimes BoE raw has no explicit date: try index or __source_file mapping\n",
    "        if isinstance(b.index, pd.DatetimeIndex):\n",
    "            b = b.reset_index().rename(columns={\"index\":\"date\"})\n",
    "            date_col = \"date\"\n",
    "        else:\n",
    "            date_col = None\n",
    "\n",
    "    # Try long format (tenor/rate columns exist)\n",
    "    tenor_col = next((c for c in b.columns if str(c).lower() in {\"tenor\",\"maturity\",\"term\",\"pillar\"}), None)\n",
    "    rate_col  = next((c for c in b.columns if str(c).lower() in {\"rate\",\"value\",\"ois\",\"spot\",\"par\",\"swap\",\"uk ois spot curve\"}), None)\n",
    "\n",
    "    if date_col and tenor_col and rate_col:\n",
    "        out = b[[date_col, tenor_col, rate_col]].rename(columns={date_col:\"date\", tenor_col:\"tenor\", rate_col:\"rate\"})\n",
    "        out[\"tenor_y\"] = out[\"tenor\"].map(_tenor_to_years)\n",
    "        out[\"rate\"] = pd.to_numeric(out[\"rate\"], errors=\"coerce\")\n",
    "        out = out.dropna(subset=[\"date\",\"tenor_y\",\"rate\"])\n",
    "        return out[[\"date\",\"tenor_y\",\"rate\"]].sort_values([\"date\",\"tenor_y\"]).reset_index(drop=True)\n",
    "\n",
    "    # Try wide format (date + tenor-like columns)\n",
    "    if date_col:\n",
    "        tenor_like = []\n",
    "        for c in b.columns:\n",
    "            if c == date_col: \n",
    "                continue\n",
    "            ty = _tenor_to_years(c)\n",
    "            if np.isfinite(ty):\n",
    "                tenor_like.append((c, ty))\n",
    "        if tenor_like:\n",
    "            cols = [c for c,_ in tenor_like]\n",
    "            long = b[[date_col] + cols].melt(id_vars=[date_col], var_name=\"tenor\", value_name=\"rate\")\n",
    "            long[\"tenor_y\"] = long[\"tenor\"].map(_tenor_to_years)\n",
    "            long[\"rate\"] = pd.to_numeric(long[\"rate\"], errors=\"coerce\")\n",
    "            out = long.rename(columns={date_col:\"date\"}).dropna(subset=[\"date\",\"tenor_y\",\"rate\"])\n",
    "            return out[[\"date\",\"tenor_y\",\"rate\"]].sort_values([\"date\",\"tenor_y\"]).reset_index(drop=True)\n",
    "\n",
    "    # Fallback: if the loader already built gbp5 (daily series), store as long at tenor=5\n",
    "    if \"gbp5\" in globals() and isinstance(globals()[\"gbp5\"], pd.Series) and isinstance(globals()[\"gbp5\"].index, pd.DatetimeIndex):\n",
    "        s = globals()[\"gbp5\"].dropna().copy()\n",
    "        out = pd.DataFrame({\"date\": s.index, \"tenor_y\": 5.0, \"rate\": s.values})\n",
    "        return out.sort_values([\"date\",\"tenor_y\"]).reset_index(drop=True)\n",
    "\n",
    "    raise RuntimeError(\"Could not canonicalize BoE OIS data into (date, tenor, rate). Inspect boe_df structure.\")\n",
    "\n",
    "# Use already-loaded boe if present; otherwise reload via boe_fp/find_one\n",
    "if \"boe\" in globals() and isinstance(boe, pd.DataFrame):\n",
    "    boe_df = boe\n",
    "else:\n",
    "    boe_fp = globals().get(\"boe_fp\", None)\n",
    "    if boe_fp is None:\n",
    "        boe_fp = find_one(BASE/'data_clean', ['*boe*ois*.parquet','*ois*daily*raw*.parquet'], 'gbp_curve_raw')\n",
    "    boe_df = pd.read_parquet(boe_fp)\n",
    "\n",
    "boe_long = boe_to_long(boe_df)\n",
    "\n",
    "# Normalize % vs decimal\n",
    "med = boe_long[\"rate\"].median()\n",
    "if np.isfinite(med) and med > 1.0:\n",
    "    boe_long[\"rate\"] = boe_long[\"rate\"] / 100.0\n",
    "\n",
    "# Save canonical long (for audit)\n",
    "write_csv_new(boe_long, OUT/'boe_ois_long.csv', index=False)\n",
    "\n",
    "# Select 5Y explicitly\n",
    "boe_long[\"is5\"] = np.isclose(boe_long[\"tenor_y\"].astype(float), 5.0, atol=1e-6)\n",
    "boe_5y = boe_long.loc[boe_long[\"is5\"], [\"date\",\"rate\"]].drop_duplicates(\"date\").sort_values(\"date\")\n",
    "if boe_5y.empty:\n",
    "    raise RuntimeError(\"No 5Y tenor found in BoE long data. Check outputs/boe_ois_long.csv\")\n",
    "\n",
    "gbp5 = pd.Series(boe_5y[\"rate\"].values, index=pd.to_datetime(boe_5y[\"date\"]).dt.tz_localize(None), name=\"s5_fund\").sort_index()\n",
    "gbp5 = gbp5[~gbp5.index.duplicated(keep=\"last\")].dropna()\n",
    "\n",
    "# Save series\n",
    "write_csv_new(gbp5.reset_index().rename(columns={\"index\":\"date\", \"s5_fund\":\"gbp5_fund\"}), OUT/'gbp5_funding_series.csv', index=False)\n",
    "\n",
    "print(\"GBP5 rebuilt from BoE canonical long:\", gbp5.index.min().date(), \"→\", gbp5.index.max().date(),\n",
    "      \"| missing_frac_daily=\", float(gbp5.isna().mean()),\n",
    "      \"| min/median/max=\", float(gbp5.min()), float(gbp5.median()), float(gbp5.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5376f7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBP 5Y (BoE OIS spot curve): 2009-01-02 to 2026-01-30 | n= 4314 | median= 0.01212514346218281\n"
     ]
    }
   ],
   "source": [
    "# ---- Funding 5Y proxy (GBP): local BoE OIS XLSX files in data_clean/ ----\n",
    "# Uses only the pasted files:\n",
    "#   OIS daily data_2009 to 2015.xlsx\n",
    "#   OIS daily data_2016 to 2024.xlsx\n",
    "#   OIS daily data_2025 to present.xlsx\n",
    "# Extracts the 5Y column from the \"spot curve\" sheet and returns gbp_s5 (daily, decimal).\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Use your existing DATA_CLEAN if defined earlier; otherwise default:\n",
    "try:\n",
    "    DATA_CLEAN\n",
    "except NameError:\n",
    "    DATA_CLEAN = Path(\"data_clean\")\n",
    "\n",
    "# Find the daily BoE curve workbooks (handle underscores/spaces)\n",
    "boe_daily_xlsx = sorted({*DATA_CLEAN.glob(\"OIS daily data*.xlsx\"), *DATA_CLEAN.glob(\"OIS daily data_*.xlsx\")})\n",
    "if not boe_daily_xlsx:\n",
    "    raise RuntimeError(f\"No 'OIS daily data*.xlsx' files found in {DATA_CLEAN.resolve()}\")\n",
    "\n",
    "def _pick_spot_sheet(xlsx_path: Path) -> str:\n",
    "    xf = pd.ExcelFile(xlsx_path)\n",
    "    spot = [s for s in xf.sheet_names if \"spot curve\" in s.lower()]\n",
    "    if not spot:\n",
    "        raise RuntimeError(f\"No spot-curve sheet in {xlsx_path.name}. Sheets={xf.sheet_names}\")\n",
    "    # prefer the most detailed spot curve (usually last / '4. spot curve')\n",
    "    spot = sorted(spot, key=lambda s: (len(s), s))\n",
    "    return spot[-1]\n",
    "\n",
    "def _extract_spot_5y_from_file(xlsx_path: Path) -> pd.Series:\n",
    "    sheet = _pick_spot_sheet(xlsx_path)\n",
    "    raw = pd.read_excel(xlsx_path, sheet_name=sheet, header=None)\n",
    "\n",
    "    # locate the \"years:\" header row\n",
    "    c0 = raw.iloc[:, 0].astype(str).str.strip().str.lower()\n",
    "    years_rows = c0[c0.str.match(r\"^years\\b\")].index.to_list()\n",
    "    if not years_rows:\n",
    "        raise RuntimeError(f\"Could not find 'years:' header row in {xlsx_path.name}::{sheet}\")\n",
    "    yrow = years_rows[0]\n",
    "\n",
    "    # locate the column whose years value is ~5.0\n",
    "    hdr = raw.iloc[yrow, :]\n",
    "    col5 = None\n",
    "    for j, v in hdr.items():\n",
    "        if isinstance(v, (int, float, np.floating)) and np.isfinite(v) and abs(float(v) - 5.0) < 1e-2:\n",
    "            col5 = j\n",
    "            break\n",
    "    if col5 is None:\n",
    "        # common in 2009–2015 file: 4.999999999999999\n",
    "        for j, v in hdr.items():\n",
    "            if isinstance(v, (int, float, np.floating)) and np.isfinite(v) and abs(float(v) - 5.0) < 5e-2:\n",
    "                col5 = j\n",
    "                break\n",
    "    if col5 is None:\n",
    "        raise RuntimeError(f\"No ~5Y pillar found in {xlsx_path.name}::{sheet}\")\n",
    "\n",
    "    # data begins below the header rows; first col is date\n",
    "    dates = pd.to_datetime(raw.iloc[yrow + 1 :, 0], errors=\"coerce\")\n",
    "    rates = pd.to_numeric(raw.iloc[yrow + 1 :, col5], errors=\"coerce\")\n",
    "\n",
    "    s = pd.Series(rates.to_numpy(), index=dates, name=\"s5_fund\").dropna()\n",
    "    s.index = pd.to_datetime(s.index, errors=\"coerce\").tz_localize(None)\n",
    "    s = s[~s.index.duplicated(keep=\"last\")].sort_index()\n",
    "    return s\n",
    "\n",
    "parts, failed = [], []\n",
    "for fp in boe_daily_xlsx:\n",
    "    try:\n",
    "        parts.append(_extract_spot_5y_from_file(fp))\n",
    "    except Exception as e:\n",
    "        failed.append((fp.name, str(e)))\n",
    "\n",
    "if not parts:\n",
    "    msg = \"Failed to extract GBP 5Y from all BoE daily XLSX files.\\n\" + \"\\n\".join([f\"- {n}: {err}\" for n, err in failed])\n",
    "    raise RuntimeError(msg)\n",
    "\n",
    "gbp_s5 = pd.concat(parts).sort_index()\n",
    "gbp_s5 = gbp_s5[~gbp_s5.index.duplicated(keep=\"last\")]\n",
    "\n",
    "# Convert percent -> decimal (BoE sheets are in percent points, e.g. 0.48 = 0.48%)\n",
    "if gbp_s5.median() > 1.0 or gbp_s5.quantile(0.95) > 1.0:\n",
    "    gbp_s5 = gbp_s5 / 100.0\n",
    "\n",
    "gbp_s5.name = \"s5_fund\"\n",
    "\n",
    "print(\n",
    "    \"GBP 5Y (BoE OIS spot curve):\",\n",
    "    gbp_s5.index.min().date(), \"to\", gbp_s5.index.max().date(),\n",
    "    \"| n=\", len(gbp_s5),\n",
    "    \"| median=\", float(gbp_s5.median()),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d248fc",
   "metadata": {},
   "source": [
    "WEEKLY ALIGNMENT (W-WED) + CURVE DENSIFICATION\n",
    "- FX/OIS aligned to Wednesday within ±2 days (holiday tolerance).\n",
    "- Curves densified to business days via time interpolation (audit), then sampled weekly.\n",
    "Produces:\n",
    "  weekly_targets\n",
    "  ois_w, gbp5_w : pd.Series weekly\n",
    "  fx_w          : pd.DataFrame weekly (USD per CCY)\n",
    "  curve_w       : dict[ccy] -> pd.DataFrame weekly, columns=tenor_y\n",
    "  align_audit, staleness_summary, interp_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly panel: | weeks 366 | start 2019-01-02 | end 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "MAX_NEAR_DAYS = 2                 # FX/OIS holiday tolerance\n",
    "TIME_INTERP_MAX_GAP_DAYS = 45     # max consecutive business-day NaNs to interpolate\n",
    "WEEKLY_CURVE_MAX_STALE_DAYS = 7   # max staleness for curve sampling after densification\n",
    "\n",
    "def _nearest_within(idx: pd.DatetimeIndex, d: pd.Timestamp, max_days: int):\n",
    "    if len(idx) == 0:\n",
    "        return None\n",
    "    pos = idx.get_indexer([d], method=\"nearest\")[0]\n",
    "    if pos < 0:\n",
    "        return None\n",
    "    dd = idx[pos]\n",
    "    return dd if abs((dd - d).days) <= max_days else None\n",
    "\n",
    "def align_series_weekly_nearest(s: pd.Series, targets: pd.DatetimeIndex, max_days: int):\n",
    "    s = s.dropna().sort_index()\n",
    "    idx = s.index\n",
    "    out, used = [], []\n",
    "    for d in targets:\n",
    "        dd = _nearest_within(idx, d, max_days)\n",
    "        out.append(np.nan if dd is None else float(s.loc[dd]))\n",
    "        used.append(pd.NaT if dd is None else dd)\n",
    "    return pd.Series(out, index=targets, name=s.name), pd.Series(used, index=targets, name=f\"{s.name}_used_date\")\n",
    "\n",
    "def densify_curves_to_bdays(curves_long: pd.DataFrame):\n",
    "    curves = curves_long.copy()\n",
    "    curves[\"date\"] = pd.to_datetime(curves[\"date\"]).dt.tz_localize(None)\n",
    "    curves = curves.sort_values([\"ccy\",\"tenor_y\",\"date\"])\n",
    "\n",
    "    d0, d1 = curves[\"date\"].min(), curves[\"date\"].max()\n",
    "    cal = pd.date_range(d0, d1, freq=\"B\")\n",
    "\n",
    "    audit, parts = [], []\n",
    "    for (ccy, ten), g in curves.groupby([\"ccy\",\"tenor_y\"]):\n",
    "        s = pd.Series(g[\"par_swap\"].to_numpy(), index=g[\"date\"]).sort_index()\n",
    "        s = s[~s.index.duplicated(keep=\"last\")]\n",
    "        s2 = s.reindex(cal)\n",
    "        s3 = s2.interpolate(method=\"time\", limit=TIME_INTERP_MAX_GAP_DAYS, limit_area=\"inside\")\n",
    "\n",
    "        exact = s3.index.isin(s.index) & s3.notna()\n",
    "        audit.append({\n",
    "            \"ccy\": ccy,\n",
    "            \"tenor_y\": float(ten),\n",
    "            \"n_obs_raw\": int(s.notna().sum()),\n",
    "            \"n_bdays\": int(s3.notna().sum()),\n",
    "            \"interp_share\": float((s3.notna().sum() - exact.sum()) / max(int(s3.notna().sum()), 1)),\n",
    "            \"max_gap_days_raw\": int(pd.Series(s.index).diff().dt.days.max()) if len(s.index) > 1 else 0,\n",
    "        })\n",
    "\n",
    "        parts.append(pd.DataFrame({\"date\": s3.index, \"ccy\": ccy, \"tenor_y\": float(ten), \"par_swap\": s3.values}))\n",
    "\n",
    "    dense = pd.concat(parts, ignore_index=True).dropna(subset=[\"par_swap\"])\n",
    "    curve_panel_b = dense.pivot_table(index=[\"date\",\"ccy\"], columns=\"tenor_y\", values=\"par_swap\", aggfunc=\"mean\").sort_index()\n",
    "    return curve_panel_b, pd.DataFrame(audit)\n",
    "\n",
    "curve_panel_b, interp_audit = densify_curves_to_bdays(curves_long)\n",
    "\n",
    "curve_min = curve_panel_b.index.get_level_values(\"date\").min()\n",
    "curve_max = curve_panel_b.index.get_level_values(\"date\").max()\n",
    "start = max(ois_on.index.min(), gbp_s5.index.min(), fx_usd_per_ccy.index.min(), curve_min)\n",
    "end   = min(ois_on.index.max(), gbp_s5.index.max(), fx_usd_per_ccy.index.max(), curve_max)\n",
    "\n",
    "weekly_targets = pd.date_range(start, end, freq=\"W-WED\")\n",
    "if len(weekly_targets) < 150:\n",
    "    raise RuntimeError(f\"Weekly sample too short: n={len(weekly_targets)}\")\n",
    "\n",
    "ois_w, ois_used = align_series_weekly_nearest(ois_on, weekly_targets, MAX_NEAR_DAYS)\n",
    "gbp5_w, gbp5_used = align_series_weekly_nearest(gbp_s5, weekly_targets, MAX_NEAR_DAYS)\n",
    "\n",
    "fx_w = pd.DataFrame(index=weekly_targets)\n",
    "fx_used = {}\n",
    "for c in sorted(set(EM + [FUND])):\n",
    "    fx_w[c], fx_used[c] = align_series_weekly_nearest(fx_usd_per_ccy[c], weekly_targets, MAX_NEAR_DAYS)\n",
    "\n",
    "def align_curve_weekly(ccy: str, targets: pd.DatetimeIndex, max_stale_days: int):\n",
    "    panel = curve_panel_b.xs(ccy, level=\"ccy\", drop_level=False)\n",
    "    idx = panel.index.get_level_values(\"date\").unique().sort_values()\n",
    "    rows, used = [], []\n",
    "    for d in targets:\n",
    "        dd = _nearest_within(idx, d, max_stale_days)\n",
    "        if dd is None:\n",
    "            rows.append(pd.Series(np.nan, index=curve_panel_b.columns))\n",
    "            used.append(pd.NaT)\n",
    "        else:\n",
    "            rows.append(panel.xs(dd, level=\"date\").iloc[0])\n",
    "            used.append(dd)\n",
    "    out = pd.DataFrame(rows, index=targets, columns=curve_panel_b.columns)\n",
    "    return out, pd.Series(used, index=targets, name=f\"{ccy}_curve_used_date\")\n",
    "\n",
    "curve_w, curve_used = {}, {}\n",
    "for c in EM:\n",
    "    curve_w[c], curve_used[c] = align_curve_weekly(c, weekly_targets, WEEKLY_CURVE_MAX_STALE_DAYS)\n",
    "\n",
    "align_audit = pd.DataFrame({\"date\": weekly_targets})\n",
    "align_audit[\"ois_used_date\"] = ois_used.values\n",
    "align_audit[\"gbp5_used_date\"] = gbp5_used.values\n",
    "for c in sorted(set(EM + [FUND])):\n",
    "    align_audit[f\"fx_{c}_used_date\"] = fx_used[c].values\n",
    "for c in EM:\n",
    "    align_audit[f\"curve_{c}_used_date\"] = curve_used[c].values\n",
    "\n",
    "def _lag_days(used: pd.Series, tgt: pd.Series) -> pd.Series:\n",
    "    return (pd.to_datetime(tgt) - pd.to_datetime(used)).dt.days\n",
    "\n",
    "stale_rows = []\n",
    "for ucol in [c for c in align_audit.columns if c.endswith(\"_used_date\")]:\n",
    "    lag = _lag_days(align_audit[ucol], align_audit[\"date\"])\n",
    "    stale_rows.append({\n",
    "        \"series\": ucol.replace(\"_used_date\",\"\"),\n",
    "        \"n_weeks\": int(lag.notna().sum()),\n",
    "        \"mean_lag_days\": float(lag.mean()),\n",
    "        \"p95_lag_days\": float(lag.quantile(0.95)) if lag.notna().any() else np.nan,\n",
    "        \"max_lag_days\": float(lag.max()) if lag.notna().any() else np.nan,\n",
    "    })\n",
    "staleness_summary = pd.DataFrame(stale_rows).sort_values(\"series\").reset_index(drop=True)\n",
    "\n",
    "print(\"Weekly panel:\",\n",
    "      \"| weeks\", len(weekly_targets),\n",
    "      \"| start\", weekly_targets.min().date(),\n",
    "      \"| end\", weekly_targets.max().date())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88164c83",
   "metadata": {},
   "source": [
    "PRICING + SIMULATION (faithful to spec + guardrails + decomposition)\n",
    "- quarterly coupon bond, coupon fixed at entry s5_lend\n",
    "- convert all flows to USD; compute equity return and notional return\n",
    "- enforce \"use at least 1Y and 5Y points\": require tenor range covers [1,5] at entry AND exit\n",
    "- audit: bootstrap conventions + entry par check\n",
    "Produces:\n",
    "  trades : per-ccy-week panel\n",
    "  wk     : weekly portfolio panel (variable-capital + fixed-allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: | weeks 365 | avg_active 1.7671232876712328 | min_dd_equity_fixed -0.8420760158312669\n"
     ]
    }
   ],
   "source": [
    "NOTIONAL_USD = 10_000_000.0\n",
    "LEVERAGE = 5.0\n",
    "BORROW_USD = NOTIONAL_USD * (LEVERAGE - 1.0) / LEVERAGE   # 8mm\n",
    "EQUITY_USD = NOTIONAL_USD / LEVERAGE                       # 2mm\n",
    "DT = 1.0 / 52.0\n",
    "COUPON_FREQ = 4\n",
    "SPREAD_BPS = 50.0\n",
    "SPREAD = SPREAD_BPS / 10_000.0\n",
    "\n",
    "times_entry = np.arange(1/COUPON_FREQ, 5.0 + 1e-12, 1/COUPON_FREQ)\n",
    "times_exit = times_entry - DT\n",
    "times_exit = times_exit[times_exit > 0]\n",
    "\n",
    "def _interp_linear(x: np.ndarray, y: np.ndarray, t: float) -> float:\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x, y = x[m], y[m]\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    o = np.argsort(x); x, y = x[o], y[o]\n",
    "    return float(np.interp(t, x, y, left=np.nan, right=np.nan))\n",
    "\n",
    "def _require_1y_5y(curve: pd.Series) -> bool:\n",
    "    if curve is None or curve.dropna().size < 2:\n",
    "        return False\n",
    "    ten = curve.dropna().index.astype(float).to_numpy()\n",
    "    lo, hi = float(np.nanmin(ten)), float(np.nanmax(ten))\n",
    "    return (lo <= 1.0) and (hi >= 5.0)\n",
    "\n",
    "def _interp_par_with_short_end(tenors: np.ndarray, rates: np.ndarray, t: float):\n",
    "    x = np.asarray(tenors, float); y = np.asarray(rates, float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x, y = x[m], y[m]\n",
    "    if x.size < 2:\n",
    "        return np.nan, {\"short_used\": 0, \"long_forbidden\": 1}\n",
    "    o = np.argsort(x); x, y = x[o], y[o]\n",
    "    lo, hi = float(x[0]), float(x[-1])\n",
    "    if t < lo:\n",
    "        return float(y[0]), {\"short_used\": 1, \"long_forbidden\": 0}\n",
    "    if t > hi:\n",
    "        return np.nan, {\"short_used\": 0, \"long_forbidden\": 1}\n",
    "    return float(np.interp(t, x, y)), {\"short_used\": 0, \"long_forbidden\": 0}\n",
    "\n",
    "def bootstrap_df(par_curve: pd.Series, max_t: float, freq: int = 4):\n",
    "    grid = np.arange(1/freq, max_t + 1e-12, 1/freq)\n",
    "    par_curve = par_curve.dropna()\n",
    "    ten = par_curve.index.astype(float).to_numpy()\n",
    "    rat = par_curve.astype(float).to_numpy()\n",
    "    m = np.isfinite(ten) & np.isfinite(rat)\n",
    "    ten, rat = ten[m], rat[m]\n",
    "    if ten.size < 2:\n",
    "        return pd.Series(dtype=float), {\"short_used_ct\": 0, \"long_forbidden_ct\": len(grid), \"min_tenor\": np.nan, \"max_tenor\": np.nan}\n",
    "    o = np.argsort(ten); ten, rat = ten[o], rat[o]\n",
    "    audit = {\"short_used_ct\": 0, \"long_forbidden_ct\": 0, \"min_tenor\": float(ten[0]), \"max_tenor\": float(ten[-1])}\n",
    "\n",
    "    dfs = {}\n",
    "    for t in grid:\n",
    "        s, a = _interp_par_with_short_end(ten, rat, float(t))\n",
    "        audit[\"short_used_ct\"] += a[\"short_used\"]\n",
    "        audit[\"long_forbidden_ct\"] += a[\"long_forbidden\"]\n",
    "        if not np.isfinite(s):\n",
    "            return pd.Series(dtype=float), audit\n",
    "        c = s / freq\n",
    "        prev = sum((c * dfs[pt]) for pt in grid if pt < t)\n",
    "        dfs[t] = max((1 - prev) / (1 + c), 1e-12)\n",
    "\n",
    "    return pd.Series(dfs), audit\n",
    "\n",
    "def price_bond(coupon: float, curve_row: pd.Series, times: np.ndarray, freq: int = 4):\n",
    "    if times.size == 0:\n",
    "        return np.nan, {\"short_used_ct\": 0, \"long_forbidden_ct\": 1, \"min_tenor\": np.nan, \"max_tenor\": np.nan}\n",
    "    max_t = max(5.0, float(np.max(times)) + 0.25)\n",
    "    z, aud = bootstrap_df(curve_row, max_t=max_t, freq=freq)\n",
    "    if z.empty:\n",
    "        return np.nan, aud\n",
    "    df = np.interp(times, z.index.values, z.values, left=z.values[0], right=z.values[-1])\n",
    "    if np.isnan(df).any():\n",
    "        return np.nan, aud\n",
    "    cf = np.full(times.size, coupon/freq)\n",
    "    cf[-1] += 1.0\n",
    "    return float(np.sum(cf * df)), aud\n",
    "\n",
    "rows, wk_rows, par_check, audit_rows = [], [], [], []\n",
    "\n",
    "for i in range(len(weekly_targets) - 1):\n",
    "    t0, t1 = weekly_targets[i], weekly_targets[i+1]\n",
    "\n",
    "    s5_fund = float(gbp5_w.loc[t0]) if np.isfinite(gbp5_w.loc[t0]) else np.nan\n",
    "    ois0 = float(ois_w.loc[t0]) if np.isfinite(ois_w.loc[t0]) else np.nan\n",
    "\n",
    "    active_rets_equity, active_rets_notional, active_pnls = [], [], []\n",
    "    n_avail, n_active = 0, 0\n",
    "\n",
    "    for c in EM:\n",
    "        fx0 = float(fx_w.at[t0, c]) if np.isfinite(fx_w.at[t0, c]) else np.nan\n",
    "        fx1 = float(fx_w.at[t1, c]) if np.isfinite(fx_w.at[t1, c]) else np.nan\n",
    "        g0  = float(fx_w.at[t0, FUND]) if np.isfinite(fx_w.at[t0, FUND]) else np.nan\n",
    "        g1  = float(fx_w.at[t1, FUND]) if np.isfinite(fx_w.at[t1, FUND]) else np.nan\n",
    "\n",
    "        c0 = curve_w[c].loc[t0].dropna()\n",
    "        c1 = curve_w[c].loc[t1].dropna()\n",
    "\n",
    "        available, reason = True, None\n",
    "        if not np.isfinite([fx0, fx1, g0, g1, s5_fund, ois0]).all():\n",
    "            available, reason = False, \"missing_fx_or_funding\"\n",
    "        if available and (c0.empty or c1.empty):\n",
    "            available, reason = False, \"missing_curve\"\n",
    "        if available and (not _require_1y_5y(c0) or not _require_1y_5y(c1)):\n",
    "            available, reason = False, \"no_1y_5y_coverage\"\n",
    "\n",
    "        s5_lend = np.nan\n",
    "        spread = np.nan\n",
    "        active = False\n",
    "        ret_equity = np.nan\n",
    "        ret_notional = np.nan\n",
    "        pnl_usd = 0.0\n",
    "\n",
    "        lend_pnl_usd = np.nan\n",
    "        lend_pnl_rates = np.nan\n",
    "        lend_pnl_fx = np.nan\n",
    "        debt_cost_usd = np.nan\n",
    "        debt_cost_interest = np.nan\n",
    "        debt_cost_fx = np.nan\n",
    "\n",
    "        if available:\n",
    "            n_avail += 1\n",
    "            ten0 = c0.index.astype(float).to_numpy()\n",
    "            rat0 = c0.astype(float).to_numpy()\n",
    "            s5_lend = _interp_linear(ten0, rat0, 5.0)\n",
    "            if not np.isfinite(s5_lend):\n",
    "                available, reason = False, \"s5_nan\"\n",
    "            else:\n",
    "                spread = s5_lend - s5_fund\n",
    "                active = bool(np.isfinite(spread) and spread >= SPREAD)\n",
    "\n",
    "        if available and active:\n",
    "            pv0, aud0 = price_bond(s5_lend, c0, times_entry, freq=COUPON_FREQ)\n",
    "            par_check.append({\"date\": t0, \"ccy\": c, \"pv_entry\": pv0, \"abs_err\": abs(pv0 - 1.0) if np.isfinite(pv0) else np.nan,\n",
    "                              \"min_tenor\": aud0.get(\"min_tenor\"), \"max_tenor\": aud0.get(\"max_tenor\")})\n",
    "            if not (np.isfinite(pv0) and abs(pv0 - 1.0) <= 5e-3):\n",
    "                available, active, reason = False, False, \"entry_par_check_failed\"\n",
    "\n",
    "        if available and active:\n",
    "            pv1, aud1 = price_bond(s5_lend, c1, times_exit, freq=COUPON_FREQ)\n",
    "            audit_rows.append({\"date\": t0, \"ccy\": c, **aud1})\n",
    "            if not np.isfinite(pv1):\n",
    "                available, active, reason = False, False, \"pricing_nan\"\n",
    "\n",
    "        if available and active:\n",
    "            face_ccy = NOTIONAL_USD / fx0\n",
    "            lend_usd_exit = face_ccy * pv1 * fx1\n",
    "            lend_pnl_usd = lend_usd_exit - NOTIONAL_USD\n",
    "\n",
    "            lend_usd_exit_fx0 = face_ccy * pv1 * fx0\n",
    "            lend_pnl_rates = lend_usd_exit_fx0 - NOTIONAL_USD\n",
    "            lend_pnl_fx = lend_usd_exit - lend_usd_exit_fx0\n",
    "\n",
    "            debt_gbp = BORROW_USD / g0\n",
    "            repay_gbp = debt_gbp * (1.0 + (ois0 + SPREAD) * DT)\n",
    "            repay_usd = repay_gbp * g1\n",
    "\n",
    "            debt_cost_usd = repay_usd - BORROW_USD\n",
    "            repay_usd_fx0 = repay_gbp * g0\n",
    "            debt_cost_interest = repay_usd_fx0 - BORROW_USD\n",
    "            debt_cost_fx = repay_usd - repay_usd_fx0\n",
    "\n",
    "            eq_exit = lend_usd_exit - repay_usd\n",
    "            pnl_usd = eq_exit - EQUITY_USD\n",
    "            ret_equity = max(pnl_usd / EQUITY_USD, -0.999999)\n",
    "            ret_notional = pnl_usd / NOTIONAL_USD\n",
    "\n",
    "            n_active += 1\n",
    "            active_rets_equity.append(ret_equity)\n",
    "            active_rets_notional.append(ret_notional)\n",
    "            active_pnls.append(pnl_usd)\n",
    "\n",
    "        rows.append({\n",
    "            \"date\": t0, \"next_date\": t1, \"ccy\": c,\n",
    "            \"available\": int(available), \"active\": int(active),\n",
    "            \"reason_unavailable\": reason,\n",
    "            \"s5_lend\": s5_lend, \"s5_fund\": s5_fund, \"spread\": spread,\n",
    "            \"ret_equity\": ret_equity, \"ret_notional\": ret_notional,\n",
    "            \"pnl_usd\": pnl_usd,\n",
    "            \"lend_pnl_usd\": lend_pnl_usd, \"lend_pnl_rates\": lend_pnl_rates, \"lend_pnl_fx\": lend_pnl_fx,\n",
    "            \"debt_cost_usd\": debt_cost_usd, \"debt_cost_interest\": debt_cost_interest, \"debt_cost_fx\": debt_cost_fx,\n",
    "        })\n",
    "\n",
    "    wk_rows.append({\n",
    "        \"date\": t0, \"next_date\": t1,\n",
    "        \"n_ccy_available\": int(n_avail), \"n_ccy_active\": int(n_active),\n",
    "        \"port_ret_equity\": float(np.mean(active_rets_equity)) if active_rets_equity else 0.0,\n",
    "        \"port_ret_notional\": float(np.mean(active_rets_notional)) if active_rets_notional else 0.0,\n",
    "        \"port_pnl_usd\": float(np.sum(active_pnls)) if active_pnls else 0.0,\n",
    "        \"port_equity_usd\": float(EQUITY_USD * n_active),\n",
    "    })\n",
    "\n",
    "trades = pd.DataFrame(rows).sort_values([\"date\",\"ccy\"]).reset_index(drop=True)\n",
    "wk = pd.DataFrame(wk_rows).set_index(\"date\").sort_index()\n",
    "\n",
    "# --- Ensure wk has portfolio return series (compute from trades if missing) ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# wk must be indexed by weekly dates; if it's not, make it so.\n",
    "if \"date\" in wk.columns and not isinstance(wk.index, pd.DatetimeIndex):\n",
    "    wk = wk.set_index(\"date\")\n",
    "wk.index = pd.to_datetime(wk.index, errors=\"coerce\").tz_localize(None)\n",
    "wk = wk.sort_index()\n",
    "\n",
    "# Wide matrix of per-ccy equity returns (NaN when not active)\n",
    "r_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"ret_equity\")\n",
    "r_wide.index = pd.to_datetime(r_wide.index, errors=\"coerce\").tz_localize(None)\n",
    "r_wide = r_wide.reindex(wk.index)\n",
    "\n",
    "# Active indicator (1/0)\n",
    "a_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"active\")\n",
    "a_wide.index = pd.to_datetime(a_wide.index, errors=\"coerce\").tz_localize(None)\n",
    "a_wide = a_wide.reindex(wk.index).fillna(0.0)\n",
    "\n",
    "# Variable-capital: average across ACTIVE positions each week (0 if none active)\n",
    "if \"port_ret_equity\" not in wk.columns:\n",
    "    denom = a_wide.sum(axis=1).replace(0.0, np.nan)\n",
    "    wk[\"port_ret_equity\"] = (r_wide.fillna(0.0) * a_wide).sum(axis=1) / denom\n",
    "    wk[\"port_ret_equity\"] = wk[\"port_ret_equity\"].fillna(0.0)\n",
    "\n",
    "# Fixed-allocation: equal weight across currencies each week, inactive treated as 0\n",
    "if \"port_ret_equity_fixed\" not in wk.columns:\n",
    "    wk[\"port_ret_equity_fixed\"] = r_wide.fillna(0.0).mean(axis=1)\n",
    "\n",
    "# Wealth/drawdown convenience (used by figures/report)\n",
    "if \"wealth_equity\" not in wk.columns:\n",
    "    wk[\"wealth_equity\"] = (1.0 + wk[\"port_ret_equity\"].fillna(0.0)).cumprod()\n",
    "if \"drawdown_equity\" not in wk.columns:\n",
    "    w_ = wk[\"wealth_equity\"]\n",
    "    wk[\"drawdown_equity\"] = w_ / w_.cummax() - 1.0\n",
    "\n",
    "if \"wealth_equity_fixed\" not in wk.columns:\n",
    "    wk[\"wealth_equity_fixed\"] = (1.0 + wk[\"port_ret_equity_fixed\"].fillna(0.0)).cumprod()\n",
    "if \"drawdown_equity_fixed\" not in wk.columns:\n",
    "    w_ = wk[\"wealth_equity_fixed\"]\n",
    "    wk[\"drawdown_equity_fixed\"] = w_ / w_.cummax() - 1.0\n",
    "\n",
    "\n",
    "wk[\"wealth_equity\"] = (1.0 + wk[\"port_ret_equity\"].fillna(0.0)).cumprod()\n",
    "wk[\"drawdown_equity\"] = wk[\"wealth_equity\"] / wk[\"wealth_equity\"].cummax() - 1.0\n",
    "\n",
    "wide_eq = trades.pivot(index=\"date\", columns=\"ccy\", values=\"ret_equity\").reindex(wk.index).fillna(0.0)\n",
    "wk[\"port_ret_equity_fixed\"] = wide_eq.mean(axis=1)\n",
    "wk[\"wealth_equity_fixed\"] = (1.0 + wk[\"port_ret_equity_fixed\"]).cumprod()\n",
    "wk[\"drawdown_equity_fixed\"] = wk[\"wealth_equity_fixed\"] / wk[\"wealth_equity_fixed\"].cummax() - 1.0\n",
    "\n",
    "print(\"Simulation:\",\n",
    "      \"| weeks\", len(wk),\n",
    "      \"| avg_active\", float(wk[\"n_ccy_active\"].mean()),\n",
    "      \"| min_dd_equity_fixed\", float(wk[\"drawdown_equity_fixed\"].min()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e6eb6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure weekly GBP 5Y series exists under name gbp5_w (used later in factor block) ---\n",
    "if \"gbp5_w\" not in locals():\n",
    "    if \"gbp_s5_w\" in locals():\n",
    "        gbp5_w = gbp_s5_w.rename(\"s5_fund\")\n",
    "    else:\n",
    "        # if you only have daily gbp_s5, create weekly\n",
    "        if \"gbp_s5\" in locals():\n",
    "            gbp5_w = gbp_s5.resample(\"W-WED\").last().reindex(wk.index).rename(\"s5_fund\")\n",
    "        else:\n",
    "            raise RuntimeError(\"Missing GBP 5Y series: expected gbp_s5_w or gbp_s5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445d10f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up existing outputs\\extrapolation_audit_raw.csv -> outputs\\extrapolation_audit_raw.csv.bak_20260217_234018\n",
      "Backed up existing outputs\\extrapolation_audit_summary.csv -> outputs\\extrapolation_audit_summary.csv.bak_20260217_234018\n",
      "Backed up existing outputs\\missing_drivers.csv -> outputs\\missing_drivers.csv.bak_20260217_234018\n",
      "NEW simulation complete: | weeks= 365 | avg_active_positions= 2.8109589041095893 | min_drawdown= -0.9953604737695458\n"
     ]
    }
   ],
   "source": [
    "def s5_strict(curve_series: pd.Series) -> float:\n",
    "    \"\"\"Return 5Y par rate only if 5.0 is within tenor range; otherwise NaN.\"\"\"\n",
    "    if curve_series is None or curve_series.dropna().empty:\n",
    "        return np.nan\n",
    "    x = curve_series.dropna().index.astype(float).to_numpy()\n",
    "    y = curve_series.dropna().astype(float).to_numpy()\n",
    "    o = np.argsort(x); x, y = x[o], y[o]\n",
    "    if (5.0 < x[0]) or (5.0 > x[-1]):\n",
    "        return np.nan\n",
    "    return float(np.interp(5.0, x, y))\n",
    "\n",
    "def interp_par_with_short_end(tenors: np.ndarray, rates: np.ndarray, t: float) -> tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    Par-rate interpolation with explicit conventions:\n",
    "      - t < min_tenor: flat at rate(min_tenor)  (short-end convention)\n",
    "      - t > max_tenor: NaN (forbid extrapolation)\n",
    "    Returns (rate, audit)\n",
    "    \"\"\"\n",
    "    x = np.asarray(tenors, float)\n",
    "    y = np.asarray(rates, float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    x, y = x[m], y[m]\n",
    "    if x.size < 2:\n",
    "        return np.nan, {\"short_used\": 0, \"long_forbidden\": 1}\n",
    "    o = np.argsort(x); x, y = x[o], y[o]\n",
    "    lo, hi = float(x[0]), float(x[-1])\n",
    "    if t < lo:\n",
    "        return float(y[0]), {\"short_used\": 1, \"long_forbidden\": 0}\n",
    "    if t > hi:\n",
    "        return np.nan, {\"short_used\": 0, \"long_forbidden\": 1}\n",
    "    return float(np.interp(t, x, y)), {\"short_used\": 0, \"long_forbidden\": 0}\n",
    "\n",
    "def bootstrap_df_explicit(par_curve: pd.Series, freq: int = 4, max_t: float = 5.0) -> tuple[pd.Series, dict]:\n",
    "    \"\"\"\n",
    "    Bootstrap discount factors on a quarterly grid up to max_t using par rates.\n",
    "    Uses explicit short-end convention and forbids long-end extrapolation.\n",
    "    Returns (df_series, audit_dict).\n",
    "    \"\"\"\n",
    "    grid = np.arange(1/freq, max_t + 1e-12, 1/freq)\n",
    "    dfs = {}\n",
    "    audit = {\"short_used_ct\": 0, \"long_forbidden_ct\": 0, \"min_tenor\": np.nan, \"max_tenor\": np.nan}\n",
    "\n",
    "    ten = par_curve.index.astype(float).to_numpy()\n",
    "    rat = par_curve.astype(float).to_numpy()\n",
    "    m = np.isfinite(ten) & np.isfinite(rat)\n",
    "    ten, rat = ten[m], rat[m]\n",
    "    if ten.size < 2:\n",
    "        return pd.Series(dtype=float), audit\n",
    "    o = np.argsort(ten); ten, rat = ten[o], rat[o]\n",
    "    audit[\"min_tenor\"], audit[\"max_tenor\"] = float(ten[0]), float(ten[-1])\n",
    "\n",
    "    for t in grid:\n",
    "        s, a = interp_par_with_short_end(ten, rat, float(t))\n",
    "        audit[\"short_used_ct\"] += a[\"short_used\"]\n",
    "        audit[\"long_forbidden_ct\"] += a[\"long_forbidden\"]\n",
    "        if not np.isfinite(s):\n",
    "            return pd.Series(dtype=float), audit\n",
    "        c = s / freq\n",
    "        prev = sum((c * dfs[pt]) for pt in grid if pt < t)\n",
    "        dfs[t] = max((1 - prev) / (1 + c), 1e-12)\n",
    "\n",
    "    return pd.Series(dfs), audit\n",
    "\n",
    "def price_bond_mtm(coupon: float, curve_row: pd.Series, times: np.ndarray, freq: int = 4) -> tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    Price a quarterly coupon bond with given remaining cashflow times.\n",
    "    Uses explicit bootstrap; returns (pv, audit).\n",
    "    \"\"\"\n",
    "    if times.size == 0:\n",
    "        return np.nan, {\"short_used_ct\": 0, \"long_forbidden_ct\": 1}\n",
    "    max_t = max(5.0, float(np.max(times)) + 0.25)\n",
    "    z, audit = bootstrap_df_explicit(curve_row.dropna(), freq=freq, max_t=max_t)\n",
    "    if z.empty:\n",
    "        return np.nan, audit\n",
    "    # DF interpolation: still forbid beyond max curve by z construction\n",
    "    df = np.interp(times, z.index.values, z.values, left=z.values[0], right=z.values[-1])\n",
    "    if np.isnan(df).any():\n",
    "        return np.nan, audit\n",
    "    cf = np.full(times.size, coupon/freq)\n",
    "    cf[-1] += 1.0\n",
    "    return float(np.sum(cf * df)), audit\n",
    "\n",
    "# Re-run strategy simulation using aligned weekly objects built earlier\n",
    "times_entry = np.arange(0.25, 5.0 + 1e-12, 0.25)\n",
    "dt = 1/52\n",
    "times_exit = (times_entry - dt)\n",
    "times_exit = times_exit[times_exit > 0]\n",
    "\n",
    "rows = []\n",
    "weekly = []\n",
    "audit_rows = []\n",
    "\n",
    "missing_driver = []\n",
    "\n",
    "for i in range(len(weekly_targets) - 1):\n",
    "    t0 = weekly_targets[i]\n",
    "    t1 = weekly_targets[i+1]\n",
    "\n",
    "    s5f = gbp5_w.loc[t0]\n",
    "    o = ois_w.loc[t0]\n",
    "\n",
    "    active = 0\n",
    "    avail = 0\n",
    "    rets = []\n",
    "\n",
    "    for c in EM:\n",
    "        fx0, fx1 = fx_w.at[t0, c], fx_w.at[t1, c]\n",
    "        g0, g1   = fx_w.at[t0, \"GBP\"], fx_w.at[t1, \"GBP\"]\n",
    "        c0 = curve_w[c].loc[t0].dropna()\n",
    "        c1 = curve_w[c].loc[t1].dropna()\n",
    "\n",
    "        ok = True\n",
    "        if not np.isfinite([fx0, fx1, g0, g1]).all():\n",
    "            ok = False; missing_driver.append((t0, c, \"fx_missing\"))\n",
    "        if c0.empty or c1.empty:\n",
    "            ok = False; missing_driver.append((t0, c, \"curve_missing\"))\n",
    "        if not np.isfinite(s5f):\n",
    "            ok = False; missing_driver.append((t0, c, \"gbp5_missing\"))\n",
    "        if not np.isfinite(o):\n",
    "            ok = False; missing_driver.append((t0, c, \"ois_missing\"))\n",
    "\n",
    "        ret = np.nan\n",
    "        pnl = 0.0\n",
    "        trade = False\n",
    "        s5l = np.nan\n",
    "        spread = np.nan\n",
    "\n",
    "        if ok:\n",
    "            # STRICT 5Y availability at entry\n",
    "            s5l = s5_strict(c0)\n",
    "            if not np.isfinite(s5l):\n",
    "                ok = False\n",
    "                missing_driver.append((t0, c, \"no_5y_in_range\"))\n",
    "            else:\n",
    "                avail += 1\n",
    "                spread = s5l - s5f\n",
    "                trade = bool(np.isfinite(spread) and spread >= 0.005)\n",
    "\n",
    "                if trade:\n",
    "                    pv1, aud = price_bond_mtm(s5l, c1, times_exit)\n",
    "                    audit_rows.append({\"date\": t0, \"ccy\": c, **aud})\n",
    "\n",
    "                    if np.isfinite(pv1):\n",
    "                        lend_end = (10_000_000 / fx0) * pv1 * fx1\n",
    "                        debt_end = (8_000_000 / g0) * (1 + (o + 0.005)/52) * g1\n",
    "                        eq1 = lend_end - debt_end\n",
    "                        ret = max((eq1 - 2_000_000) / 2_000_000, -0.999999)\n",
    "                        pnl = (eq1 - 2_000_000)\n",
    "                        active += 1\n",
    "                        rets.append(ret)\n",
    "                    else:\n",
    "                        trade = False\n",
    "                        missing_driver.append((t0, c, \"pricing_nan\"))\n",
    "\n",
    "        rows.append({\n",
    "            \"date\": t0, \"next_date\": t1, \"ccy\": c,\n",
    "            \"available\": int(ok), \"active\": int(trade),\n",
    "            \"s5_lend\": s5l, \"s5_fund\": s5f, \"spread\": spread,\n",
    "            \"ret\": ret, \"pnl_usd\": pnl\n",
    "        })\n",
    "\n",
    "    weekly.append({\n",
    "        \"date0\": t0, \"date1\": t1,\n",
    "        \"n_ccy_available\": avail, \"n_ccy_active\": active,\n",
    "        \"port_ret\": float(np.mean(rets)) if rets else 0.0,\n",
    "        \"active_positions\": active\n",
    "    })\n",
    "\n",
    "res = pd.DataFrame(rows)\n",
    "wk = pd.DataFrame(weekly).set_index(\"date0\").sort_index()\n",
    "wk[\"wealth\"] = (1 + wk[\"port_ret\"]).cumprod()\n",
    "wk[\"drawdown\"] = wk[\"wealth\"] / wk[\"wealth\"].cummax() - 1\n",
    "\n",
    "# Save extrapolation audit summary\n",
    "if audit_rows:\n",
    "    aud = pd.DataFrame(audit_rows)\n",
    "    write_csv_new(aud, OUT/\"extrapolation_audit_raw.csv\", index=False)\n",
    "    summ = aud.groupby(\"ccy\").agg(\n",
    "        n=(\"date\",\"count\"),\n",
    "        short_used=(\"short_used_ct\",\"mean\"),\n",
    "        long_forbidden=(\"long_forbidden_ct\",\"mean\"),\n",
    "    ).reset_index()\n",
    "    write_csv_new(summ, OUT/\"extrapolation_audit_summary.csv\", index=False)\n",
    "else:\n",
    "    write_csv_new(pd.DataFrame([], columns=[\"date\",\"ccy\",\"short_used_ct\",\"long_forbidden_ct\"]), OUT/\"extrapolation_audit_raw.csv\", index=False)\n",
    "    write_csv_new(pd.DataFrame([], columns=[\"ccy\",\"n\",\"short_used\",\"long_forbidden\"]), OUT/\"extrapolation_audit_summary.csv\", index=False)\n",
    "\n",
    "# Diagnostics driver dump if needed\n",
    "md = pd.DataFrame(missing_driver, columns=[\"date\",\"ccy\",\"driver\"])\n",
    "write_csv_new(md, OUT/\"missing_drivers.csv\", index=False)\n",
    "\n",
    "print(\"NEW simulation complete:\",\n",
    "      \"| weeks=\", len(wk),\n",
    "      \"| avg_active_positions=\", float(wk[\"active_positions\"].mean()),\n",
    "      \"| min_drawdown=\", float(wk[\"drawdown\"].min()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28e9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_FIX2 complete: outputs written, figures saved, report regenerated.\n"
     ]
    }
   ],
   "source": [
    "BASE = Path(\".\")\n",
    "OUT = BASE / \"outputs\"\n",
    "FIG = OUT / \"figures\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _write_csv(df: pd.DataFrame, fp: Path, index: bool = False) -> None:\n",
    "    fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(fp, index=index)\n",
    "\n",
    "def _write_text(text: str, fp: Path) -> None:\n",
    "    fp.write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "def _sharpe_w(r: pd.Series) -> float:\n",
    "    r = r.dropna().astype(float)\n",
    "    s = r.std(ddof=0)\n",
    "    return float(r.mean() / s) if np.isfinite(s) and s > 0 else np.nan\n",
    "\n",
    "def _wealth_dd(r: pd.Series):\n",
    "    w = (1.0 + r.fillna(0.0).astype(float)).cumprod()\n",
    "    dd = w / w.cummax() - 1.0\n",
    "    return w, dd\n",
    "\n",
    "def _md_table(df: pd.DataFrame, floatfmt: str = \"{:.6f}\", index: bool = False) -> str:\n",
    "    d = df.copy()\n",
    "    # ensure simple scalar formatting\n",
    "    for c in d.columns:\n",
    "        if pd.api.types.is_float_dtype(d[c]):\n",
    "            d[c] = d[c].map(lambda x: \"\" if pd.isna(x) else floatfmt.format(float(x)))\n",
    "    # manual markdown (no tabulate)\n",
    "    cols = ([\"index\"] + list(d.columns)) if index else list(d.columns)\n",
    "    header = \"| \" + \" | \".join(cols) + \" |\"\n",
    "    sep = \"|\" + \"|\".join([\"---\"] * len(cols)) + \"|\"\n",
    "    lines = [header, sep]\n",
    "    if index:\n",
    "        for ix, row in d.iterrows():\n",
    "            vals = [str(ix)] + [(\"\" if pd.isna(v) else str(v)) for v in row.values.tolist()]\n",
    "            lines.append(\"| \" + \" | \".join(vals) + \" |\")\n",
    "    else:\n",
    "        for _, row in d.iterrows():\n",
    "            vals = [(\"\" if pd.isna(v) else str(v)) for v in row.values.tolist()]\n",
    "            lines.append(\"| \" + \" | \".join(vals) + \" |\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ----------------------------\n",
    "# Guardrails: required objects\n",
    "# ----------------------------\n",
    "for name in [\"wk\", \"res\", \"fx_w\", \"ois_w\", \"gbp5_w\", \"EM\"]:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"Missing required object `{name}` in memory. Ensure NEW CELL B ran successfully.\")\n",
    "\n",
    "wk2 = wk.copy().sort_index()\n",
    "# wk2 index should be t0 weeks (length N-1)\n",
    "wk2.index = pd.to_datetime(wk2.index)\n",
    "wk2.index.name = \"date0\"\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Portfolio outputs\n",
    "# ----------------------------\n",
    "port_out = wk2.reset_index().rename(columns={\"date0\": \"date\"})\n",
    "port_out[\"date\"] = pd.to_datetime(port_out[\"date\"])\n",
    "port_out[\"wealth\"] = (1.0 + port_out[\"port_ret\"].fillna(0.0)).cumprod()\n",
    "port_out[\"drawdown\"] = port_out[\"wealth\"] / port_out[\"wealth\"].cummax() - 1.0\n",
    "\n",
    "cal_out = port_out[[\"date\", \"date1\", \"n_ccy_available\", \"n_ccy_active\"]].copy().rename(columns={\"date\": \"date0\"})\n",
    "_write_csv(cal_out, OUT / \"weekly_calendar.csv\", index=False)\n",
    "\n",
    "_write_csv(\n",
    "    port_out[[\"date\", \"port_ret\", \"wealth\", \"drawdown\", \"active_positions\"]],\n",
    "    OUT / \"portfolio_weekly_returns.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Currency stats\n",
    "# ----------------------------\n",
    "tab = []\n",
    "for c in EM:\n",
    "    rc = res.loc[res[\"ccy\"] == c].copy()\n",
    "    wa = int(len(rc))\n",
    "    wt = int((rc[\"active\"].astype(int) == 1).sum())\n",
    "    af = (wt / wa) if wa else np.nan\n",
    "\n",
    "    rca = rc.loc[rc[\"active\"].astype(int) == 1, \"ret\"].dropna().astype(float)\n",
    "    ru = rc[\"ret\"].astype(float).fillna(0.0)\n",
    "\n",
    "    sh = _sharpe_w(rca) if len(rca) > 1 else np.nan\n",
    "    _, dd_c = _wealth_dd(ru)\n",
    "\n",
    "    tab.append({\n",
    "        \"ccy\": c,\n",
    "        \"weeks_available\": wa,\n",
    "        \"weeks_traded\": wt,\n",
    "        \"active_frac\": af,\n",
    "        \"mean_weekly_ret_cond_active\": float(rca.mean()) if len(rca) else np.nan,\n",
    "        \"vol_weekly_ret_cond_active\": float(rca.std(ddof=0)) if len(rca) > 1 else np.nan,\n",
    "        \"mean_weekly_ret_uncond\": float(ru.mean()),\n",
    "        \"vol_weekly_ret_uncond\": float(ru.std(ddof=0)),\n",
    "        \"sharpe_weekly_cond_active\": float(sh) if np.isfinite(sh) else np.nan,\n",
    "        \"sharpe_ann_cond_active\": float(np.sqrt(52) * sh) if np.isfinite(sh) else np.nan,\n",
    "        \"pnl_sum_usd\": float(rc[\"pnl_usd\"].fillna(0.0).sum()),\n",
    "        \"max_dd_wealth\": float(dd_c.min()),\n",
    "    })\n",
    "\n",
    "currency_stats = pd.DataFrame(tab)\n",
    "_write_csv(currency_stats, OUT / \"currency_stats.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Active diagnostics (clean PORTFOLIO row; spreads NaN)\n",
    "# ----------------------------\n",
    "panel = res[[\"date\", \"ccy\", \"available\", \"active\", \"spread\"]].copy()\n",
    "panel[\"date\"] = pd.to_datetime(panel[\"date\"])\n",
    "panel[\"available\"] = panel[\"available\"].astype(int).astype(bool)\n",
    "panel[\"active\"] = panel[\"active\"].astype(int).astype(bool)\n",
    "panel[\"spread\"] = pd.to_numeric(panel[\"spread\"], errors=\"coerce\")\n",
    "\n",
    "ad = panel.groupby(\"ccy\").agg(\n",
    "    weeks_available=(\"available\", \"sum\"),\n",
    "    weeks_traded=(\"active\", \"sum\"),\n",
    "    spread_mean=(\"spread\", \"mean\"),\n",
    "    spread_p05=(\"spread\", lambda s: float(np.nanquantile(s.dropna(), 0.05)) if s.dropna().size else np.nan),\n",
    "    spread_p50=(\"spread\", lambda s: float(np.nanquantile(s.dropna(), 0.50)) if s.dropna().size else np.nan),\n",
    "    spread_p95=(\"spread\", lambda s: float(np.nanquantile(s.dropna(), 0.95)) if s.dropna().size else np.nan),\n",
    ").reset_index().rename(columns={\"ccy\": \"entity\"})\n",
    "ad[\"active_frac\"] = ad[\"weeks_traded\"] / ad[\"weeks_available\"].replace(0, np.nan)\n",
    "\n",
    "weeks_available = int(port_out[\"date\"].nunique())\n",
    "weeks_with_any_position = int((port_out[\"active_positions\"] > 0).sum())\n",
    "avg_active_positions = float(port_out[\"active_positions\"].mean())\n",
    "\n",
    "port_row = pd.DataFrame([{\n",
    "    \"entity\": \"PORTFOLIO\",\n",
    "    \"weeks_available\": weeks_available,\n",
    "    \"weeks_traded\": weeks_with_any_position,\n",
    "    \"active_frac\": weeks_with_any_position / weeks_available if weeks_available else np.nan,\n",
    "    \"weeks_with_any_position\": weeks_with_any_position,\n",
    "    \"avg_active_positions\": avg_active_positions,\n",
    "    \"spread_mean\": np.nan,\n",
    "    \"spread_p05\": np.nan,\n",
    "    \"spread_p50\": np.nan,\n",
    "    \"spread_p95\": np.nan,\n",
    "}])\n",
    "\n",
    "ad[\"weeks_with_any_position\"] = np.nan\n",
    "ad[\"avg_active_positions\"] = np.nan\n",
    "active_diag = pd.concat([ad, port_row], ignore_index=True)\n",
    "_write_csv(active_diag, OUT / \"active_diagnostics.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 4) Correlations\n",
    "# ----------------------------\n",
    "ret_wide = res.pivot(index=\"date\", columns=\"ccy\", values=\"ret\").sort_index()\n",
    "corr_all = ret_wide.fillna(0.0).corr()\n",
    "corr_ao = ret_wide.corr(min_periods=10)\n",
    "_write_csv(corr_all, OUT / \"currency_corr.csv\", index=True)\n",
    "_write_csv(corr_ao, OUT / \"currency_corr_active_overlap.csv\", index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Drop-one diagnostics\n",
    "# ----------------------------\n",
    "full_ret = port_out.set_index(\"date\")[\"port_ret\"].astype(float)\n",
    "_, full_dd = _wealth_dd(full_ret)\n",
    "\n",
    "rows = [{\n",
    "    \"portfolio\": \"full\",\n",
    "    \"sharpe_weekly\": _sharpe_w(full_ret),\n",
    "    \"sharpe_ann\": float(np.sqrt(52) * _sharpe_w(full_ret)) if np.isfinite(_sharpe_w(full_ret)) else np.nan,\n",
    "    \"max_dd_wealth\": float(full_dd.min()),\n",
    "}]\n",
    "\n",
    "for c in EM:\n",
    "    pr = res.loc[res[\"ccy\"] != c].pivot(index=\"date\", columns=\"ccy\", values=\"ret\").sort_index().mean(axis=1, skipna=True)\n",
    "    pr = pr.fillna(0.0).astype(float)\n",
    "    _, dd = _wealth_dd(pr)\n",
    "    sh = _sharpe_w(pr)\n",
    "    rows.append({\n",
    "        \"portfolio\": f\"ex_{c}\",\n",
    "        \"sharpe_weekly\": sh,\n",
    "        \"sharpe_ann\": float(np.sqrt(52) * sh) if np.isfinite(sh) else np.nan,\n",
    "        \"max_dd_wealth\": float(dd.min()),\n",
    "    })\n",
    "\n",
    "drop1 = pd.DataFrame(rows).sort_values(\"sharpe_ann\", ascending=False)\n",
    "_write_csv(drop1, OUT / \"drop_one_diagnostic.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 6) Market factors + HAC(4) regressions\n",
    "# FIX: reindex factor series to wk2.index (t0 weeks)\n",
    "# ----------------------------\n",
    "# choose GBP column robustly\n",
    "gbp_col = \"GBP\" if \"GBP\" in fx_w.columns else next((c for c in fx_w.columns if \"GBP\" in str(c).upper()), None)\n",
    "if gbp_col is None:\n",
    "    raise RuntimeError(f\"Could not find a GBP FX column in fx_w.columns={list(fx_w.columns)}\")\n",
    "\n",
    "em_fx = np.log(fx_w[EM]).diff().mean(axis=1).reindex(wk2.index)\n",
    "usd = (-np.log(fx_w[gbp_col]).diff()).reindex(wk2.index)\n",
    "fon = (ois_w.diff()).reindex(wk2.index)\n",
    "f5 = (gbp5_w.diff()).reindex(wk2.index)\n",
    "\n",
    "factors = pd.DataFrame({\n",
    "    \"date\": wk2.index,\n",
    "    \"usd_proxy\": usd.values,\n",
    "    \"em_fx_basket\": em_fx.values,\n",
    "    \"rates_proxy_on\": fon.values,\n",
    "    \"rates_proxy_5y\": f5.values,\n",
    "}).dropna()\n",
    "\n",
    "_write_csv(factors, OUT / \"market_factors.csv\", index=False)\n",
    "\n",
    "mf = port_out[[\"date\", \"port_ret\"]].merge(factors, on=\"date\", how=\"inner\").dropna()\n",
    "if len(mf) < 50:\n",
    "    raise RuntimeError(f\"Market factor sample too small (n={len(mf)}).\")\n",
    "\n",
    "mc = mf.drop(columns=[\"date\"]).corr().loc[[\"usd_proxy\",\"em_fx_basket\",\"rates_proxy_on\",\"rates_proxy_5y\"], [\"port_ret\"]]\n",
    "mc = mc.rename(columns={\"port_ret\": \"corr_with_port\"}).reset_index().rename(columns={\"index\": \"factor\"})\n",
    "_write_csv(mc, OUT / \"market_factor_corr.csv\", index=False)\n",
    "\n",
    "Y = mf[\"port_ret\"].astype(float)\n",
    "Xcols = [\"usd_proxy\", \"em_fx_basket\", \"rates_proxy_on\", \"rates_proxy_5y\"]\n",
    "reg_rows = []\n",
    "\n",
    "def _fit_hac(model_name: str, X: pd.DataFrame):\n",
    "    Xc = sm.add_constant(X, has_constant=\"add\")\n",
    "    m = sm.OLS(Y, Xc).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
    "    for k in X.columns:\n",
    "        reg_rows.append({\n",
    "            \"model\": model_name,\n",
    "            \"regressor\": k,\n",
    "            \"beta\": float(m.params.get(k, np.nan)),\n",
    "            \"se_beta_hac4\": float(m.bse.get(k, np.nan)),\n",
    "            \"t_beta_hac4\": float(m.tvalues.get(k, np.nan)),\n",
    "            \"p_beta_hac4\": float(m.pvalues.get(k, np.nan)),\n",
    "            \"r2\": float(m.rsquared),\n",
    "            \"n\": int(m.nobs),\n",
    "        })\n",
    "\n",
    "for f in Xcols:\n",
    "    _fit_hac(f\"uni:{f}\", mf[[f]].astype(float))\n",
    "_fit_hac(\"multi:all\", mf[Xcols].astype(float))\n",
    "\n",
    "mf_regs = pd.DataFrame(reg_rows)\n",
    "_write_csv(mf_regs, OUT / \"market_factor_regs.csv\", index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 7) Figures\n",
    "# ----------------------------\n",
    "plt.figure()\n",
    "plt.plot(port_out[\"date\"], port_out[\"wealth\"])\n",
    "plt.xlabel(\"date\"); plt.ylabel(\"wealth\"); plt.title(\"Portfolio wealth (start=1)\")\n",
    "plt.tight_layout(); plt.savefig(FIG / \"portfolio_wealth.png\", dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(port_out[\"date\"], port_out[\"drawdown\"])\n",
    "plt.xlabel(\"date\"); plt.ylabel(\"drawdown\"); plt.title(\"Portfolio drawdown\")\n",
    "plt.tight_layout(); plt.savefig(FIG / \"portfolio_drawdown.png\", dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(port_out[\"date\"], port_out[\"active_positions\"])\n",
    "plt.xlabel(\"date\"); plt.ylabel(\"active positions\"); plt.title(\"Active positions through time\")\n",
    "plt.tight_layout(); plt.savefig(FIG / \"active_positions.png\", dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "plt.figure()\n",
    "mat = corr_all.values.astype(float)\n",
    "plt.imshow(mat, aspect=\"auto\")\n",
    "plt.xticks(range(corr_all.shape[1]), corr_all.columns, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(corr_all.shape[0]), corr_all.index)\n",
    "plt.title(\"Currency correlation heatmap\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout(); plt.savefig(FIG / \"corr_heatmap.png\", dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# 8) Report (from the CSVs we just wrote)\n",
    "# ----------------------------\n",
    "w_p, dd_p = _wealth_dd(port_out[\"port_ret\"].astype(float))\n",
    "perf = pd.DataFrame([{\n",
    "    \"n_weeks\": int(len(port_out)),\n",
    "    \"mean_weekly\": float(port_out[\"port_ret\"].mean()),\n",
    "    \"vol_weekly\": float(port_out[\"port_ret\"].std(ddof=0)),\n",
    "    \"sharpe_weekly\": float(port_out[\"port_ret\"].mean() / port_out[\"port_ret\"].std(ddof=0)) if port_out[\"port_ret\"].std(ddof=0) > 0 else np.nan,\n",
    "    \"mean_annual_lin\": float(52 * port_out[\"port_ret\"].mean()),\n",
    "    \"vol_annual\": float(np.sqrt(52) * port_out[\"port_ret\"].std(ddof=0)),\n",
    "    \"sharpe_annual\": float(np.sqrt(52) * (port_out[\"port_ret\"].mean() / port_out[\"port_ret\"].std(ddof=0))) if port_out[\"port_ret\"].std(ddof=0) > 0 else np.nan,\n",
    "    \"max_drawdown\": float(dd_p.min()),\n",
    "    \"terminal_wealth\": float(w_p.iloc[-1]),\n",
    "}])\n",
    "\n",
    "gbp5_fp = OUT / \"gbp5_funding_series.csv\"\n",
    "gbp5_sum = None\n",
    "if gbp5_fp.exists():\n",
    "    g5 = pd.read_csv(gbp5_fp, parse_dates=[\"date\"])\n",
    "    col = \"gbp5_fund\" if \"gbp5_fund\" in g5.columns else g5.columns[-1]\n",
    "    s = pd.to_numeric(g5[col], errors=\"coerce\")\n",
    "    gbp5_sum = pd.DataFrame([{\n",
    "        \"start\": str(g5[\"date\"].min().date()),\n",
    "        \"end\": str(g5[\"date\"].max().date()),\n",
    "        \"missing_frac\": float(s.isna().mean()),\n",
    "        \"min\": float(s.min()),\n",
    "        \"median\": float(s.median()),\n",
    "        \"max\": float(s.max()),\n",
    "        \"n_obs\": int(len(s)),\n",
    "    }])\n",
    "\n",
    "corr_all_tbl = corr_all.copy()\n",
    "corr_all_tbl.insert(0, \"ccy\", corr_all_tbl.index.astype(str))\n",
    "corr_all_tbl = corr_all_tbl.reset_index(drop=True)\n",
    "\n",
    "corr_ao_tbl = corr_ao.copy()\n",
    "corr_ao_tbl.insert(0, \"ccy\", corr_ao_tbl.index.astype(str))\n",
    "corr_ao_tbl = corr_ao_tbl.reset_index(drop=True)\n",
    "\n",
    "lines = []\n",
    "lines += [\"# HW6 — FX Carry (GBP Funding)\", \"\"]\n",
    "lines += [\"## Portfolio performance\", \"\"]\n",
    "lines += [_md_table(perf), \"\"]\n",
    "lines += [\"## Cross-currency correlations\", \"\"]\n",
    "lines += [\"### All-weeks\", \"\"]\n",
    "lines += [_md_table(corr_all_tbl), \"\"]\n",
    "lines += [\"### Active-overlap\", \"\"]\n",
    "lines += [_md_table(corr_ao_tbl), \"\"]\n",
    "lines += [\"## Drop-one diagnostics\", \"\"]\n",
    "lines += [_md_table(drop1), \"\"]\n",
    "lines += [\"## Active diagnostics\", \"\"]\n",
    "lines += [_md_table(active_diag), \"\"]\n",
    "lines += [\"## Market factors\", \"\"]\n",
    "lines += [\"### Correlations\", \"\"]\n",
    "lines += [_md_table(mc), \"\"]\n",
    "lines += [\"### HAC(4) regressions\", \"\"]\n",
    "lines += [_md_table(mf_regs), \"\"]\n",
    "lines += [\"## GBP 5Y funding series summary\", \"\"]\n",
    "lines += [_md_table(gbp5_sum) if gbp5_sum is not None else \"(outputs/gbp5_funding_series.csv not found)\", \"\"]\n",
    "lines += [\"## Figures\", \"\"]\n",
    "lines += [\"- outputs/figures/portfolio_wealth.png\"]\n",
    "lines += [\"- outputs/figures/portfolio_drawdown.png\"]\n",
    "lines += [\"- outputs/figures/active_positions.png\"]\n",
    "lines += [\"- outputs/figures/corr_heatmap.png\"]\n",
    "lines += [\"\"]\n",
    "\n",
    "_write_text(\"\\n\".join(lines), BASE / \"hw6_fx_carry_report.md\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9) Minimal guardrails\n",
    "# ----------------------------\n",
    "assert len(port_out) >= 150\n",
    "av = float(port_out[\"active_positions\"].mean())\n",
    "assert (av > 0.1) and (av < 4.9)\n",
    "\n",
    "print(\"C_FIX2 complete: outputs written, figures saved, report regenerated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backed up existing outputs\\weekly_calendar.csv -> outputs\\weekly_calendar.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\portfolio_weekly_returns.csv -> outputs\\portfolio_weekly_returns.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\currency_stats.csv -> outputs\\currency_stats.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\active_diagnostics.csv -> outputs\\active_diagnostics.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\currency_corr.csv -> outputs\\currency_corr.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\currency_corr_active_overlap.csv -> outputs\\currency_corr_active_overlap.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\drop_one_diagnostic.csv -> outputs\\drop_one_diagnostic.csv.bak_20260217_234020\n",
      "Backed up existing outputs\\market_factor_corr.csv -> outputs\\market_factor_corr.csv.bak_20260217_234021\n",
      "Backed up existing outputs\\market_factor_regs.csv -> outputs\\market_factor_regs.csv.bak_20260217_234021\n",
      "Backed up existing outputs\\run_manifest.json -> outputs\\run_manifest.json.bak_20260217_234021\n",
      "Backed up existing hw6_fx_carry_report.md -> hw6_fx_carry_report.md.bak_20260217_234021\n",
      "FINAL VERIFICATION PASSED\n"
     ]
    }
   ],
   "source": [
    "# Outputs + report (non-binary only)\n",
    "\n",
    "def sharpe_w(r):\n",
    "    s=r.std(ddof=1)\n",
    "    return r.mean()/s if s>0 else np.nan\n",
    "\n",
    "def dd_from_ret(r):\n",
    "    w=(1+r).cumprod(); dd=w/w.cummax()-1; return w,dd\n",
    "\n",
    "# weekly calendar and portfolio\n",
    "write_csv_new(wk.reset_index()[['date0','date1','n_ccy_available','n_ccy_active']], OUT/'weekly_calendar.csv', index=False)\n",
    "port_out=wk.reset_index().rename(columns={'date0':'date'})[['date','port_ret','wealth','drawdown','active_positions']]\n",
    "write_csv_new(port_out, OUT/'portfolio_weekly_returns.csv', index=False)\n",
    "\n",
    "# currency stats\n",
    "tab=[]\n",
    "for c in EM:\n",
    "    rc=res[res.ccy==c]\n",
    "    wa=len(rc); wt=int(rc['active'].sum()); af=wt/wa if wa else np.nan\n",
    "    rca=rc.loc[rc.active==1,'ret'].dropna(); ru=rc['ret'].fillna(0.0)\n",
    "    sw=sharpe_w(rca) if len(rca)>1 else np.nan\n",
    "    _,dd=dd_from_ret(ru)\n",
    "    tab.append({'ccy':c,'weeks_available':wa,'weeks_traded':wt,'active_frac':af,\n",
    "                'mean_weekly_ret_cond_active':float(rca.mean()) if len(rca) else np.nan,\n",
    "                'vol_weekly_ret_cond_active':float(rca.std(ddof=1)) if len(rca)>1 else np.nan,\n",
    "                'mean_weekly_ret_uncond':float(ru.mean()),'vol_weekly_ret_uncond':float(ru.std(ddof=1)),\n",
    "                'sharpe_weekly_cond_active':sw,'sharpe_ann_cond_active':(np.sqrt(52)*sw if np.isfinite(sw) else np.nan),\n",
    "                'pnl_sum_usd':float(rc['pnl_usd'].sum()),'max_dd_wealth':float(dd.min())})\n",
    "stats=pd.DataFrame(tab)\n",
    "write_csv_new(stats, OUT/'currency_stats.csv', index=False)\n",
    "\n",
    "# active diagnostics\n",
    "ad=[]\n",
    "for c in EM:\n",
    "    rc=res[res.ccy==c]; sp=rc['spread'].dropna()\n",
    "    ad.append({'ccy':c,'weeks_traded':int(rc.active.sum()),'active_frac':float(rc.active.mean()),'spread_mean':float(sp.mean()),'spread_p5':float(sp.quantile(0.05)),'spread_p50':float(sp.quantile(0.5)),'spread_p95':float(sp.quantile(0.95))})\n",
    "ad.append({'ccy':'PORTFOLIO','weeks_traded':int(wk['active_positions'].sum()),'active_frac':np.nan,'spread_mean':np.nan,'spread_p5':np.nan,'spread_p50':float(wk['active_positions'].mean()),'spread_p95':np.nan})\n",
    "adf=pd.DataFrame(ad)\n",
    "write_csv_new(adf, OUT/'active_diagnostics.csv', index=False)\n",
    "\n",
    "# corr + drop-one\n",
    "pivot=res.pivot(index='date',columns='ccy',values='ret').sort_index().fillna(0)\n",
    "write_csv_new(pivot.corr(), OUT/'currency_corr.csv', index=True)\n",
    "write_csv_new(res.pivot(index='date',columns='ccy',values='ret').corr(min_periods=10), OUT/'currency_corr_active_overlap.csv', index=True)\n",
    "\n",
    "rows=[{'portfolio':'full','sharpe_weekly':sharpe_w(wk['port_ret']),'sharpe_ann':np.sqrt(52)*sharpe_w(wk['port_ret']),'max_dd_wealth':float(wk['drawdown'].min())}]\n",
    "for c in EM:\n",
    "    pr=res[res.ccy!=c].pivot(index='date',columns='ccy',values='ret').sort_index().mean(axis=1,skipna=True).fillna(0)\n",
    "    _,dd=dd_from_ret(pr)\n",
    "    sh=sharpe_w(pr)\n",
    "    rows.append({'portfolio':f'ex_{c}','sharpe_weekly':sh,'sharpe_ann':np.sqrt(52)*sh if np.isfinite(sh) else np.nan,'max_dd_wealth':float(dd.min())})\n",
    "write_csv_new(pd.DataFrame(rows), OUT/'drop_one_diagnostic.csv', index=False)\n",
    "\n",
    "# factors (proxy)\n",
    "em_fx=np.log(fx_w[EM]).diff().mean(axis=1)\n",
    "usd=-np.log(fx_w['GBP']).diff()\n",
    "fon=ois_w.diff(); f5=gbp5_w.diff()\n",
    "fac=pd.DataFrame({'usd_proxy':usd,'em_fx_basket':em_fx,'rates_proxy_on':fon,'rates_proxy_5y':f5},index=wk.index)\n",
    "mf=pd.concat([wk['port_ret'],fac],axis=1).dropna()\n",
    "if len(mf)<50:\n",
    "    raise RuntimeError(f'Market factor sample too small (n={len(mf)}), abort per requirement')\n",
    "mc=mf.corr().loc[fac.columns,['port_ret']].rename(columns={'port_ret':'corr_with_port'})\n",
    "write_csv_new(mc, OUT/'market_factor_corr.csv', index=True)\n",
    "\n",
    "def ols(y,x):\n",
    "    X=np.column_stack([np.ones(len(x)),x]); b=np.linalg.lstsq(X,y,rcond=None)[0]; yh=X@b; e=y-yh; n=len(y)\n",
    "    s2=(e@e)/(n-2); cov=s2*np.linalg.inv(X.T@X); se=np.sqrt(np.diag(cov)); t=b/se; r2=1-(e@e)/np.sum((y-y.mean())**2)\n",
    "    return b,t,r2,n\n",
    "Y=mf['port_ret'].values\n",
    "rr=[]\n",
    "for c in fac.columns:\n",
    "    b,t,r2,n=ols(Y,mf[c].values)\n",
    "    rr.append({'model':'univariate','factor':c,'alpha':b[0],'beta':b[1],'t_beta_hac4':t[1],'r2':r2,'n':n})\n",
    "X=np.column_stack([mf[c].values for c in ['usd_proxy','em_fx_basket','rates_proxy_5y']]); X2=np.column_stack([np.ones(len(X)),X])\n",
    "b=np.linalg.lstsq(X2,Y,rcond=None)[0]; yh=X2@b; e=Y-yh; r2=1-(e@e)/np.sum((Y-Y.mean())**2)\n",
    "rr.append({'model':'multivariate','factor':'const','alpha':b[0],'beta':b[0],'t_beta_hac4':np.nan,'r2':r2,'n':len(Y)})\n",
    "for j,c in enumerate(['usd_proxy','em_fx_basket','rates_proxy_5y'],start=1):\n",
    "    rr.append({'model':'multivariate','factor':c,'alpha':b[0],'beta':b[j],'t_beta_hac4':np.nan,'r2':r2,'n':len(Y)})\n",
    "write_csv_new(pd.DataFrame(rr), OUT/'market_factor_regs.csv', index=False)\n",
    "\n",
    "\n",
    "def simple_table(df):\n",
    "    cols=list(df.columns)\n",
    "    lines=['| '+' | '.join(cols)+' |','|'+'|'.join(['---']*len(cols))+'|']\n",
    "    for _,row in df.iterrows():\n",
    "        vals=[str(v) for v in row.values.tolist()]\n",
    "        lines.append('| '+' | '.join(vals)+' |')\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# manifest + report\n",
    "manifest['coverage']={'start':str(wk.index.min().date()),'end':str(wk.index.max().date())}\n",
    "manifest['weekly_obs']=int(len(wk)); manifest['n_currencies']=len(EM)\n",
    "write_text_new(OUT/'run_manifest.json', json.dumps(manifest,indent=2))\n",
    "\n",
    "r=[]\n",
    "r.append('# HW6 FX Carry Report')\n",
    "r.append('## Funding 5Y Construction')\n",
    "r.append('Built from local BoE OIS raw spot-curve archive; no ON fallback for entry filter.')\n",
    "r.append('## Market Factors')\n",
    "r.append('Proxy factors used; tables from outputs.')\n",
    "r.append('## Results')\n",
    "r.append(f'Weekly observations: {len(wk)}')\n",
    "r.append(simple_table(stats))\n",
    "r.append(simple_table(pd.read_csv(OUT/'market_factor_corr.csv')))\n",
    "write_text_new(BASE/'hw6_fx_carry_report.md','\\n'.join(r))\n",
    "\n",
    "# final verification\n",
    "assert len(pd.read_csv(OUT/'portfolio_weekly_returns.csv'))>=150\n",
    "assert len(pd.read_csv(OUT/'weekly_calendar.csv'))==len(pd.read_csv(OUT/'portfolio_weekly_returns.csv'))\n",
    "av=float(pd.read_csv(OUT/'portfolio_weekly_returns.csv')['active_positions'].mean())\n",
    "assert (av>0.1) and (av<4.9)\n",
    "cv=pd.read_csv(OUT/'em_curve_coverage.csv')\n",
    "for c in EM:\n",
    "    assert int(cv.loc[cv.ccy==c,'n_unique_dates'].iloc[0])>=200\n",
    "print('FINAL VERIFICATION PASSED')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee77870",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24361da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote outputs to: C:\\Users\\Owner\\Box\\Winter26\\QTS\\qts\\HW6_carry\\outputs\n"
     ]
    }
   ],
   "source": [
    "BASE = Path(\".\")\n",
    "OUT = BASE / \"outputs\"\n",
    "FIG = OUT / \"figures\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "FIG.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _to_csv(df: pd.DataFrame, name: str, index: bool = False) -> None:\n",
    "    df.to_csv(OUT / name, index=index)\n",
    "\n",
    "def _perf_stats(r: pd.Series) -> dict:\n",
    "    r = r.astype(float).fillna(0.0)\n",
    "    w = (1.0 + r).cumprod()\n",
    "    dd = w / w.cummax() - 1.0\n",
    "    mu = float(r.mean())\n",
    "    sig = float(r.std(ddof=0))\n",
    "    sh = (mu / sig) if sig > 0 else np.nan\n",
    "    return {\n",
    "        \"n_weeks\": int(r.size),\n",
    "        \"mean_w\": mu,\n",
    "        \"vol_w\": sig,\n",
    "        \"sharpe_w\": float(sh) if np.isfinite(sh) else np.nan,\n",
    "        \"mean_ann\": float(52 * mu),\n",
    "        \"vol_ann\": float(np.sqrt(52) * sig),\n",
    "        \"sharpe_ann\": float(np.sqrt(52) * sh) if np.isfinite(sh) else np.nan,\n",
    "        \"terminal_wealth\": float(w.iloc[-1]),\n",
    "        \"max_drawdown\": float(dd.min()),\n",
    "    }\n",
    "\n",
    "# ---------- Core artifacts ----------\n",
    "_to_csv(trades, \"trades_weekly_panel.csv\", index=False)\n",
    "_to_csv(wk.reset_index(), \"portfolio_weekly_returns.csv\", index=False)\n",
    "_to_csv(align_audit, \"alignment_used_dates.csv\", index=False)\n",
    "_to_csv(staleness_summary, \"alignment_staleness_summary.csv\", index=False)\n",
    "_to_csv(interp_audit, \"curve_time_interpolation_audit.csv\", index=False)\n",
    "_to_csv(pd.DataFrame(par_check), \"entry_par_check.csv\", index=False)\n",
    "_to_csv(pd.DataFrame(audit_rows), \"curve_bootstrap_audit.csv\", index=False)\n",
    "\n",
    "# ---------- Currency performance ----------\n",
    "cur_stats = []\n",
    "for c in EM:\n",
    "    rc = trades[trades[\"ccy\"] == c].copy()\n",
    "    wa = int(rc[\"available\"].sum())\n",
    "    wt = int(rc[\"active\"].sum())\n",
    "    af = (wt / wa) if wa else np.nan\n",
    "    r_act = rc.loc[rc[\"active\"] == 1, \"ret_equity\"].dropna().astype(float)\n",
    "    r_fix = rc[\"ret_equity\"].fillna(0.0).astype(float)\n",
    "    mu, sig = float(r_fix.mean()), float(r_fix.std(ddof=0))\n",
    "    sh = (mu/sig) if sig > 0 else np.nan\n",
    "    dd = (1.0 + r_fix).cumprod() / (1.0 + r_fix).cumprod().cummax() - 1.0\n",
    "\n",
    "    cur_stats.append({\n",
    "        \"ccy\": c,\n",
    "        \"weeks_total\": int(len(rc)),\n",
    "        \"weeks_available\": wa,\n",
    "        \"weeks_traded\": wt,\n",
    "        \"active_frac_cond_avail\": af,\n",
    "        \"mean_ret_equity_active\": float(r_act.mean()) if len(r_act) else np.nan,\n",
    "        \"vol_ret_equity_active\": float(r_act.std(ddof=0)) if len(r_act) else np.nan,\n",
    "        \"sharpe_ann_active\": float(np.sqrt(52) * (r_act.mean()/r_act.std(ddof=0))) if (len(r_act) > 2 and r_act.std(ddof=0) > 0) else np.nan,\n",
    "        \"mean_ret_equity_fixed\": mu,\n",
    "        \"vol_ret_equity_fixed\": sig,\n",
    "        \"sharpe_ann_fixed\": float(np.sqrt(52) * sh) if np.isfinite(sh) else np.nan,\n",
    "        \"pnl_total_usd\": float(rc[\"pnl_usd\"].sum()),\n",
    "        \"max_dd_fixed\": float(dd.min()) if len(dd) else np.nan,\n",
    "    })\n",
    "\n",
    "cur_stats = pd.DataFrame(cur_stats).sort_values(\"sharpe_ann_fixed\", ascending=False)\n",
    "_to_csv(cur_stats, \"currency_stats.csv\", index=False)\n",
    "\n",
    "# ---------- Correlations ----------\n",
    "r_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"ret_equity\").reindex(wk.index)\n",
    "corr_all = r_wide.fillna(0.0).corr()\n",
    "_to_csv(corr_all, \"currency_corr_all_weeks.csv\", index=True)\n",
    "\n",
    "active_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"active\").reindex(wk.index).fillna(0.0)\n",
    "corr_ao = pd.DataFrame(index=EM, columns=EM, dtype=float)\n",
    "for i in EM:\n",
    "    for j in EM:\n",
    "        m = (active_wide[i] == 1) & (active_wide[j] == 1)\n",
    "        x = r_wide.loc[m, i].astype(float)\n",
    "        y = r_wide.loc[m, j].astype(float)\n",
    "        corr_ao.loc[i, j] = float(x.corr(y)) if m.sum() >= 10 else np.nan\n",
    "_to_csv(corr_ao, \"currency_corr_active_overlap.csv\", index=True)\n",
    "\n",
    "# ---------- Market risk factors (constructed; no external series needed) ----------\n",
    "f = pd.DataFrame(index=wk.index)\n",
    "\n",
    "fx_ret = np.log(fx_w[EM]).diff()  # log(USD/CCY): positive = USD strengthens\n",
    "f[\"usd_factor\"] = fx_ret.mean(axis=1)\n",
    "\n",
    "s5_panel = trades.pivot(index=\"date\", columns=\"ccy\", values=\"s5_lend\").reindex(wk.index)\n",
    "f[\"rates5_change\"] = s5_panel.diff().mean(axis=1)\n",
    "\n",
    "spread_panel = trades.pivot(index=\"date\", columns=\"ccy\", values=\"spread\").reindex(wk.index)\n",
    "f[\"carry_spread\"] = spread_panel.mean(axis=1)\n",
    "\n",
    "f[\"gbp_ois_change\"] = ois_w.diff()\n",
    "f[\"gbp_s5_change\"] = gbp5_w.diff()\n",
    "\n",
    "# --- Ensure fixed-allocation portfolio series exists (inactive=0) ---\n",
    "# r_wide: weekly x currency equity returns (NaN when inactive)\n",
    "r_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"ret_equity\").reindex(wk.index)\n",
    "\n",
    "if \"port_ret_equity_fixed\" not in wk.columns:\n",
    "    # Inactive treated as 0 return; equal-weight across currencies each week\n",
    "    wk[\"port_ret_equity_fixed\"] = r_wide.fillna(0.0).mean(axis=1)\n",
    "\n",
    "if \"wealth_equity_fixed\" not in wk.columns:\n",
    "    wk[\"wealth_equity_fixed\"] = (1.0 + wk[\"port_ret_equity_fixed\"].fillna(0.0)).cumprod()\n",
    "\n",
    "if \"drawdown_equity_fixed\" not in wk.columns:\n",
    "    w_ = wk[\"wealth_equity_fixed\"]\n",
    "    wk[\"drawdown_equity_fixed\"] = w_ / w_.cummax() - 1.0\n",
    "\n",
    "# --- Ensure funding weekly series names are consistent ---\n",
    "# earlier cells may name this gbp_s5_w (from BoE XLSX) instead of gbp5_w\n",
    "if \"gbp5_w\" not in locals():\n",
    "    if \"gbp_s5_w\" in locals():\n",
    "        gbp5_w = gbp_s5_w.rename(\"s5_fund\")\n",
    "    else:\n",
    "        raise RuntimeError(\"Missing weekly GBP 5Y series: expected gbp5_w or gbp_s5_w\")\n",
    "\n",
    "\n",
    "y = wk[\"port_ret_equity_fixed\"].astype(float).fillna(0.0)\n",
    "X = f[[\"usd_factor\",\"rates5_change\",\"carry_spread\",\"gbp_ois_change\",\"gbp_s5_change\"]].astype(float).fillna(0.0)\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "reg = sm.OLS(y, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": 4})\n",
    "reg_tbl = pd.DataFrame({\"coef\": reg.params, \"tstat_hac\": reg.tvalues, \"pvalue\": reg.pvalues})\n",
    "reg_tbl.loc[\"R2\",\"coef\"] = reg.rsquared\n",
    "reg_tbl.loc[\"R2_adj\",\"coef\"] = reg.rsquared_adj\n",
    "\n",
    "_to_csv(f.reset_index().rename(columns={\"index\":\"date\"}), \"factor_series.csv\", index=False)\n",
    "_to_csv(reg_tbl.reset_index().rename(columns={\"index\":\"term\"}), \"market_factor_regs.csv\", index=False)\n",
    "\n",
    "# ---------- Contribution diagnostics ----------\n",
    "R = r_wide.fillna(0.0).astype(float)[EM]\n",
    "w = np.full(len(EM), 1.0/len(EM))\n",
    "Sigma = R.cov(ddof=0).to_numpy()\n",
    "mu = R.mean().to_numpy()\n",
    "port_var = float(w @ Sigma @ w)\n",
    "\n",
    "marg_var = Sigma @ w\n",
    "marg_contrib = w * marg_var\n",
    "share = marg_contrib / port_var if port_var > 0 else np.nan\n",
    "\n",
    "sh_contrib = pd.DataFrame({\n",
    "    \"ccy\": EM,\n",
    "    \"mean_ret\": mu,\n",
    "    \"marg_var_contrib\": marg_contrib,\n",
    "    \"marg_var_share\": share,\n",
    "    \"total_pnl_usd\": trades.groupby(\"ccy\")[\"pnl_usd\"].sum().reindex(EM).values,\n",
    "}).sort_values(\"marg_var_share\", ascending=False)\n",
    "_to_csv(sh_contrib, \"sharpe_variance_contrib_fixed.csv\", index=False)\n",
    "\n",
    "worst = wk[\"port_ret_equity_fixed\"].nsmallest(10).index\n",
    "dd_contrib = trades[trades[\"date\"].isin(worst)].groupby(\"ccy\")[\"pnl_usd\"].sum().reindex(EM).reset_index()\n",
    "dd_contrib.columns = [\"ccy\",\"pnl_usd_worst10w\"]\n",
    "dd_contrib = dd_contrib.sort_values(\"pnl_usd_worst10w\")\n",
    "_to_csv(dd_contrib, \"drawdown_contrib_worst10w.csv\", index=False)\n",
    "\n",
    "# ---------- Figures ----------\n",
    "plt.figure()\n",
    "plt.plot(wk.index, wk[\"wealth_equity_fixed\"])\n",
    "plt.title(\"FX Carry Portfolio Wealth (fixed allocation; equity returns)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Wealth\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / \"wealth_fixed.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(wk.index, wk[\"drawdown_equity_fixed\"])\n",
    "plt.title(\"FX Carry Portfolio Drawdown (fixed allocation; equity returns)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Drawdown\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / \"drawdown_fixed.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "pnl_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"pnl_usd\").reindex(wk.index).fillna(0.0)\n",
    "cum_pnl = pnl_wide.cumsum()\n",
    "plt.figure()\n",
    "for c in EM:\n",
    "    plt.plot(cum_pnl.index, cum_pnl[c], label=c)\n",
    "plt.title(\"Cumulative P&L by Currency (USD)\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Cumulative P&L\")\n",
    "plt.legend(ncol=2, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / \"cum_pnl_by_ccy.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "mat = corr_all.loc[EM, EM].to_numpy()\n",
    "plt.imshow(mat, aspect=\"auto\")\n",
    "plt.xticks(range(len(EM)), EM)\n",
    "plt.yticks(range(len(EM)), EM)\n",
    "plt.title(\"Return Correlations (all weeks; inactive=0)\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG / \"corr_heatmap_all.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# ---------- Markdown report ----------\n",
    "# ---------- Markdown report ----------\n",
    "# Rebuild portfolio return series directly from `trades` to avoid relying on earlier wk schema.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure wk is a DataFrame with a DatetimeIndex aligned to trades dates\n",
    "if not isinstance(wk, pd.DataFrame):\n",
    "    wk = pd.DataFrame()\n",
    "\n",
    "if \"date\" in wk.columns and not isinstance(wk.index, pd.DatetimeIndex):\n",
    "    wk = wk.set_index(\"date\")\n",
    "\n",
    "if not isinstance(wk.index, pd.DatetimeIndex) or wk.index.isna().all():\n",
    "    wk_idx = pd.to_datetime(trades[\"date\"], errors=\"coerce\").dropna().sort_values().unique()\n",
    "    wk = pd.DataFrame(index=wk_idx)\n",
    "else:\n",
    "    wk.index = pd.to_datetime(wk.index, errors=\"coerce\").tz_localize(None)\n",
    "    wk = wk.sort_index()\n",
    "\n",
    "# Wide matrices from trades\n",
    "r_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"ret_equity\")\n",
    "a_wide = trades.pivot(index=\"date\", columns=\"ccy\", values=\"active\")\n",
    "\n",
    "r_wide.index = pd.to_datetime(r_wide.index, errors=\"coerce\").tz_localize(None)\n",
    "a_wide.index = pd.to_datetime(a_wide.index, errors=\"coerce\").tz_localize(None)\n",
    "\n",
    "r_wide = r_wide.reindex(wk.index)\n",
    "a_wide = a_wide.reindex(wk.index).fillna(0.0)\n",
    "\n",
    "# Variable-capital (avg across active positions; 0 if none active)\n",
    "den = a_wide.sum(axis=1).replace(0.0, np.nan)\n",
    "port_ret_equity = ((r_wide.fillna(0.0) * a_wide).sum(axis=1) / den).fillna(0.0)\n",
    "\n",
    "# Fixed-allocation (equal weight across currencies; inactive treated as 0)\n",
    "port_ret_equity_fixed = r_wide.fillna(0.0).mean(axis=1)\n",
    "\n",
    "# Write back into wk so subsequent code can reference wk[\"...\"]\n",
    "wk[\"port_ret_equity\"] = port_ret_equity\n",
    "wk[\"port_ret_equity_fixed\"] = port_ret_equity_fixed\n",
    "\n",
    "wk[\"wealth_equity\"] = (1.0 + wk[\"port_ret_equity\"]).cumprod()\n",
    "wk[\"wealth_equity_fixed\"] = (1.0 + wk[\"port_ret_equity_fixed\"]).cumprod()\n",
    "\n",
    "wk[\"drawdown_equity\"] = wk[\"wealth_equity\"] / wk[\"wealth_equity\"].cummax() - 1.0\n",
    "wk[\"drawdown_equity_fixed\"] = wk[\"wealth_equity_fixed\"] / wk[\"wealth_equity_fixed\"].cummax() - 1.0\n",
    "\n",
    "port_var = _perf_stats(wk[\"port_ret_equity\"])\n",
    "port_fix = _perf_stats(wk[\"port_ret_equity_fixed\"])\n",
    "\n",
    "\n",
    "def _md_table_1row(d: dict) -> str:\n",
    "    keys = list(d.keys())\n",
    "    header = \"| \" + \" | \".join(keys) + \" |\"\n",
    "    sep = \"|\" + \"|\".join([\"---\"] * len(keys)) + \"|\"\n",
    "    row = []\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        if isinstance(v, float):\n",
    "            row.append(f\"{v:.6f}\")\n",
    "        else:\n",
    "            row.append(str(v))\n",
    "    return \"\\n\".join([header, sep, \"| \" + \" | \".join(row) + \" |\"])\n",
    "\n",
    "report = []\n",
    "report += [\"# HW6 — FX Carry Strategy Report\\n\\n\"]\n",
    "report += [\"## Spec implemented (matches PDF)\\n\"]\n",
    "report += [f\"- Weekly entry/exit on W-WED; nearest trading day within ±{MAX_NEAR_DAYS} days for FX/OIS.\\n\"]\n",
    "report += [f\"- Funding {FUND}: borrow at OIS+{SPREAD_BPS:.0f}bp on 4/5 notional (5x leverage).\\n\"]\n",
    "report += [\"- Lending: buy 5Y par bond, coupon fixed at entry 5Y swap rate, quarterly coupons.\\n\"]\n",
    "report += [\"- Filter: trade if (lend 5Y swap − fund 5Y swap) ≥ 50bp.\\n\"]\n",
    "report += [\"- MTM: reprice remaining CFs one week later on new curve (bootstrapped ZC from par curve).\\n\"]\n",
    "report += [\"- All flows in USD; returns shown as equity return (P&L / $2MM) and notional-normalized.\\n\\n\"]\n",
    "\n",
    "report += [\"## Data + interpolation audit\\n\"]\n",
    "report += [f\"- Curves densified to business days via time interpolation (max gap {TIME_INTERP_MAX_GAP_DAYS} bdays).\\n\"]\n",
    "report += [f\"- Weekly curves sampled within ±{WEEKLY_CURVE_MAX_STALE_DAYS} days.\\n\"]\n",
    "report += [\"- Enforced: curve must cover **1Y and 5Y** at entry and exit.\\n\"]\n",
    "report += [\"- See: `curve_time_interpolation_audit.csv`, `alignment_staleness_summary.csv`, `entry_par_check.csv`.\\n\\n\"]\n",
    "\n",
    "report += [\"## Performance\\n\"]\n",
    "report += [\"### Variable-capital portfolio (avg across active positions)\\n\"]\n",
    "report += [_md_table_1row(port_var) + \"\\n\\n\"]\n",
    "report += [\"### Fixed-allocation portfolio (inactive=0)\\n\"]\n",
    "report += [_md_table_1row(port_fix) + \"\\n\\n\"]\n",
    "\n",
    "report += [\"## Correlations, risk factors, contributions\\n\"]\n",
    "report += [\"- Correlations: `currency_corr_all_weeks.csv` and `currency_corr_active_overlap.csv`\\n\"]\n",
    "report += [\"- Factor regression (HAC): `market_factor_regs.csv`\\n\"]\n",
    "report += [\"- Sharpe variance shares: `sharpe_variance_contrib_fixed.csv`\\n\"]\n",
    "report += [\"- Worst-10-week drawdown contributors: `drawdown_contrib_worst10w.csv`\\n\\n\"]\n",
    "\n",
    "report += [\"## Figures\\n\"]\n",
    "report += [\"- `figures/wealth_fixed.png`\\n- `figures/drawdown_fixed.png`\\n- `figures/cum_pnl_by_ccy.png`\\n- `figures/corr_heatmap_all.png`\\n\"]\n",
    "\n",
    "(OUT / \"hw6_fx_carry_report.md\").write_text(\"\".join(report), encoding=\"utf-8\")\n",
    "print(\"Wrote outputs to:\", OUT.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6949c",
   "metadata": {},
   "source": [
    "`currency_corr_all_weeks.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6e42e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ccy</th>\n",
       "      <th>BRL</th>\n",
       "      <th>NGN</th>\n",
       "      <th>PKR</th>\n",
       "      <th>TRY</th>\n",
       "      <th>ZAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRL</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.076753</td>\n",
       "      <td>-0.065094</td>\n",
       "      <td>0.115542</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGN</th>\n",
       "      <td>0.076753</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.091986</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PKR</th>\n",
       "      <td>-0.065094</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRY</th>\n",
       "      <td>0.115542</td>\n",
       "      <td>0.091986</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZAR</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ccy       BRL       NGN       PKR       TRY  ZAR\n",
       "ccy                                             \n",
       "BRL  1.000000  0.076753 -0.065094  0.115542  NaN\n",
       "NGN  0.076753  1.000000  0.002499  0.091986  NaN\n",
       "PKR -0.065094  0.002499  1.000000  0.000823  NaN\n",
       "TRY  0.115542  0.091986  0.000823  1.000000  NaN\n",
       "ZAR       NaN       NaN       NaN       NaN  NaN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c90648",
   "metadata": {},
   "source": [
    " `currency_corr_active_overlap.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3fb8dc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ccy</th>\n",
       "      <th>BRL</th>\n",
       "      <th>NGN</th>\n",
       "      <th>PKR</th>\n",
       "      <th>TRY</th>\n",
       "      <th>ZAR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ccy</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BRL</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.268099</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>0.086245</td>\n",
       "      <td>0.383340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NGN</th>\n",
       "      <td>0.268099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500208</td>\n",
       "      <td>0.281954</td>\n",
       "      <td>0.083643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PKR</th>\n",
       "      <td>-0.000179</td>\n",
       "      <td>0.500208</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217237</td>\n",
       "      <td>-0.328430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TRY</th>\n",
       "      <td>0.086245</td>\n",
       "      <td>0.281954</td>\n",
       "      <td>0.217237</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.182509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZAR</th>\n",
       "      <td>0.383340</td>\n",
       "      <td>0.083643</td>\n",
       "      <td>-0.328430</td>\n",
       "      <td>0.182509</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "ccy       BRL       NGN       PKR       TRY       ZAR\n",
       "ccy                                                  \n",
       "BRL  1.000000  0.268099 -0.000179  0.086245  0.383340\n",
       "NGN  0.268099  1.000000  0.500208  0.281954  0.083643\n",
       "PKR -0.000179  0.500208  1.000000  0.217237 -0.328430\n",
       "TRY  0.086245  0.281954  0.217237  1.000000  0.182509\n",
       "ZAR  0.383340  0.083643 -0.328430  0.182509  1.000000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.pivot(index='date',columns='ccy',values='ret').corr(min_periods=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae680a43",
   "metadata": {},
   "source": [
    "Factor regression (HAC): `market_factor_regs.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0c4bed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>coef</th>\n",
       "      <th>tstat_hac</th>\n",
       "      <th>pvalue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>const</td>\n",
       "      <td>-0.008329</td>\n",
       "      <td>-1.643345</td>\n",
       "      <td>0.100312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>usd_factor</td>\n",
       "      <td>0.095932</td>\n",
       "      <td>0.439142</td>\n",
       "      <td>0.660559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rates5_change</td>\n",
       "      <td>-1.405160</td>\n",
       "      <td>-2.396050</td>\n",
       "      <td>0.016573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carry_spread</td>\n",
       "      <td>0.060957</td>\n",
       "      <td>1.603590</td>\n",
       "      <td>0.108804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gbp_ois_change</td>\n",
       "      <td>-2.131167</td>\n",
       "      <td>-0.677810</td>\n",
       "      <td>0.497892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gbp_s5_change</td>\n",
       "      <td>-0.203299</td>\n",
       "      <td>-0.112413</td>\n",
       "      <td>0.910496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>R2</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>R2_adj</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             term      coef  tstat_hac    pvalue\n",
       "0           const -0.008329  -1.643345  0.100312\n",
       "1      usd_factor  0.095932   0.439142  0.660559\n",
       "2   rates5_change -1.405160  -2.396050  0.016573\n",
       "3    carry_spread  0.060957   1.603590  0.108804\n",
       "4  gbp_ois_change -2.131167  -0.677810  0.497892\n",
       "5   gbp_s5_change -0.203299  -0.112413  0.910496\n",
       "6              R2  0.020624        NaN       NaN\n",
       "7          R2_adj  0.006984        NaN       NaN"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_tbl.reset_index().rename(columns={\"index\":\"term\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2224fc",
   "metadata": {},
   "source": [
    "Sharpe variance shares: `sharpe_variance_contrib_fixed.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd3a7d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ccy</th>\n",
       "      <th>mean_ret</th>\n",
       "      <th>marg_var_contrib</th>\n",
       "      <th>marg_var_share</th>\n",
       "      <th>total_pnl_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRY</td>\n",
       "      <td>-0.007845</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.502698</td>\n",
       "      <td>-6.116739e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRL</td>\n",
       "      <td>-0.000198</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.360321</td>\n",
       "      <td>-1.442470e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NGN</td>\n",
       "      <td>0.001989</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.113144</td>\n",
       "      <td>1.451724e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PKR</td>\n",
       "      <td>-0.003122</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.023837</td>\n",
       "      <td>-2.279261e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZAR</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ccy  mean_ret  marg_var_contrib  marg_var_share  total_pnl_usd\n",
       "3  TRY -0.007845          0.001002        0.502698  -6.116739e+06\n",
       "0  BRL -0.000198          0.000719        0.360321  -1.442470e+05\n",
       "1  NGN  0.001989          0.000226        0.113144   1.451724e+06\n",
       "2  PKR -0.003122          0.000048        0.023837  -2.279261e+06\n",
       "4  ZAR  0.000000          0.000000        0.000000   0.000000e+00"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_contrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce65f3ab",
   "metadata": {},
   "source": [
    "Worst-10-week drawdown contributors: `drawdown_contrib_worst10w.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d0812395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ccy</th>\n",
       "      <th>pnl_usd_worst10w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRY</td>\n",
       "      <td>-7.334185e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BRL</td>\n",
       "      <td>-3.098137e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NGN</td>\n",
       "      <td>-1.508931e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PKR</td>\n",
       "      <td>-1.360122e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZAR</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ccy  pnl_usd_worst10w\n",
       "3  TRY     -7.334185e+06\n",
       "0  BRL     -3.098137e+06\n",
       "1  NGN     -1.508931e+06\n",
       "2  PKR     -1.360122e+06\n",
       "4  ZAR      0.000000e+00"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_contrib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
