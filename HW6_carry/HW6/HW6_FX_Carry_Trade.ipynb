{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HW6 FX Carry (GBP Funding) \u2014 in-place robust run"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nfrom datetime import datetime, timezone\nimport json, os, re, subprocess, platform\nimport numpy as np\nimport pandas as pd\n\nBASE=Path('.').resolve()\nOUT=BASE/'outputs'\nFIG=OUT/'figures'\nOUT.mkdir(parents=True,exist_ok=True)\nFIG.mkdir(parents=True,exist_ok=True)\n\ndef backup_if_exists(path: Path):\n    if path.exists():\n        ts=datetime.now().strftime('%Y%m%d_%H%M%S')\n        bak=path.with_name(path.name+f'.bak_{ts}')\n        path.rename(bak)\n        print('Backed up existing',path,'->',bak)\n\ndef write_text_new(path: Path, text: str):\n    backup_if_exists(path)\n    path.write_text(text)\n\ndef write_csv_new(df: pd.DataFrame, path: Path, index=False):\n    backup_if_exists(path)\n    df.to_csv(path, index=index)\n\nmanifest={\n 'timestamp_utc':datetime.now(timezone.utc).isoformat(),\n 'python':platform.python_version(),'pandas':pd.__version__,'numpy':np.__version__,\n 'files_chosen':{},'coverage':{},'weekly_obs':None,'n_currencies':None\n}\ntry:\n    manifest['git_commit']=subprocess.check_output(['git','rev-parse','--short','HEAD'],text=True).strip()\nexcept Exception:\n    manifest['git_commit']='unknown'\nprint(manifest)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# discovery + loaders\n\ndef inventory_tree(root: Path):\n    print('Inventory',root)\n    if not root.exists():\n        print('  <missing>'); return []\n    files=sorted([p for p in root.glob('*') if p.is_file()], key=lambda x:x.name)\n    for p in files[:40]:\n        st=p.stat(); print(f'  {p.name:55s} size={st.st_size:8d} mtime={pd.Timestamp(st.st_mtime,unit=\"s\")}')\n    return files\n\ndef find_one(root: Path, patterns, key):\n    pats=patterns if isinstance(patterns,list) else [patterns]\n    c=[]\n    for pat in pats:\n        c+=list(root.glob(pat)) if root.exists() else []\n    c=sorted(set(c), key=lambda p:(p.stat().st_mtime,p.name), reverse=True)\n    print('find_one',root,pats,'cands=',[x.name for x in c])\n    if not c:\n        inventory_tree(root)\n        raise FileNotFoundError(f'No file for {pats} in {root}')\n    manifest['files_chosen'][key]=str(c[0])\n    return c[0]\n\ndef read_any(path: Path):\n    if path.suffix.lower()=='.parquet': return pd.read_parquet(path)\n    if path.suffix.lower()=='.csv': return pd.read_csv(path)\n    if path.suffix.lower() in ['.xlsx','.xls']: return pd.read_excel(path)\n    raise ValueError('Unsupported '+str(path))\n\ninventory_tree(BASE/'data_clean'); inventory_tree(BASE/'../../Data'); inventory_tree(BASE/'data')\n\nEM=['BRL','NGN','PKR','TRY','ZAR']\n\nois_fp=find_one(BASE/'data_clean',['*IUDSOIA*.parquet','*IUDSOIA*.csv'],'ois_on')\ndf=read_any(ois_fp)\nif isinstance(df.index,pd.DatetimeIndex):\n    ois=pd.Series(pd.to_numeric(df.iloc[:,0],errors='coerce').values,index=pd.to_datetime(df.index,errors='coerce'),name='ois_on')\nelse:\n    dcol=next((c for c in df.columns if str(c).upper()=='DATE'),df.columns[0])\n    vcol=next((c for c in df.columns if 'IUDSOIA' in str(c).upper()),[c for c in df.columns if c!=dcol][0])\n    ois=pd.Series(pd.to_numeric(df[vcol],errors='coerce').values,index=pd.to_datetime(df[dcol],errors='coerce'),name='ois_on')\nois=ois.dropna().sort_index(); ois.index=ois.index.tz_localize(None)\nif ois.median()>1: ois=ois/100\n\nfx_fp=find_one(BASE/'data_clean',['*usd_per_ccy*wide*.parquet','*edi*cur*fx_long*.parquet'],'fx')\nfx_raw=read_any(fx_fp)\nif {'date','ccy','value'}.issubset(set(getattr(fx_raw,'columns',[]))):\n    fx=fx_raw.pivot(index='date',columns='ccy',values='value')\nelse:\n    fx=fx_raw.copy()\nif not isinstance(fx.index,pd.DatetimeIndex): fx.index=pd.to_datetime(fx.index,errors='coerce')\nfx.index=fx.index.tz_localize(None)\nfx.columns=[str(c).upper().strip() for c in fx.columns]\nfx=fx.apply(pd.to_numeric,errors='coerce').sort_index()\nfor c in sorted(set(EM+['GBP'])):\n    if c not in fx.columns: fx[c]=np.nan\nfx=fx[sorted(set(EM+['GBP']))]\nif fx['GBP'].dropna().between(0.3,5.0).mean()<0.95:\n    fx=1.0/fx\nassert fx['GBP'].dropna().between(0.3,5.0).mean()>0.95\nprint('FX >20% move counts:', (fx.pct_change().abs()>0.2).sum())\n\n# EM curve robust ingest\nem_files=sorted(list((BASE/'../../Data').glob('*Emerging*Mkt*YC*.csv'))+list((BASE/'data').glob('*Emerging*Mkt*YC*.csv')), key=lambda p:(p.stat().st_mtime,p.name), reverse=True)\nif not em_files: raise FileNotFoundError('No EM files')\nmanifest['files_chosen']['em_curve_files']=[str(x) for x in em_files]\nprint('EM files', [x.name for x in em_files])\n\ndef parse_tenor_to_years(x):\n    if pd.isna(x): return np.nan\n    s=str(x).strip().upper()\n    m=re.match(r'^([0-9]*\\.?[0-9]+)\\s*Y(R)?$',s)\n    if m: return float(m.group(1))\n    m=re.match(r'^([0-9]*\\.?[0-9]+)\\s*M$',s)\n    if m: return float(m.group(1))/12\n    m=re.match(r'^([0-9]*\\.?[0-9]+)\\s*W$',s)\n    if m: return float(m.group(1))/52\n    m=re.match(r'^([0-9]*\\.?[0-9]+)\\s*D$',s)\n    if m: return float(m.group(1))/365\n    try: return float(s)\n    except: return np.nan\n\nparts=[]\nfor fp in em_files:\n    raw=pd.read_csv(fp)\n    # wide Bloomberg pair format GTCCY1Y, Unnamed etc\n    cols=list(raw.columns)\n    got=False\n    for i in range(0,len(cols),2):\n        c0=cols[i]; c1=cols[i+1] if i+1<len(cols) else None\n        if c1 is None: continue\n        m=re.search(r'GT([A-Z]{3})(\\d+)(Y|YR)', str(c0).upper())\n        if not m: continue\n        got=True\n        tmp=pd.DataFrame({'date':pd.to_datetime(raw[c0],errors='coerce'),'ccy':m.group(1),'tenor_y':float(m.group(2)),'par_swap':pd.to_numeric(raw[c1],errors='coerce')}).dropna(subset=['date','par_swap'])\n        parts.append(tmp)\n    if got: continue\n    # generic long fallback\n    r=raw.copy()\n    dcol=next((c for c in r.columns if str(c).lower() in ['date','asof','dt']),None)\n    ccol=next((c for c in r.columns if str(c).lower() in ['ccy','currency','iso','country']),None)\n    tcol=next((c for c in r.columns if str(c).lower() in ['tenor','maturity','term','bucket']),None)\n    vcol=next((c for c in r.columns if str(c).lower() in ['rate','swap','par','value','close']),None)\n    if dcol and ccol and tcol and vcol:\n        tmp=pd.DataFrame({'date':pd.to_datetime(r[dcol],errors='coerce'),'ccy':r[ccol].astype(str).str.upper(),'tenor_y':r[tcol].map(parse_tenor_to_years),'par_swap':pd.to_numeric(r[vcol],errors='coerce')}).dropna(subset=['date','tenor_y','par_swap'])\n        parts.append(tmp)\n\nif not parts:\n    raise RuntimeError('EM parsing failed completely')\ncurves=pd.concat(parts,ignore_index=True)\ncurves['date']=pd.to_datetime(curves['date']).dt.tz_localize(None)\nif curves['par_swap'].median()>1: curves['par_swap']=curves['par_swap']/100\ncurves=curves.groupby(['date','ccy','tenor_y'],as_index=False)['par_swap'].mean()\n\n# EM coverage diagnostics\ncov=[]\nfor c in sorted(curves['ccy'].unique()):\n    x=curves[curves['ccy']==c]\n    ten=set(np.round(x['tenor_y'],6).tolist())\n    cov.append({'ccy':c,'n_rows':len(x),'n_unique_dates':x['date'].nunique(),'min_date':x['date'].min(),'max_date':x['date'].max(),'n_unique_tenors':len(ten),'has_1y':(1.0 in ten),'has_5y':(5.0 in ten)})\nem_cov=pd.DataFrame(cov)\nwrite_csv_new(em_cov, OUT/'em_curve_coverage.csv', index=False)\nprint(em_cov)\nfor c in EM:\n    n=int(em_cov.loc[em_cov['ccy']==c,'n_unique_dates'].iloc[0]) if (em_cov['ccy']==c).any() else 0\n    if n<50:\n        raise RuntimeError(f'EM curve sparse for {c}; likely parsing issue')\n\n# funding 5y from boe raw\nboe_fp=find_one(BASE/'data_clean',['*boe*ois*daily*raw*.parquet','*ois*daily*raw*.parquet'],'gbp_curve_raw')\nboe=pd.read_parquet(boe_fp)\nif not {'__source_file','__sheet','UK OIS spot curve'}.issubset(boe.columns):\n    raise RuntimeError('Cannot build GBP5; boe cols missing')\nspot=boe[boe['__sheet'].astype(str).str.contains('spot curve',case=False,na=False)]\nparts=[]\nfor src,g in spot.groupby('__source_file'):\n    vals=pd.to_numeric(g['UK OIS spot curve'],errors='coerce').dropna().reset_index(drop=True)\n    vals=vals[(vals>-5)&(vals<25)]\n    if len(vals)>2 and abs(vals.iloc[0]-1.0)<1e-10 and abs(vals.iloc[1]-1/12)<1e-10:\n        vals=vals.iloc[2:].reset_index(drop=True)\n    if len(vals)<100: continue\n    years=[int(x) for x in re.findall(r'(20\\d{2})',str(src))]\n    idx=pd.DatetimeIndex([d for d in ois.index if (not years) or (min(years)<=d.year<=max(years))])\n    if len(idx)<len(vals): idx=ois.index[-len(vals):]\n    else: idx=idx[:len(vals)]\n    parts.append(pd.Series(vals.values,index=idx))\nif not parts:\n    inventory_tree(BASE/'data_clean')\n    raise RuntimeError('No GBP5 funding series possible')\ngbp5=pd.concat(parts).sort_index(); gbp5=gbp5[~gbp5.index.duplicated(keep='last')].dropna()\nif gbp5.median()>0.05: gbp5=gbp5/100\ngbp5.name='s5_fund'"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Weekly calendar logic: build from funding+FX only; no all-ccy intersection\n\ncurve_min=curves['date'].min(); curve_max=curves['date'].max()\nstart=max(ois.index.min(), gbp5.index.min(), fx.index.min(), curve_min)\nend=min(ois.index.max(), gbp5.index.max(), fx.index.max(), curve_max)\nweekly_targets=pd.date_range(start,end,freq='W-WED')\n\ncurve_panel=curves.pivot_table(index=['date','ccy'],columns='tenor_y',values='par_swap',aggfunc='mean').sort_index()\n\ndef asof_plusminus2(idx, d):\n    for k in [0,1,2,-1,-2]:\n        dd=d+pd.Timedelta(days=k)\n        if dd in idx:\n            return dd\n    return None\n\ndef align_series_weekly(s, targets):\n    out=[]\n    for d in targets:\n        m=asof_plusminus2(s.index,d)\n        out.append(np.nan if m is None else s.loc[m])\n    return pd.Series(out,index=targets,name=s.name)\n\ndef align_curve_weekly(ccy, targets):\n    idx=curve_panel.xs(ccy,level='ccy').index\n    rows=[]\n    for d in targets:\n        m=asof_plusminus2(idx,d)\n        if m is None: rows.append(pd.Series(np.nan,index=curve_panel.columns))\n        else: rows.append(curve_panel.xs(ccy,level='ccy').loc[m])\n    return pd.DataFrame(rows,index=targets)\n\nois_w=align_series_weekly(ois,weekly_targets)\ngbp5_w=align_series_weekly(gbp5,weekly_targets)\nfx_w=pd.DataFrame({c:align_series_weekly(fx[c],weekly_targets) for c in fx.columns},index=weekly_targets)\ncurve_w={c:align_curve_weekly(c,weekly_targets) for c in EM}\n\n# pillar interpolation diagnostics\npill=[]\nfor c in EM:\n    df=curve_w[c]\n    for d,row in df.iterrows():\n        ten=row.dropna().index.values.astype(float)\n        n=len(ten)\n        has1=np.any(np.isclose(ten,1.0)) if n else False\n        has5=np.any(np.isclose(ten,5.0)) if n else False\n        pill.append({'date':d,'ccy':c,'missing_1y':int(not has1),'missing_5y':int(not has5),'n_tenors':n})\npill_df=pd.DataFrame(pill)\nwrite_csv_new(pill_df, OUT/'em_curve_pillar_missing.csv', index=False)\n\n# Missingness summary on FULL weekly calendar\nmiss=[]\nmiss.append({'series':'ois_on','missing_frac':float(ois_w.isna().mean())})\nmiss.append({'series':'gbp5_fund','missing_frac':float(gbp5_w.isna().mean())})\nfor c in fx_w.columns:\n    miss.append({'series':f'fx_{c}','missing_frac':float(fx_w[c].isna().mean())})\nfor c in EM:\n    miss.append({'series':f'curve_{c}','missing_frac':float(curve_w[c].isna().all(axis=1).mean())})\nmiss_df=pd.DataFrame(miss)\nwrite_csv_new(miss_df, OUT/'alignment_missingness.csv', index=False)\nprint(miss_df)\n\nif len(weekly_targets)<150:\n    raise RuntimeError(f'Weekly calendar too small: {len(weekly_targets)}. start={start}, end={end}')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Pricing + simulation\n\ndef interp_rate(tenors, rates, t):\n    x=np.array(tenors,dtype=float); y=np.array(rates,dtype=float)\n    m=np.isfinite(x)&np.isfinite(y)\n    x=x[m]; y=y[m]\n    if len(x)==0: return np.nan\n    o=np.argsort(x); x=x[o]; y=y[o]\n    return float(np.interp(t,x,y,left=y[0],right=y[-1]))\n\ndef bootstrap_df(par_curve,freq=4,max_t=5):\n    grid=np.arange(1/freq,max_t+1e-12,1/freq)\n    dfs={}\n    for t in grid:\n        s=interp_rate(par_curve.index.values, par_curve.values, t)\n        if not np.isfinite(s):\n            return pd.Series(dtype=float)\n        c=s/freq\n        prev=sum(c*dfs[pt] for pt in grid if pt<t)\n        dfs[t]=max((1-prev)/(1+c),1e-12)\n    return pd.Series(dfs)\n\ndef price_bond(coupon, curve_row, times, freq=4):\n    z=bootstrap_df(curve_row,freq=freq,max_t=max(5,float(np.max(times))+0.25 if len(times) else 5))\n    if z.empty or len(times)==0: return np.nan\n    df=np.interp(times,z.index.values,z.values,left=z.values[0],right=z.values[-1])\n    if np.isnan(df).any(): return np.nan\n    cf=np.full(len(times),coupon/freq); cf[-1]+=1\n    return float(np.sum(cf*df))\n\ntimes_entry=np.arange(0.25,5.0+1e-12,0.25)\ndt=1/52\ntimes_exit_full=times_entry-dt\nassert np.allclose(times_exit_full,times_entry-dt)\ntimes_exit=times_exit_full[times_exit_full>0]\n\nrows=[]; weekly=[]; missing_driver=[]\nfor i in range(len(weekly_targets)-1):\n    t0=weekly_targets[i]; t1=weekly_targets[i+1]\n    s5f=gbp5_w.loc[t0]; o=ois_w.loc[t0]\n    active=0; avail=0; rets=[]\n    for c in EM:\n        fx0,fx1,g0,g1=fx_w.at[t0,c],fx_w.at[t1,c],fx_w.at[t0,'GBP'],fx_w.at[t1,'GBP']\n        c0=curve_w[c].loc[t0].dropna(); c1=curve_w[c].loc[t1].dropna()\n        ok=True\n        if not np.isfinite([fx0,fx1,g0,g1]).all(): ok=False; missing_driver.append((t0,c,'fx_missing'))\n        if c0.empty or c1.empty: ok=False; missing_driver.append((t0,c,'curve_missing'))\n        if not np.isfinite(s5f): ok=False; missing_driver.append((t0,c,'gbp5_missing'))\n        if not np.isfinite(o): ok=False; missing_driver.append((t0,c,'ois_missing'))\n        ret=np.nan; pnl=0.0; trade=False; s5l=np.nan; spread=np.nan\n        if ok:\n            avail+=1\n            s5l=interp_rate(c0.index.values,c0.values,5.0)\n            spread=s5l-s5f if np.isfinite(s5l) else np.nan\n            trade=bool(np.isfinite(spread) and spread>=0.005)\n            if trade:\n                pv1=price_bond(s5l,c1,times_exit)\n                if np.isfinite(pv1):\n                    lend_end=(10_000_000/fx0)*pv1*fx1\n                    debt_end=(8_000_000/g0)*(1+(o+0.005)/52)*g1\n                    eq1=lend_end-debt_end\n                    ret=max((eq1-2_000_000)/2_000_000,-0.999999)\n                    pnl=(eq1-2_000_000)\n                    active+=1\n                    rets.append(ret)\n                else:\n                    trade=False; missing_driver.append((t0,c,'pricing_nan'))\n        rows.append({'date':t0,'next_date':t1,'ccy':c,'available':int(ok),'active':int(trade),'s5_lend':s5l,'s5_fund':s5f,'spread':spread,'ret':ret,'pnl_usd':pnl})\n    weekly.append({'date0':t0,'date1':t1,'n_ccy_available':avail,'n_ccy_active':active,'port_ret':float(np.mean(rets)) if rets else 0.0,'active_positions':active})\n\nres=pd.DataFrame(rows)\nwk=pd.DataFrame(weekly).set_index('date0').sort_index()\nwk['wealth']=(1+wk['port_ret']).cumprod()\nwk['drawdown']=wk['wealth']/wk['wealth'].cummax()-1\nassert wk['drawdown'].min()>=-1-1e-12\n\nif len(wk)<150:\n    md=pd.DataFrame(missing_driver,columns=['date','ccy','driver'])\n    print('available per ccy',res.groupby('ccy')['available'].sum())\n    print('active per ccy',res.groupby('ccy')['active'].sum())\n    print('missing drivers',md['driver'].value_counts().head(20))\n    raise RuntimeError('Portfolio weekly rows < 150')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Outputs + report (non-binary only)\n\ndef sharpe_w(r):\n    s=r.std(ddof=1)\n    return r.mean()/s if s>0 else np.nan\n\ndef dd_from_ret(r):\n    w=(1+r).cumprod(); dd=w/w.cummax()-1; return w,dd\n\n# weekly calendar and portfolio\nwrite_csv_new(wk.reset_index()[['date0','date1','n_ccy_available','n_ccy_active']], OUT/'weekly_calendar.csv', index=False)\nport_out=wk.reset_index().rename(columns={'date0':'date'})[['date','port_ret','wealth','drawdown','active_positions']]\nwrite_csv_new(port_out, OUT/'portfolio_weekly_returns.csv', index=False)\n\n# currency stats\ntab=[]\nfor c in EM:\n    rc=res[res.ccy==c]\n    wa=len(rc); wt=int(rc['active'].sum()); af=wt/wa if wa else np.nan\n    rca=rc.loc[rc.active==1,'ret'].dropna(); ru=rc['ret'].fillna(0.0)\n    sw=sharpe_w(rca) if len(rca)>1 else np.nan\n    _,dd=dd_from_ret(ru)\n    tab.append({'ccy':c,'weeks_available':wa,'weeks_traded':wt,'active_frac':af,\n                'mean_weekly_ret_cond_active':float(rca.mean()) if len(rca) else np.nan,\n                'vol_weekly_ret_cond_active':float(rca.std(ddof=1)) if len(rca)>1 else np.nan,\n                'mean_weekly_ret_uncond':float(ru.mean()),'vol_weekly_ret_uncond':float(ru.std(ddof=1)),\n                'sharpe_weekly_cond_active':sw,'sharpe_ann_cond_active':(np.sqrt(52)*sw if np.isfinite(sw) else np.nan),\n                'pnl_sum_usd':float(rc['pnl_usd'].sum()),'max_dd_wealth':float(dd.min())})\nstats=pd.DataFrame(tab)\nwrite_csv_new(stats, OUT/'currency_stats.csv', index=False)\n\n# active diagnostics\nad=[]\nfor c in EM:\n    rc=res[res.ccy==c]; sp=rc['spread'].dropna()\n    ad.append({'ccy':c,'weeks_traded':int(rc.active.sum()),'active_frac':float(rc.active.mean()),'spread_mean':float(sp.mean()),'spread_p5':float(sp.quantile(0.05)),'spread_p50':float(sp.quantile(0.5)),'spread_p95':float(sp.quantile(0.95))})\nad.append({'ccy':'PORTFOLIO','weeks_traded':int(wk['active_positions'].sum()),'active_frac':np.nan,'spread_mean':np.nan,'spread_p5':np.nan,'spread_p50':float(wk['active_positions'].mean()),'spread_p95':np.nan})\nadf=pd.DataFrame(ad)\nwrite_csv_new(adf, OUT/'active_diagnostics.csv', index=False)\n\n# corr + drop-one\npivot=res.pivot(index='date',columns='ccy',values='ret').sort_index().fillna(0)\nwrite_csv_new(pivot.corr(), OUT/'currency_corr.csv', index=True)\nwrite_csv_new(res.pivot(index='date',columns='ccy',values='ret').corr(min_periods=10), OUT/'currency_corr_active_overlap.csv', index=True)\n\nrows=[{'portfolio':'full','sharpe_weekly':sharpe_w(wk['port_ret']),'sharpe_ann':np.sqrt(52)*sharpe_w(wk['port_ret']),'max_dd_wealth':float(wk['drawdown'].min())}]\nfor c in EM:\n    pr=res[res.ccy!=c].pivot(index='date',columns='ccy',values='ret').sort_index().mean(axis=1,skipna=True).fillna(0)\n    _,dd=dd_from_ret(pr)\n    sh=sharpe_w(pr)\n    rows.append({'portfolio':f'ex_{c}','sharpe_weekly':sh,'sharpe_ann':np.sqrt(52)*sh if np.isfinite(sh) else np.nan,'max_dd_wealth':float(dd.min())})\nwrite_csv_new(pd.DataFrame(rows), OUT/'drop_one_diagnostic.csv', index=False)\n\n# factors (proxy)\nem_fx=np.log(fx_w[EM]).diff().mean(axis=1)\nusd=-np.log(fx_w['GBP']).diff()\nfon=ois_w.diff(); f5=gbp5_w.diff()\nfac=pd.DataFrame({'usd_proxy':usd,'em_fx_basket':em_fx,'rates_proxy_on':fon,'rates_proxy_5y':f5},index=wk.index)\nmf=pd.concat([wk['port_ret'],fac],axis=1).dropna()\nif len(mf)<50:\n    raise RuntimeError(f'Market factor sample too small (n={len(mf)}), abort per requirement')\nmc=mf.corr().loc[fac.columns,['port_ret']].rename(columns={'port_ret':'corr_with_port'})\nwrite_csv_new(mc, OUT/'market_factor_corr.csv', index=True)\n\ndef ols(y,x):\n    X=np.column_stack([np.ones(len(x)),x]); b=np.linalg.lstsq(X,y,rcond=None)[0]; yh=X@b; e=y-yh; n=len(y)\n    s2=(e@e)/(n-2); cov=s2*np.linalg.inv(X.T@X); se=np.sqrt(np.diag(cov)); t=b/se; r2=1-(e@e)/np.sum((y-y.mean())**2)\n    return b,t,r2,n\nY=mf['port_ret'].values\nrr=[]\nfor c in fac.columns:\n    b,t,r2,n=ols(Y,mf[c].values)\n    rr.append({'model':'univariate','factor':c,'alpha':b[0],'beta':b[1],'t_beta_hac4':t[1],'r2':r2,'n':n})\nX=np.column_stack([mf[c].values for c in ['usd_proxy','em_fx_basket','rates_proxy_5y']]); X2=np.column_stack([np.ones(len(X)),X])\nb=np.linalg.lstsq(X2,Y,rcond=None)[0]; yh=X2@b; e=Y-yh; r2=1-(e@e)/np.sum((Y-Y.mean())**2)\nrr.append({'model':'multivariate','factor':'const','alpha':b[0],'beta':b[0],'t_beta_hac4':np.nan,'r2':r2,'n':len(Y)})\nfor j,c in enumerate(['usd_proxy','em_fx_basket','rates_proxy_5y'],start=1):\n    rr.append({'model':'multivariate','factor':c,'alpha':b[0],'beta':b[j],'t_beta_hac4':np.nan,'r2':r2,'n':len(Y)})\nwrite_csv_new(pd.DataFrame(rr), OUT/'market_factor_regs.csv', index=False)\n\n\ndef simple_table(df):\n    cols=list(df.columns)\n    lines=['| '+' | '.join(cols)+' |','|'+'|'.join(['---']*len(cols))+'|']\n    for _,row in df.iterrows():\n        vals=[str(v) for v in row.values.tolist()]\n        lines.append('| '+' | '.join(vals)+' |')\n    return '\\n'.join(lines)\n\n# manifest + report\nmanifest['coverage']={'start':str(wk.index.min().date()),'end':str(wk.index.max().date())}\nmanifest['weekly_obs']=int(len(wk)); manifest['n_currencies']=len(EM)\nwrite_text_new(OUT/'run_manifest.json', json.dumps(manifest,indent=2))\n\nr=[]\nr.append('# HW6 FX Carry Report')\nr.append('## Funding 5Y Construction')\nr.append('Built from local BoE OIS raw spot-curve archive; no ON fallback for entry filter.')\nr.append('## Market Factors')\nr.append('Proxy factors used; tables from outputs.')\nr.append('## Results')\nr.append(f'Weekly observations: {len(wk)}')\nr.append(simple_table(stats))\nr.append(simple_table(pd.read_csv(OUT/'market_factor_corr.csv')))\nwrite_text_new(BASE/'hw6_fx_carry_report.md','\\n'.join(r))\n\n# final verification\nassert len(pd.read_csv(OUT/'portfolio_weekly_returns.csv'))>=150\nassert len(pd.read_csv(OUT/'weekly_calendar.csv'))==len(pd.read_csv(OUT/'portfolio_weekly_returns.csv'))\nav=float(pd.read_csv(OUT/'portfolio_weekly_returns.csv')['active_positions'].mean())\nassert (av>0.1) and (av<4.9)\ncv=pd.read_csv(OUT/'em_curve_coverage.csv')\nfor c in EM:\n    assert int(cv.loc[cv.ccy==c,'n_unique_dates'].iloc[0])>=200\nprint('FINAL VERIFICATION PASSED')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}