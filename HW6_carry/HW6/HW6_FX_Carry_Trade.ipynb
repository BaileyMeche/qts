{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HW6 FX Carry (GBP Funding) \u2014 Single Source of Truth"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# RUN MANIFEST + environment\nfrom pathlib import Path\nfrom datetime import datetime, timezone\nimport json, subprocess, platform, re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nBASE = Path('.').resolve()\nOUTPUT_DIR = BASE / 'outputs'\nFIG_DIR = OUTPUT_DIR / 'figures'\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nFIG_DIR.mkdir(parents=True, exist_ok=True)\n\nmanifest = {\n    'timestamp_utc': datetime.now(timezone.utc).isoformat(),\n    'python': platform.python_version(),\n    'pandas': pd.__version__,\n    'numpy': np.__version__,\n    'base_dir': str(BASE),\n    'files_chosen': {},\n    'coverage': {},\n    'weekly_obs': None,\n    'n_currencies': None,\n}\n\ntry:\n    git_commit = subprocess.check_output(['git', 'rev-parse', '--short', 'HEAD'], text=True).strip()\nexcept Exception:\n    git_commit = 'unknown'\nmanifest['git_commit'] = git_commit\nprint('timestamp_utc:', manifest['timestamp_utc'])\nprint('git_commit:', git_commit)\nprint('python/pandas/numpy:', manifest['python'], manifest['pandas'], manifest['numpy'])"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Deterministic discovery utilities\n\ndef inventory_tree(root: Path, limit: int = 30):\n    print(f'Inventory: {root}')\n    if not root.exists():\n        print('  <missing>')\n        return []\n    files = sorted([p for p in root.glob('*') if p.is_file()], key=lambda x: x.name)\n    for p in files[:limit]:\n        st = p.stat()\n        print(f'  {p.name:55s} size={st.st_size:8d} mtime={pd.Timestamp(st.st_mtime, unit=\"s\")}')\n    return files\n\n\ndef find_one(root: Path, patterns, manifest_key: str):\n    if isinstance(patterns, str):\n        patterns = [patterns]\n    candidates = []\n    for pat in patterns:\n        candidates.extend(list(root.glob(pat)))\n    candidates = sorted(set(candidates), key=lambda p: (p.stat().st_mtime, p.name), reverse=True)\n    print(f'find_one root={root} patterns={patterns}')\n    print('  candidates:', [c.name for c in candidates])\n    if not candidates:\n        inventory_tree(root)\n        raise FileNotFoundError(f'No file matched patterns={patterns} in {root}')\n    chosen = candidates[0]\n    print('  chosen:', chosen)\n    manifest['files_chosen'][manifest_key] = str(chosen)\n    return chosen\n\n\ndef read_any(path: Path):\n    suf = path.suffix.lower()\n    if suf == '.parquet':\n        return pd.read_parquet(path)\n    if suf == '.csv':\n        return pd.read_csv(path)\n    if suf in ('.xlsx', '.xls'):\n        return pd.read_excel(path)\n    raise ValueError(f'Unsupported file type: {path}')\n\ninventory_tree(BASE/'data_clean')\ninventory_tree(BASE/'../../Data')\ninventory_tree(BASE/'data')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Data loading\nEM = ['BRL','NGN','PKR','TRY','ZAR']\n\n# OIS overnight (accrual)\nois_fp = find_one(BASE/'data_clean', ['*IUDSOIA*.parquet','*IUDSOIA*.csv'], 'ois_overnight')\nois_df = read_any(ois_fp)\nif isinstance(ois_df.index, pd.DatetimeIndex):\n    ois = pd.Series(pd.to_numeric(ois_df.iloc[:,0], errors='coerce').values,\n                    index=pd.to_datetime(ois_df.index, errors='coerce'), name='ois_on')\nelse:\n    dcol = next((c for c in ois_df.columns if str(c).upper()=='DATE'), ois_df.columns[0])\n    vcol = next((c for c in ois_df.columns if 'IUDSOIA' in str(c).upper()), [c for c in ois_df.columns if c!=dcol][0])\n    ois = pd.Series(pd.to_numeric(ois_df[vcol], errors='coerce').values,\n                    index=pd.to_datetime(ois_df[dcol], errors='coerce'), name='ois_on')\nois = ois.dropna().sort_index()\nois.index = ois.index.tz_localize(None)\nif ois.median() > 1:\n    ois = ois / 100.0\n\n# FX USD per CCY\nfx_fp = find_one(BASE/'data_clean', ['*usd_per_ccy*wide*.parquet','*edi*cur*fx_long*.parquet'], 'fx')\nfx_raw = read_any(fx_fp)\nif {'date','ccy','value'}.issubset(set(getattr(fx_raw,'columns',[]))):\n    fx = fx_raw.pivot(index='date', columns='ccy', values='value')\nelse:\n    fx = fx_raw.copy()\nif not isinstance(fx.index, pd.DatetimeIndex):\n    fx.index = pd.to_datetime(fx.index, errors='coerce')\nfx.index = fx.index.tz_localize(None)\nfx.columns = [str(c).upper().strip() for c in fx.columns]\nfx = fx.apply(pd.to_numeric, errors='coerce').sort_index()\nfor c in sorted(set(EM+['GBP'])):\n    if c not in fx.columns:\n        fx[c] = np.nan\nfx = fx[sorted(set(EM+['GBP']))]\n\ngbp = fx['GBP'].dropna()\nif gbp.between(0.3,5.0).mean() < 0.95:\n    fx = 1.0 / fx\n    print('FX auto-inversion applied (GBP range sanity).')\nassert fx['GBP'].dropna().between(0.3,5.0).mean() > 0.95, 'GBP quote sanity failed after inversion logic.'\nprint('FX >20% daily move counts:')\nprint((fx.pct_change().abs()>0.2).sum())\n\n# EM curves from ../../Data preferred, fallback ./data\nem_candidates = sorted(list((BASE/'../../Data').glob('*Emerging*Mkt*YC*.csv')) + list((BASE/'data').glob('*Emerging*Mkt*YC*.csv')),\n                       key=lambda p:(p.stat().st_mtime,p.name), reverse=True)\nif not em_candidates:\n    raise FileNotFoundError('No EM curve csv found in ../../Data or ./data')\nprint('EM candidates:', [x.name for x in em_candidates])\nmanifest['files_chosen']['em_curve_files'] = [str(x) for x in em_candidates]\n\ndef parse_em_csv(fp):\n    raw = pd.read_csv(fp)\n    out=[]\n    cols=list(raw.columns)\n    for i in range(0,len(cols),2):\n        c0 = cols[i]\n        c1 = cols[i+1] if i+1 < len(cols) else None\n        if c1 is None:\n            continue\n        m = re.search(r'GT([A-Z]{3})(\\d+)(Y|YR)', str(c0).upper())\n        if not m:\n            continue\n        tmp = pd.DataFrame({\n            'date': pd.to_datetime(raw[c0], errors='coerce'),\n            'ccy': m.group(1),\n            'tenor': float(m.group(2)),\n            'rate': pd.to_numeric(raw[c1], errors='coerce')\n        }).dropna(subset=['date','rate'])\n        out.append(tmp)\n    if not out:\n        raise RuntimeError(f'No parsable columns in {fp}')\n    return pd.concat(out, ignore_index=True)\n\ncurves = pd.concat([parse_em_csv(f) for f in em_candidates], ignore_index=True)\ncurves['date'] = pd.to_datetime(curves['date']).dt.tz_localize(None)\nif curves['rate'].median() > 1:\n    curves['rate'] = curves['rate']/100.0\ncurves = curves.groupby(['date','ccy','tenor'], as_index=False)['rate'].mean()\n\n# Funding 5Y construction (fail loudly if impossible)\nboe_raw_fp = find_one(BASE/'data_clean', ['*boe*ois*daily*raw*.parquet','*ois*daily*raw*.parquet'], 'gbp_curve_raw')\nboe_raw = pd.read_parquet(boe_raw_fp)\n\nrequired_cols = {'__source_file','__sheet','UK OIS spot curve'}\nif not required_cols.issubset(boe_raw.columns):\n    print('Columns seen in boe raw:', boe_raw.columns.tolist())\n    raise RuntimeError('Cannot construct GBP 5Y: required columns missing in boe raw dataset')\n\nspot_rows = boe_raw[boe_raw['__sheet'].astype(str).str.contains('spot curve', case=False, na=False)].copy()\nif spot_rows.empty:\n    raise RuntimeError('Cannot construct GBP 5Y: no spot curve rows available')\n\nparts=[]\nfor src, g in spot_rows.groupby('__source_file'):\n    vals = pd.to_numeric(g['UK OIS spot curve'], errors='coerce').dropna().reset_index(drop=True)\n    vals = vals[(vals>-5) & (vals<30)]\n    # remove tenor marker rows if present\n    if len(vals)>2 and abs(vals.iloc[0]-1.0)<1e-10 and abs(vals.iloc[1]-1/12)<1e-10:\n        vals = vals.iloc[2:].reset_index(drop=True)\n    if len(vals)<100:\n        continue\n    years = [int(x) for x in re.findall(r'(20\\d{2})', str(src))]\n    if years:\n        y0,y1 = min(years), max(years)\n        idx = pd.DatetimeIndex([d for d in ois.index if y0<=d.year<=y1])\n    else:\n        idx = ois.index\n    if len(idx) < len(vals):\n        idx = ois.index[-len(vals):]\n    else:\n        idx = idx[:len(vals)]\n    parts.append(pd.Series(vals.values, index=idx))\n\nif not parts:\n    print('data_clean inventory:'); inventory_tree(BASE/'data_clean')\n    print('patterns attempted: *boe*ois*daily*raw*.parquet, *ois*daily*raw*.parquet')\n    print('boe raw columns:', boe_raw.columns.tolist())\n    raise RuntimeError('Cannot construct GBP 5Y funding series from local data.')\n\ngbp5 = pd.concat(parts).sort_index()\ngbp5 = gbp5[~gbp5.index.duplicated(keep='last')].dropna()\n# BoE spot values appear in percent units; convert to decimal robustly\na = gbp5.median()\nif a > 0.2:\n    gbp5 = gbp5 / 100.0\ngbp5.name='s5_fund'\nmanifest['files_chosen']['gbp5y_method'] = 'BoE OIS spot-curve raw parsed into GBP 5Y-like funding proxy from local archive'"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Canonical weekly alignment: WED with asof forward within +2 days\n\ndef align_forward_2d(obj, targets):\n    idx = obj.index\n    if isinstance(obj, pd.Series):\n        vals=[]\n        for d in targets:\n            cand=[d+pd.Timedelta(days=k) for k in [0,1,2]]\n            c=next((x for x in cand if x in idx), None)\n            vals.append(np.nan if c is None else obj.loc[c])\n        return pd.Series(vals, index=targets, name=obj.name)\n    rows=[]\n    for d in targets:\n        cand=[d+pd.Timedelta(days=k) for k in [0,1,2]]\n        c=next((x for x in cand if x in idx), None)\n        if c is None:\n            rows.append(pd.Series(np.nan, index=obj.columns))\n        else:\n            rows.append(pd.Series(obj.loc[c].values, index=obj.columns))\n    return pd.DataFrame(rows, index=targets)\n\ncurve_wide = curves.pivot_table(index=['date','ccy'], columns='tenor', values='rate', aggfunc='mean').sort_index()\n\n# initial broad window\nt_start = max(ois.index.min(), gbp5.index.min(), fx.index.min(), curves['date'].min())\nt_end = min(ois.index.max(), gbp5.index.max(), fx.index.max(), curves['date'].max())\nall_targets = pd.date_range(t_start, t_end, freq='W-WED')\n\ndef has_match(idx, d):\n    return any((d+pd.Timedelta(days=k)) in idx for k in [0,1,2])\n\n# keep only weeks where all required datasets have forward+2d matches\nvalid_targets=[]\ncurve_idx = {c: curve_wide.xs(c, level='ccy').index for c in EM}\nfor d in all_targets:\n    ok = has_match(ois.index,d) and has_match(gbp5.index,d) and has_match(fx.index,d)\n    if ok:\n        for c in EM:\n            ok = ok and has_match(curve_idx[c], d)\n            if not ok:\n                break\n    if ok:\n        valid_targets.append(d)\ntargets = pd.DatetimeIndex(valid_targets)\n\nif len(targets)==0:\n    raise RuntimeError('No valid weekly targets after forward+2d alignment constraints')\n\nois_w = align_forward_2d(ois, targets)\ngbp5_w = align_forward_2d(gbp5, targets)\nfx_w = align_forward_2d(fx, targets)\ndates = targets\ncurve_ccy = {c: align_forward_2d(curve_wide.xs(c,level='ccy'), targets) for c in EM}\n\n# Missingness checks\nmiss = {}\nmiss['ois_on'] = float(ois_w.isna().mean())\nmiss['gbp5_fund'] = float(gbp5_w.isna().mean())\nfor c in fx_w.columns:\n    miss[f'fx_{c}'] = float(fx_w[c].isna().mean())\nfor c in EM:\n    miss[f'curve_{c}'] = float(curve_ccy[c].isna().all(axis=1).mean())\nmiss_df = pd.DataFrame({'series':list(miss.keys()), 'missing_frac':list(miss.values())})\nmiss_df.to_csv(OUTPUT_DIR/'alignment_missingness.csv', index=False)\nprint(miss_df)\n\nfor c in EM:\n    if miss[f'fx_{c}'] > 0.05:\n        raise RuntimeError(f'FX missing >5% for {c}')\n    if miss[f'curve_{c}'] > 0.05:\n        raise RuntimeError(f'Curve missing >5% for {c}')\nif miss['gbp5_fund'] > 0.05:\n    raise RuntimeError('GBP funding 5Y missing >5%')\n\n# fill small residual holes after guardrail\nois_w = ois_w.ffill()\ngbp5_w = gbp5_w.ffill()\nfx_w = fx_w.ffill()\nfor c in EM:\n    curve_ccy[c] = curve_ccy[c].ffill()\n\nmanifest['coverage'] = {\n    'start': str(dates.min().date()),\n    'end': str(dates.max().date()),\n    'weekly_obs': int(len(dates))\n}\nmanifest['weekly_obs'] = int(len(dates))\nmanifest['n_currencies'] = int(len(EM))\nprint('Weekly targets kept:', len(dates), dates.min().date(), dates.max().date())\n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Curve pricing and MTM shift validation\n\ndef interp_rate(tenors, rates, t):\n    x=np.array(tenors,dtype=float); y=np.array(rates,dtype=float)\n    m=np.isfinite(x)&np.isfinite(y)\n    x=x[m]; y=y[m]\n    if len(x)==0: return np.nan\n    o=np.argsort(x); x=x[o]; y=y[o]\n    return float(np.interp(t, x, y, left=y[0], right=y[-1]))\n\ndef bootstrap_df_from_par(par_curve, freq=4, max_t=5.0):\n    grid=np.arange(1/freq, max_t+1e-12, 1/freq)\n    dfs={}\n    for t in grid:\n        s=interp_rate(par_curve.index.values, par_curve.values, t)\n        c=s/freq\n        prev=sum(c*dfs[pt] for pt in grid if pt<t)\n        dfs[t]=max((1-prev)/(1+c),1e-12)\n    return pd.Series(dfs)\n\ndef price_bond_shifted(coupon_rate, curve_row, times, freq=4):\n    if len(times)==0:\n        return 1.0\n    z=bootstrap_df_from_par(curve_row, freq=freq, max_t=max(5.0,float(np.max(times))+0.25))\n    df=np.interp(times, z.index.values, z.values, left=z.values[0], right=z.values[-1])\n    cf=np.full(len(times), coupon_rate/freq)\n    cf[-1]+=1.0\n    return float(np.sum(cf*df))\n\ntimes_entry = np.arange(0.25, 5.0+1e-12, 0.25)\ndt = 1/52\ntimes_exit_full = times_entry - dt\ntimes_exit = times_exit_full[times_exit_full > 0]\nprint('entry first/last5:', times_entry[:5], times_entry[-5:])\nprint('exit_full first/last5:', times_exit_full[:5], times_exit_full[-5:])\nassert np.allclose(times_exit_full, times_entry - dt)\n\n# par sanity sample\nrng=np.random.default_rng(7)\npar_checks=[]\nfor c in EM:\n    cdf=curve_ccy[c]\n    valid_idx=np.where(~cdf.isna().all(axis=1))[0]\n    if len(valid_idx)==0: continue\n    picks=rng.choice(valid_idx, size=min(5,len(valid_idx)), replace=False)\n    for j in picks:\n        row=cdf.iloc[int(j)].dropna()\n        s5=interp_rate(row.index.values,row.values,5.0)\n        pv0=price_bond_shifted(s5,row,times_entry)\n        par_checks.append({'ccy':c,'date':str(cdf.index[int(j)].date()),'s5':s5,'pv0':pv0,'abs_dev':abs(pv0-1)})\npar_df=pd.DataFrame(par_checks)\nprint(par_df.head(10))\nif not par_df.empty and (par_df['abs_dev']>0.02).any():\n    print('Par sanity diagnostics failures:')\n    print(par_df[par_df['abs_dev']>0.02].head(20))\n    raise RuntimeError('Par sanity check failed (abs(PV0-1)>0.02)')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Strategy simulation\nrows=[]\nport_rows=[]\n\nfor i in range(len(dates)-1):\n    t0, t1 = dates[i], dates[i+1]\n    s5_fund = float(gbp5_w.loc[t0])\n    ois_on = float(ois_w.loc[t0])\n    borrow_rate = ois_on + 0.005\n\n    active_rets=[]\n    active_count=0\n    spreads_this_week=[]\n    for c in EM:\n        c0 = curve_ccy[c].loc[t0].dropna()\n        c1 = curve_ccy[c].loc[t1].dropna()\n        if c0.empty or c1.empty:\n            continue\n        s5_lend = interp_rate(c0.index.values, c0.values, 5.0)\n        spread = s5_lend - s5_fund\n        trade = bool(np.isfinite(s5_lend) and np.isfinite(s5_fund) and spread >= 0.005)\n\n        f0 = fx_w.at[t0,c]; f1 = fx_w.at[t1,c]\n        g0 = fx_w.at[t0,'GBP']; g1 = fx_w.at[t1,'GBP']\n        if not np.isfinite([f0,f1,g0,g1]).all():\n            trade = False\n\n        ret=np.nan; pnl=0.0\n        if trade:\n            units_ccy = 10_000_000 / f0\n            pv1 = price_bond_shifted(s5_lend, c1, times_exit)\n            lend_end_usd = units_ccy * pv1 * f1\n\n            debt_units_gbp = 8_000_000 / g0\n            debt_end_usd = debt_units_gbp * (1 + borrow_rate/52) * g1\n\n            eq0 = 2_000_000\n            eq1 = lend_end_usd - debt_end_usd\n            pnl = eq1 - eq0\n            ret = pnl / eq0\n            # equity cannot lose more than 100% in a week\n            ret = max(ret, -0.999999)\n            active_rets.append(ret)\n            active_count += 1\n\n        rows.append({\n            'date':t0,'next_date':t1,'ccy':c,'active':int(trade),\n            's5_lend':s5_lend,'s5_fund':s5_fund,'spread':spread,\n            'ois_on':ois_on,'borrow_rate':borrow_rate,'ret':ret,'pnl_usd':pnl\n        })\n        spreads_this_week.append(spread)\n\n    port_ret = float(np.mean(active_rets)) if active_rets else 0.0\n    port_rows.append({'date':t0,'port_ret':port_ret,'active_positions':active_count,'spread_mean':float(np.mean(spreads_this_week)) if spreads_this_week else np.nan})\n\nres = pd.DataFrame(rows)\nport = pd.DataFrame(port_rows).set_index('date').sort_index()\nport['wealth'] = (1+port['port_ret']).cumprod()\nport['drawdown'] = port['wealth']/port['wealth'].cummax()-1\nassert float(port['drawdown'].min()) >= -1 - 1e-12\n\n# If suspiciously always active, print spread diagnostics\navg_active = float(port['active_positions'].mean())\nprint('avg_active_positions_per_week:', avg_active)\nif avg_active > 4.9:\n    print('SUSPICIOUS activity. spread diagnostics by currency:')\n    print(res.groupby('ccy')['spread'].describe())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Output tables with exact semantics\n\ndef sharpe_weekly(r):\n    s=r.std(ddof=1)\n    return (r.mean()/s) if s>0 else np.nan\n\ndef wealth_dd_from_returns(r):\n    w=(1+r).cumprod()\n    dd=w/w.cummax()-1\n    return w,dd\n\n# portfolio output\nportfolio_out = port.reset_index()[['date','port_ret','wealth','drawdown','active_positions']]\nportfolio_out.to_csv(OUTPUT_DIR/'portfolio_weekly_returns.csv', index=False)\n\n# currency stats\nstats=[]\nfor c in EM:\n    rc = res[res['ccy']==c].copy()\n    weeks_available = len(rc)\n    weeks_traded = int(rc['active'].sum())\n    active_frac = weeks_traded / weeks_available if weeks_available>0 else np.nan\n\n    r_cond = rc.loc[rc['active']==1,'ret'].dropna()\n    r_uncond = rc['ret'].fillna(0.0)\n\n    sw = sharpe_weekly(r_cond) if len(r_cond)>1 else np.nan\n    # per currency wealth with zero return when inactive\n    w,dd = wealth_dd_from_returns(r_uncond)\n\n    stats.append({\n        'ccy':c,\n        'weeks_available':weeks_available,\n        'weeks_traded':weeks_traded,\n        'active_frac':active_frac,\n        'mean_weekly_ret_cond_active':float(r_cond.mean()) if len(r_cond)>0 else np.nan,\n        'vol_weekly_ret_cond_active':float(r_cond.std(ddof=1)) if len(r_cond)>1 else np.nan,\n        'mean_weekly_ret_uncond':float(r_uncond.mean()),\n        'vol_weekly_ret_uncond':float(r_uncond.std(ddof=1)),\n        'sharpe_weekly_cond_active':sw,\n        'sharpe_ann_cond_active':(np.sqrt(52)*sw if np.isfinite(sw) else np.nan),\n        'pnl_sum_usd':float(rc['pnl_usd'].sum()),\n        'max_dd_wealth':float(dd.min())\n    })\nstats_df=pd.DataFrame(stats).sort_values('sharpe_ann_cond_active', ascending=False)\nstats_df.to_csv(OUTPUT_DIR/'currency_stats.csv', index=False)\n\n# active diagnostics + spread distribution\na_rows=[]\nfor c in EM:\n    rc=res[res['ccy']==c]\n    sp=rc['spread'].dropna()\n    a_rows.append({\n        'ccy':c,\n        'weeks_traded':int(rc['active'].sum()),\n        'active_frac':float(rc['active'].mean()),\n        'spread_mean':float(sp.mean()),\n        'spread_p5':float(sp.quantile(0.05)),\n        'spread_p50':float(sp.quantile(0.50)),\n        'spread_p95':float(sp.quantile(0.95))\n    })\na_rows.append({'ccy':'PORTFOLIO','weeks_traded':int(port['active_positions'].sum()),'active_frac':np.nan,\n               'spread_mean':np.nan,'spread_p5':np.nan,'spread_p50':avg_active,'spread_p95':np.nan})\nactive_diag=pd.DataFrame(a_rows)\nactive_diag.to_csv(OUTPUT_DIR/'active_diagnostics.csv', index=False)\n\n# currency correlations\nret_zero = res.pivot(index='date', columns='ccy', values='ret').sort_index().fillna(0.0)\nret_active = res.pivot(index='date', columns='ccy', values='ret').sort_index()\nret_zero.corr().to_csv(OUTPUT_DIR/'currency_corr.csv')\nret_active.corr(min_periods=10).to_csv(OUTPUT_DIR/'currency_corr_active_overlap.csv')\n\n# drop-one diagnostics\nfull_w = sharpe_weekly(port['port_ret'])\nfull_a = np.sqrt(52)*full_w if np.isfinite(full_w) else np.nan\nfull_dd = float(port['drawdown'].min())\n\ndrop_rows=[{'portfolio':'full','sharpe_weekly':full_w,'sharpe_ann':full_a,'max_dd_wealth':full_dd}]\nfor c in EM:\n    tmp = res[res['ccy']!=c].pivot(index='date', columns='ccy', values='ret').sort_index()\n    pr = tmp.mean(axis=1, skipna=True).fillna(0.0)\n    w,dd = wealth_dd_from_returns(pr)\n    sh = sharpe_weekly(pr)\n    drop_rows.append({'portfolio':f'ex_{c}','sharpe_weekly':sh,'sharpe_ann':(np.sqrt(52)*sh if np.isfinite(sh) else np.nan),'max_dd_wealth':float(dd.min())})\n\ndrop_df=pd.DataFrame(drop_rows)\ndrop_df.to_csv(OUTPUT_DIR/'drop_one_diagnostic.csv', index=False)\n\nprint('tables written')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Market factors (real if available, else proxies) + HAC(4) regressions\nfactor_hits=[]\nfor pat in ['*VIX*','*DXY*','*SPX*','*MSCI*','*UST*','*rates*','*factor*']:\n    factor_hits += list((BASE/'data_clean').glob(pat))\nfactor_hits = sorted(set(factor_hits), key=lambda p:p.name)\nprint('factor files found:', [f.name for f in factor_hits])\n\n# proxy factors\nem_fx_basket = np.log(fx_w[EM]).diff().mean(axis=1)\nusd_proxy = -np.log(fx_w['GBP']).diff()  # USD strength proxy via GBPUSD inverse\nrates_proxy_on = ois_w.diff()\nrates_proxy_5y = gbp5_w.diff()\n\nfactors = pd.DataFrame({\n    'usd_proxy': usd_proxy,\n    'em_fx_basket': em_fx_basket,\n    'rates_proxy_on': rates_proxy_on,\n    'rates_proxy_5y': rates_proxy_5y,\n}, index=port.index)\n\nmf = pd.concat([port['port_ret'], factors], axis=1).dropna()\nmcorr = mf.corr().loc[factors.columns, ['port_ret']].rename(columns={'port_ret':'corr_with_port'})\nmcorr.to_csv(OUTPUT_DIR/'market_factor_corr.csv')\n\n\ndef ols_hac(y, X, lags=4):\n    # X includes intercept column\n    y=np.asarray(y).reshape(-1,1)\n    X=np.asarray(X)\n    n,k=X.shape\n    beta=np.linalg.inv(X.T@X)@(X.T@y)\n    u=(y-X@beta)\n    # Newey-West\n    S=np.zeros((k,k))\n    for t in range(n):\n        xt=X[t:t+1].T\n        S += float(u[t,0]**2) * (xt@xt.T)\n    for L in range(1,lags+1):\n        w=1 - L/(lags+1)\n        for t in range(L,n):\n            xt=X[t:t+1].T; xlag=X[t-L:t-L+1].T\n            S += w*float(u[t,0]*u[t-L,0])*(xt@xlag.T + xlag@xt.T)\n    XXi=np.linalg.inv(X.T@X)\n    V=XXi@S@XXi\n    se=np.sqrt(np.diag(V)).reshape(-1,1)\n    tstat=(beta/se).flatten()\n    yhat=X@beta\n    e=y-yhat\n    r2=1 - float(((e.T@e)/((y-y.mean()).T@(y-y.mean()))).item())\n    return beta.flatten(), tstat, r2, n\n\nreg_rows=[]\ny=mf['port_ret'].values\nfor fac in factors.columns:\n    X=np.column_stack([np.ones(len(mf)), mf[fac].values])\n    b,t,r2,n = ols_hac(y,X,lags=4)\n    reg_rows.append({'model':'univariate','factor':fac,'alpha':b[0],'beta':b[1],'t_beta_hac4':t[1],'r2':r2,'n':n})\n\nmulti_cols=['usd_proxy','em_fx_basket','rates_proxy_5y']\nX=np.column_stack([np.ones(len(mf))] + [mf[c].values for c in multi_cols])\nb,t,r2,n = ols_hac(y,X,lags=4)\nreg_rows.append({'model':'multivariate','factor':'const','alpha':b[0],'beta':b[0],'t_beta_hac4':t[0],'r2':r2,'n':n})\nfor j,c in enumerate(multi_cols, start=1):\n    reg_rows.append({'model':'multivariate','factor':c,'alpha':b[0],'beta':b[j],'t_beta_hac4':t[j],'r2':r2,'n':n})\n\nreg_df=pd.DataFrame(reg_rows)\nreg_df.to_csv(OUTPUT_DIR/'market_factor_regs.csv', index=False)\n\n# factor figures\nfor fac in factors.columns:\n    tmp=mf[['port_ret',fac]].dropna()\n    plt.figure(figsize=(5,4))\n    plt.scatter(tmp[fac], tmp['port_ret'], s=10, alpha=0.6)\n    z=np.polyfit(tmp[fac], tmp['port_ret'], 1)\n    xs=np.linspace(tmp[fac].min(), tmp[fac].max(), 100)\n    plt.plot(xs, z[0]*xs+z[1], color='red')\n    plt.xlabel(fac); plt.ylabel('port_ret'); plt.tight_layout()\n    plt.savefig(FIG_DIR/f'factor_scatter_{fac}.png', dpi=120)\n    plt.close()\n\nprint(mcorr)\nprint(reg_df)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Figures + report + manifest + verification\n\n# figures\nplt.figure(figsize=(9,4)); plt.plot(port.index,port['wealth']); plt.title('Portfolio Wealth'); plt.tight_layout(); plt.savefig(FIG_DIR/'portfolio_wealth.png', dpi=130); plt.close()\nplt.figure(figsize=(9,4)); plt.plot(port.index,port['drawdown']); plt.title('Portfolio Drawdown'); plt.tight_layout(); plt.savefig(FIG_DIR/'portfolio_drawdown.png', dpi=130); plt.close()\nplt.figure(figsize=(9,4)); plt.plot(port.index,port['active_positions']); plt.title('Active Positions'); plt.tight_layout(); plt.savefig(FIG_DIR/'active_positions.png', dpi=130); plt.close()\n\ncorr=pd.read_csv(OUTPUT_DIR/'currency_corr.csv', index_col=0)\nplt.figure(figsize=(6,5))\nmat=corr.values\nplt.imshow(mat,cmap='RdBu_r',vmin=-1,vmax=1); plt.colorbar()\nplt.xticks(range(len(corr.columns)),corr.columns,rotation=45,ha='right')\nplt.yticks(range(len(corr.index)),corr.index)\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        if np.isfinite(mat[i,j]):\n            plt.text(j,i,f'{mat[i,j]:.2f}',ha='center',va='center',fontsize=8)\nplt.tight_layout(); plt.savefig(FIG_DIR/'corr_heatmap.png', dpi=130); plt.close()\n\n# report generation from outputs\nstats=pd.read_csv(OUTPUT_DIR/'currency_stats.csv')\ndrop=pd.read_csv(OUTPUT_DIR/'drop_one_diagnostic.csv')\nport_out=pd.read_csv(OUTPUT_DIR/'portfolio_weekly_returns.csv', parse_dates=['date'])\nmc=pd.read_csv(OUTPUT_DIR/'market_factor_corr.csv', index_col=0)\nmr=pd.read_csv(OUTPUT_DIR/'market_factor_regs.csv')\nmiss=pd.read_csv(OUTPUT_DIR/'alignment_missingness.csv')\nactive=pd.read_csv(OUTPUT_DIR/'active_diagnostics.csv')\n\n\ndef md_table(df, nd=6):\n    cols=list(df.columns)\n    lines=['| '+' | '.join(cols)+' |','|'+'|'.join(['---']*len(cols))+'|']\n    for _,r in df.iterrows():\n        vals=[]\n        for c in cols:\n            v=r[c]\n            if isinstance(v,float): vals.append(f'{v:.{nd}f}')\n            else: vals.append(str(v))\n        lines.append('| '+' | '.join(vals)+' |')\n    return '\\n'.join(lines)\n\ntext=[]\ntext.append('# HW6 FX Carry Report')\ntext.append('## Spec Recap')\ntext.append('- Weekly USD 10MM lending, GBP 8MM borrowing at OIS+50bp, entry filter uses GBP 5Y funding rate: s5_lend >= s5_fund + 50bp.')\ntext.append('- Lending MTM shifts all coupon/principal times by 1/52 and reprices on exit curve (dirty pricing via shifted cashflows).')\ntext.append('## Data & Coverage')\ntext.append('- EM curves sourced from ../../Data/ with fallback ./data (manifest has exact files).')\ntext.append('- OIS/FX/funding curve raw from ./data_clean/.')\ntext.append(f\"- Weekly window: {port_out['date'].min().date()} to {port_out['date'].max().date()}, N={len(port_out)}.\")\ntext.append('## Funding 5Y Construction')\ntext.append('- GBP 5Y funding series constructed from local BoE raw OIS spot-curve file (not overnight proxy).')\ntext.append('- Overnight OIS is used only for borrowing accrual +50bp.')\ntext.append('## Methodology')\ntext.append('- Par-curve bootstrap and discounting follow Zero/Spot curve conventions (linear interpolation + recursive DF solve).')\ntext.append('- Cashflow schedule shift validation: times_exit_full = times_entry - 1/52 with assertion in notebook.')\ntext.append('## Results')\ntext.append(md_table(stats))\ntext.append(md_table(port_out[['port_ret','wealth','drawdown','active_positions']].describe().T))\ntext.append('![wealth](outputs/figures/portfolio_wealth.png)')\ntext.append('![drawdown](outputs/figures/portfolio_drawdown.png)')\ntext.append('![active](outputs/figures/active_positions.png)')\ntext.append('![corr](outputs/figures/corr_heatmap.png)')\ntext.append('## Drop-One Diagnostics')\ntext.append(md_table(drop))\ntext.append('## Market Factors')\ntext.append(md_table(mc.reset_index()))\ntext.append(md_table(mr))\ntext.append('Carry framing: factor signs are consistent with carry-vs-crash intuition where USD/risk shocks can dominate carry accrual.')\ntext.append('## Robustness & Guardrails')\ntext.append(md_table(miss))\ntext.append(md_table(active))\n\nreport_path = BASE/'hw6_fx_carry_report.md'\nreport_path.write_text('\\n\\n'.join(text))\n\n# finalize manifest\nmanifest['coverage']['start'] = str(port_out['date'].min().date())\nmanifest['coverage']['end'] = str(port_out['date'].max().date())\nmanifest['coverage']['weekly_obs'] = int(len(port_out))\nmanifest['coverage']['currencies'] = EM\n(OUTPUT_DIR/'run_manifest.json').write_text(json.dumps(manifest, indent=2))\n\n# verification\nrequired_csv = {\n 'currency_stats.csv': ['ccy','weeks_available','weeks_traded','active_frac','sharpe_weekly_cond_active','sharpe_ann_cond_active'],\n 'currency_corr.csv': [],\n 'drop_one_diagnostic.csv': ['portfolio','sharpe_weekly','sharpe_ann','max_dd_wealth'],\n 'portfolio_weekly_returns.csv': ['date','port_ret','wealth','drawdown','active_positions'],\n 'market_factor_corr.csv': ['corr_with_port'],\n 'market_factor_regs.csv': ['model','factor','beta','t_beta_hac4','r2','n'],\n 'active_diagnostics.csv': ['ccy','weeks_traded','active_frac']\n}\nfor fn, cols in required_csv.items():\n    fp=OUTPUT_DIR/fn\n    if (not fp.exists()) or fp.stat().st_size==0:\n        raise RuntimeError(f'Missing/empty output: {fn}')\n    df=pd.read_csv(fp)\n    for c in cols:\n        if c not in df.columns:\n            raise RuntimeError(f'Column {c} missing in {fn}')\n\nfor fig in ['portfolio_wealth.png','portfolio_drawdown.png','corr_heatmap.png','active_positions.png']:\n    fp=FIG_DIR/fig\n    if (not fp.exists()) or fp.stat().st_size==0:\n        raise RuntimeError(f'Missing/empty figure: {fig}')\n\nif (not report_path.exists()) or report_path.stat().st_size==0:\n    raise RuntimeError('Report missing/empty')\nrt=report_path.read_text()\nfor sec in ['Market Factors','Funding 5Y Construction']:\n    if sec not in rt:\n        raise RuntimeError(f'Report section missing: {sec}')\n\nad = pd.read_csv(OUTPUT_DIR/'active_diagnostics.csv')\navg_diag = float(ad.loc[ad['ccy']=='PORTFOLIO','spread_p50'].iloc[0])\navg_port = float(pd.read_csv(OUTPUT_DIR/'portfolio_weekly_returns.csv')['active_positions'].mean())\nif abs(avg_diag-avg_port) > 1e-6:\n    raise RuntimeError('Active diagnostics mismatch with portfolio active_positions mean')\n\nprint('Verification PASSED')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}