{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# HW6 \u2014 FX Carry Strategy (GBP funding)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "from pathlib import Path\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 220)\nBASE = Path('.').resolve()\nOUT = BASE/'outputs'\nFIG = OUT/'figures'\nOUT.mkdir(parents=True, exist_ok=True)\nFIG.mkdir(parents=True, exist_ok=True)\n\nprint('Working directory:', BASE)\nprint('Directory inventory:')\nfor d in [BASE/'data_clean', BASE/'data', BASE/'../../Data']:\n    files = sorted(d.glob('*')) if d.exists() else []\n    print(' -', d, 'files=', len(files))\n    for f in files[:10]:\n        print('    ', f.name)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def pick_file(paths, patterns):\n    hits=[]\n    for pp in paths:\n        if not pp.exists():\n            continue\n        for pat in patterns:\n            hits += list(pp.glob(pat))\n    hits = sorted(set(hits), key=lambda x:(x.stat().st_mtime, x.name), reverse=True)\n    if not hits:\n        return None\n    print('Chosen file:', hits[0])\n    return hits[0]\n\ndef load_sonia():\n    fp=pick_file([BASE/'data_clean'], ['*IUDSOIA*.parquet','*IUDSOIA*.csv'])\n    if fp is None:\n        raise FileNotFoundError('No *IUDSOIA* file found in data_clean')\n    if fp.suffix=='.parquet':\n        df=pd.read_parquet(fp)\n    else:\n        df=pd.read_csv(fp)\n    if isinstance(df.index, pd.DatetimeIndex):\n        idx=pd.to_datetime(df.index, errors='coerce').tz_localize(None)\n        s=pd.Series(pd.to_numeric(df.iloc[:,0], errors='coerce').values, index=idx, name='sonia')\n    else:\n        date_col=next((c for c in df.columns if str(c).upper()=='DATE'), df.columns[0])\n        val_col=next((c for c in df.columns if 'IUDSOIA' in str(c).upper()), [c for c in df.columns if c!=date_col][0])\n        s=pd.Series(pd.to_numeric(df[val_col], errors='coerce').values, index=pd.to_datetime(df[date_col], errors='coerce'), name='sonia')\n    s=s.dropna().sort_index()\n    if s.median()>1:\n        s=s/100.0\n    return s\n\ndef load_gbp5y(sonia_idx):\n    fp=pick_file([BASE/'data_clean'], ['*boe*ois*daily*raw*.parquet','*ois*daily*raw*.parquet'])\n    if fp is None:\n        inv=sorted((BASE/'data_clean').glob('*'))\n        raise RuntimeError('Missing boe raw OIS archive. inventory=' + ','.join(x.name for x in inv))\n    df=pd.read_parquet(fp)\n    if not {'__source_file','__sheet','UK OIS spot curve'}.issubset(df.columns):\n        raise RuntimeError('boe raw missing required columns')\n    sub=df[df['__sheet'].astype(str).str.contains('spot curve', case=False, na=False)].copy()\n    pieces=[]\n    for src,g in sub.groupby('__source_file'):\n        vals=pd.to_numeric(g['UK OIS spot curve'], errors='coerce').dropna().reset_index(drop=True)\n        vals=vals[(vals>-5)&(vals<25)]\n        if len(vals)>2 and abs(vals.iloc[0]-1.0)<1e-8 and abs(vals.iloc[1]-1/12)<1e-8:\n            vals=vals.iloc[2:].reset_index(drop=True)\n        if len(vals)<100:\n            continue\n        years=[int(x) for x in re.findall(r'(20\\d{2})', str(src))]\n        if years:\n            y0,y1=min(years),max(years)\n            idx_pool=pd.DatetimeIndex([d for d in sonia_idx if y0<=d.year<=y1])\n        else:\n            idx_pool=sonia_idx\n        if len(idx_pool)<len(vals):\n            idx_pool=sonia_idx[-len(vals):]\n        else:\n            idx_pool=idx_pool[:len(vals)]\n        pieces.append(pd.Series(vals.values, index=idx_pool))\n    if not pieces:\n        inv=sorted((BASE/'data_clean').glob('*'))\n        msg='Could not build GBP 5Y from boe raw. inventory=' + ','.join(x.name for x in inv)\n        raise RuntimeError(msg)\n    s=pd.concat(pieces).sort_index()\n    s=s[~s.index.duplicated(keep='last')].dropna()\n    if s.median()>1:\n        s=s/100.0\n    s.name='gbp5y'\n    return s\n\ndef load_fx(em):\n    fp=pick_file([BASE/'data_clean'], ['*usd_per_ccy*wide*.parquet','*edi*cur*fx_long*.parquet'])\n    if fp is None:\n        raise FileNotFoundError('No FX file found in data_clean')\n    if 'fx_long' in fp.name:\n        d=pd.read_parquet(fp)\n        fx=d.pivot(index='date', columns='ccy', values='value')\n    else:\n        fx=pd.read_parquet(fp)\n    if not isinstance(fx.index,pd.DatetimeIndex):\n        fx.index=pd.to_datetime(fx.index, errors='coerce')\n    fx.index=fx.index.tz_localize(None)\n    fx.columns=[str(c).upper().strip() for c in fx.columns]\n    fx=fx.apply(pd.to_numeric, errors='coerce').sort_index()\n    need=sorted(set(em+['GBP']))\n    for c in need:\n        if c not in fx.columns:\n            fx[c]=np.nan\n    fx=fx[need]\n    med=fx['GBP'].dropna().median()\n    if np.isfinite(med) and med<0.3:\n        fx=1.0/fx\n        print('FX inverted to USD per CCY')\n    assert fx['GBP'].dropna().between(0.3,5.0).mean()>0.95, 'GBP quote sanity failed'\n    jumps=(fx.pct_change().abs()>0.20).sum()\n    print('Daily FX jumps >20% counts:')\n    print(jumps)\n    return fx\n\ndef parse_curve(fp):\n    raw=pd.read_csv(fp)\n    out=[]\n    cols=list(raw.columns)\n    for i in range(0,len(cols),2):\n        c0=cols[i]\n        c1=cols[i+1] if i+1<len(cols) else None\n        if c1 is None:\n            continue\n        m=re.search(r'GT([A-Z]{3})(\\d+)(Y|YR)', str(c0).upper())\n        if not m:\n            continue\n        tmp=pd.DataFrame({\n            'date':pd.to_datetime(raw[c0], errors='coerce'),\n            'ccy':m.group(1),\n            'tenor':float(m.group(2)),\n            'rate':pd.to_numeric(raw[c1], errors='coerce')\n        }).dropna(subset=['date','rate'])\n        out.append(tmp)\n    if not out:\n        raise RuntimeError('No parsable tenors in '+str(fp))\n    return pd.concat(out, ignore_index=True)\n\ndef load_curves():\n    dirs=[BASE/'../../Data', BASE/'data', BASE.parent/'data']\n    files=[]\n    for d in dirs:\n        if d.exists():\n            files += list(d.glob('*Emerging*Mkt*YC*.csv'))\n    files=sorted(set(files), key=lambda x:(x.stat().st_mtime,x.name), reverse=True)\n    if not files:\n        raise FileNotFoundError('No EM YC csv files found in ../../Data or ./data')\n    print('Curve files:', [f.name for f in files])\n    df=pd.concat([parse_curve(f) for f in files], ignore_index=True)\n    if df['rate'].median()>1:\n        df['rate']=df['rate']/100.0\n    df['date']=pd.to_datetime(df['date']).dt.tz_localize(None)\n    df=df.groupby(['date','ccy','tenor'], as_index=False)['rate'].mean()\n    return df\n\nEM=['BRL','NGN','PKR','TRY','ZAR']\nsonia=load_sonia()\ngbp5=load_gbp5y(sonia.index)\nfx=load_fx(EM)\ncurves=load_curves()\nprint('Coverage sonia', sonia.index.min().date(), sonia.index.max().date(), len(sonia))\nprint('Coverage gbp5 ', gbp5.index.min().date(), gbp5.index.max().date(), len(gbp5))\nprint('Coverage fx   ', fx.index.min().date(), fx.index.max().date(), len(fx))\nprint('Coverage curve', curves['date'].min().date(), curves['date'].max().date(), len(curves))"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def align_weekly(obj, start, end, freq='W-WED'):\n    t=pd.date_range(start, end, freq=freq)\n    out=[]; idx=[]\n    for d in t:\n        cand=[d,d+pd.Timedelta(days=1),d+pd.Timedelta(days=2),d-pd.Timedelta(days=1),d-pd.Timedelta(days=2)]\n        c=next((x for x in cand if x in obj.index), None)\n        if c is None:\n            continue\n        out.append(obj.loc[c]); idx.append(d)\n    if isinstance(obj,pd.Series):\n        return pd.Series(out, index=pd.DatetimeIndex(idx), name=obj.name)\n    return pd.DataFrame(out, index=pd.DatetimeIndex(idx), columns=obj.columns)\n\ncurve_wide=curves.pivot_table(index=['date','ccy'], columns='tenor', values='rate', aggfunc='mean').sort_index()\nstart=max(sonia.index.min(), gbp5.index.min(), fx.index.min(), curves['date'].min())\nend=min(sonia.index.max(), gbp5.index.max(), fx.index.max(), curves['date'].max())\n\nsonia_w=align_weekly(sonia,start,end)\ngbp5_w=align_weekly(gbp5,start,end)\nfx_w=align_weekly(fx,start,end)\ndates=sonia_w.index.intersection(gbp5_w.index).intersection(fx_w.index)\n\ncurve_ccy={}\nfor c in EM:\n    if c not in curve_wide.index.get_level_values('ccy'):\n        continue\n    curve_ccy[c]=align_weekly(curve_wide.xs(c,level='ccy'),start,end).reindex(dates).ffill()\n\nsonia_w=sonia_w.reindex(dates).ffill()\ngbp5_w=gbp5_w.reindex(dates).ffill()\nfx_w=fx_w.reindex(dates).ffill()\n\nprint('Weekly dates', dates.min().date(), dates.max().date(), 'n=', len(dates))\nprint('Missing weekly sonia', int(sonia_w.isna().sum()), 'gbp5', int(gbp5_w.isna().sum()))\nprint('Missing weekly fx by ccy')\nprint(fx_w.isna().sum())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def interp_rate(tenors, rates, t):\n    x=np.array(tenors,dtype=float); y=np.array(rates,dtype=float)\n    m=np.isfinite(x)&np.isfinite(y)\n    x=x[m]; y=y[m]\n    if len(x)==0:\n        return np.nan\n    o=np.argsort(x); x=x[o]; y=y[o]\n    return float(np.interp(t,x,y,left=y[0],right=y[-1]))\n\ndef bootstrap_df(par_curve,freq=4,max_t=5.0):\n    grid=np.arange(1/freq,max_t+1e-12,1/freq)\n    d={}\n    for t in grid:\n        s=interp_rate(par_curve.index.values, par_curve.values, t)\n        c=s/freq\n        pv_prev=sum(c*d[pt] for pt in grid if pt<t)\n        d[t]=max((1-pv_prev)/(1+c),1e-10)\n    return pd.Series(d)\n\ndef price_bond(coupon_rate, par_curve, pay_times, freq=4):\n    if len(pay_times)==0:\n        return 1.0\n    z=bootstrap_df(par_curve,freq=freq,max_t=max(5.0,float(np.max(pay_times))+0.25))\n    dfs=np.interp(pay_times,z.index.values,z.values,left=z.values[0],right=z.values[-1])\n    cf=np.full(len(pay_times), coupon_rate/freq)\n    cf[-1]+=1.0\n    return float(np.sum(cf*dfs))\n\nentry_t=np.arange(0.25,5.0+1e-12,0.25)\nexit_t=entry_t-1/52\nexit_t=exit_t[exit_t>0]\nprint('entry head', entry_t[:5], 'tail', entry_t[-5:])\nprint('exit head', exit_t[:5], 'tail', exit_t[-5:])\nprint('shift check', bool(np.allclose(entry_t[:len(exit_t)]-exit_t, 1/52, atol=1e-10)))\n\n# par checks\nchecks=[]\nrng=np.random.default_rng(0)\nfor c in EM:\n    if c not in curve_ccy:\n        continue\n    cdf=curve_ccy[c].dropna(how='all')\n    if cdf.empty:\n        continue\n    for j in rng.choice(len(cdf), size=min(4,len(cdf)), replace=False):\n        row=cdf.iloc[int(j)].dropna()\n        if row.empty:\n            continue\n        s5=interp_rate(row.index.values,row.values,5.0)\n        p=price_bond(s5,row,entry_t)\n        checks.append((c,cdf.index[int(j)],s5,p))\nchk=pd.DataFrame(checks,columns=['ccy','date','s5','par_price'])\nprint(chk.head(10))\nprint('max abs par error', float((chk['par_price']-1).abs().max()) if len(chk) else np.nan)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "rows=[]\nport=[]\nfor i in range(len(dates)-1):\n    t0,t1=dates[i],dates[i+1]\n    s5fund=float(gbp5_w.loc[t0])\n    ois=float(sonia_w.loc[t0])\n    borrow=ois+0.005\n    ccy_rets=[]\n    active_count=0\n    for c in EM:\n        if c not in curve_ccy:\n            continue\n        c0=curve_ccy[c].loc[t0].dropna(); c1=curve_ccy[c].loc[t1].dropna()\n        if c0.empty or c1.empty:\n            continue\n        s5lend=interp_rate(c0.index.values,c0.values,5.0)\n        active=bool(np.isfinite(s5lend) and np.isfinite(s5fund) and (s5lend>=s5fund+0.005))\n\n        fx0=fx_w.at[t0,c]; fx1=fx_w.at[t1,c]\n        gbp0=fx_w.at[t0,'GBP']; gbp1=fx_w.at[t1,'GBP']\n        if not np.isfinite([fx0,fx1,gbp0,gbp1]).all():\n            active=False\n        ret=np.nan; pnl=0.0\n        if active:\n            units=10_000_000/fx0\n            p1=price_bond(s5lend,c1,exit_t)\n            lend_end=units*p1*fx1\n\n            debt_units=8_000_000/gbp0\n            debt_end=debt_units*(1+borrow/52)*gbp1\n\n            eq0=2_000_000\n            eq1=lend_end-debt_end\n            pnl=eq1-eq0\n            ret=pnl/eq0\n            active_count+=1\n            ccy_rets.append(ret)\n\n        rows.append({'date':t0,'next_date':t1,'ccy':c,'active':int(active),'s5_lend':s5lend,'s5_fund':s5fund,'hurdle':s5fund+0.005,'ois':ois,'ret':ret,'pnl_usd':pnl})\n    pr=float(np.mean(ccy_rets)) if ccy_rets else 0.0\n    port.append({'date':t0,'port_ret':pr,'active_positions':active_count})\n\nres=pd.DataFrame(rows)\nport=pd.DataFrame(port).set_index('date').sort_index()\nport['wealth']=(1+port['port_ret']).cumprod()\nport['dd']=port['wealth']/port['wealth'].cummax()-1\nassert (port['dd']>=-1-1e-10).all()\n\nactive_diag=res.groupby('ccy',as_index=False)['active'].mean().rename(columns={'active':'active_frac'})\nactive_diag.loc[len(active_diag)]={'ccy':'ALL_AVG_ACTIVE_POS','active_frac':port['active_positions'].mean()}\nactive_diag.to_csv(OUT/'active_diagnostics.csv', index=False)\nprint(active_diag)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def ann_sharpe(r):\n    s=r.std(ddof=1)\n    return np.sqrt(52)*r.mean()/s if s>0 else np.nan\n\ndef maxdd_from_ret(r):\n    w=(1+r.fillna(0)).cumprod()\n    return float((w/w.cummax()-1).min())\n\npivot=res.pivot(index='date',columns='ccy',values='ret').sort_index()\nstats=[]\nfor c in pivot.columns:\n    r=pivot[c].dropna()\n    if len(r)==0:\n        continue\n    stats.append({'ccy':c,'mean_weekly':r.mean(),'vol_weekly':r.std(ddof=1),'ann_sharpe':ann_sharpe(r),'active_frac':float(res.loc[res.ccy==c,'active'].mean()),'pnl_sum_usd':float(res.loc[res.ccy==c,'pnl_usd'].sum()),'max_dd_wealth':maxdd_from_ret(r),'active_weeks':int(r.notna().sum())})\nstats_df=pd.DataFrame(stats).sort_values('ann_sharpe',ascending=False)\ncorr=pivot.corr(min_periods=25)\n\nbase=port['port_ret']\nbase_sh=ann_sharpe(base)\nbase_dd=float(port['dd'].min())\nrows=[]\nfor c in pivot.columns:\n    ew=pivot.drop(columns=[c]).mean(axis=1,skipna=True).fillna(0)\n    w=(1+ew).cumprod(); dd=float((w/w.cummax()-1).min())\n    rows.append({'ccy_removed':c,'ann_sharpe_drop_one':ann_sharpe(ew),'max_dd_drop_one':dd,'delta_sharpe':ann_sharpe(ew)-base_sh,'delta_dd':dd-base_dd})\ndrop_df=pd.DataFrame(rows).sort_values('delta_sharpe')\n\nstats_df.to_csv(OUT/'currency_stats.csv',index=False)\ncorr.to_csv(OUT/'currency_corr.csv')\ndrop_df.to_csv(OUT/'drop_one_diagnostic.csv',index=False)\nport.reset_index().to_csv(OUT/'portfolio_weekly_returns.csv',index=False)\n\nplt.figure(figsize=(9,4)); plt.plot(port.index,port['wealth']); plt.title('Portfolio wealth'); plt.tight_layout(); plt.savefig(FIG/'portfolio_wealth.png',dpi=130); plt.close()\nplt.figure(figsize=(9,4)); plt.plot(port.index,port['dd']); plt.title('Portfolio drawdown'); plt.tight_layout(); plt.savefig(FIG/'portfolio_drawdown.png',dpi=130); plt.close()\nplt.figure(figsize=(6,5)); mat=corr.values; plt.imshow(mat,cmap='RdBu_r',vmin=-1,vmax=1); plt.colorbar(); plt.xticks(range(len(corr.columns)),corr.columns,rotation=45,ha='right'); plt.yticks(range(len(corr.index)),corr.index)\nfor i in range(mat.shape[0]):\n    for j in range(mat.shape[1]):\n        if np.isfinite(mat[i,j]):\n            plt.text(j,i,f'{mat[i,j]:.2f}',ha='center',va='center',fontsize=8)\nplt.tight_layout(); plt.savefig(FIG/'corr_heatmap.png',dpi=130); plt.close()\nprint('core outputs done')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# Market factor analysis (proxy factors if no external factors)\nfactor_hits=[]\nfor pat in ['*VIX*','*DXY*','*SPX*','*MSCI*','*UST*','*rates*','*factors*']:\n    factor_hits += list((BASE/'data_clean').glob(pat))\nprint('factor files found', [f.name for f in sorted(set(factor_hits))])\n\nemfx=np.log(fx_w[EM]).diff().mean(axis=1)\nusd_strength=-np.log(fx_w['GBP']).diff()\nproxy_usd=0.5*usd_strength+0.5*emfx\nproxy_d_gbp5=gbp5_w.diff()\nproxy_d_sonia=sonia_w.diff()\n\nfactors=pd.DataFrame({'proxy_usd_broad':proxy_usd,'proxy_emfx_basket_ret':emfx,'proxy_rates_d_gbp5y':proxy_d_gbp5,'proxy_rates_d_sonia':proxy_d_sonia}, index=port.index)\nmf=pd.concat([port['port_ret'],factors],axis=1).dropna()\n\ncorr_f=mf.corr().loc[factors.columns,['port_ret']].rename(columns={'port_ret':'corr_with_port'})\ncorr_f.to_csv(OUT/'market_factor_corr.csv')\n\nregs=[]\ny=mf['port_ret'].values\nfor fac in factors.columns:\n    x=mf[[fac]].values[:,0]\n    X=np.column_stack([np.ones(len(x)),x])\n    b=np.linalg.lstsq(X,y,rcond=None)[0]\n    yh=X@b\n    e=y-yh\n    n=len(y); k=2\n    s2=(e@e)/(n-k)\n    cov=s2*np.linalg.inv(X.T@X)\n    se=np.sqrt(np.diag(cov))\n    t=b[1]/se[1] if se[1]>0 else np.nan\n    r2=1-(e@e)/np.sum((y-y.mean())**2)\n    regs.append({'model':'univariate','factor':fac,'alpha':b[0],'beta':b[1],'t_beta':t,'r2':r2,'n':n})\n\nX=np.column_stack([np.ones(len(mf))]+[mf[c].values for c in factors.columns])\nb=np.linalg.lstsq(X,y,rcond=None)[0]\nyh=X@b\ne=y-yh\nn=len(y); k=X.shape[1]\ns2=(e@e)/(n-k)\ncov=s2*np.linalg.inv(X.T@X)\nse=np.sqrt(np.diag(cov))\nr2=1-(e@e)/np.sum((y-y.mean())**2)\nlabels=['const']+list(factors.columns)\nfor j,lab in enumerate(labels):\n    regs.append({'model':'multivariate','factor':lab,'alpha':b[0],'beta':b[j],'t_beta':(b[j]/se[j] if se[j]>0 else np.nan),'r2':r2,'n':n})\nreg_df=pd.DataFrame(regs)\nreg_df.to_csv(OUT/'market_factor_regs.csv',index=False)\n\nfor fac in factors.columns:\n    tmp=mf[['port_ret',fac]].dropna()\n    plt.figure(figsize=(5,4)); plt.scatter(tmp[fac],tmp['port_ret'],s=10,alpha=0.6)\n    z=np.polyfit(tmp[fac],tmp['port_ret'],1)\n    xs=np.linspace(tmp[fac].min(),tmp[fac].max(),100)\n    plt.plot(xs,z[0]*xs+z[1],color='red'); plt.xlabel(fac); plt.ylabel('port_ret'); plt.tight_layout(); plt.savefig(FIG/f'factor_scatter_{fac}.png',dpi=120); plt.close()\n\nprint(corr_f)\nprint(reg_df.head(12))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}