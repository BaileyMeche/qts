{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "843b91dd",
   "metadata": {},
   "source": [
    "# HW6 — FX Carry Strategy (GBP funding)  \n",
    "**Objective:** Implement a weekly FX carry backtest using:\n",
    "- **Funding leg:** UK OIS (IUDSOIA) + 50bp, applied to **4/5 of USD notional** (5x leverage proxy).  \n",
    "- **Lending leg:** 5Y par bond in each EM currency with **quarterly coupons** at the **5Y swap rate**.  \n",
    "- **Mark-to-market:** reprice the bond each week using the **new swap curve**, with time-to-cashflows reduced by **1/52 years**.  \n",
    "- **Entry rule:** trade only if `s5_lend - s5_fund >= 50bp`.  \n",
    "- **Accounting:** Convert all cashflows to **USD** (home currency).  \n",
    "\n",
    "**Data locations (relative to this notebook):**\n",
    "- EM swap curves + EM FX: `../../Data/`  \n",
    "- UK OIS (IUDSOIA) + FX tables (Nasdaq EDI/CUR extracts): `./data_clean/`\n",
    "\n",
    "**Primary reference notebook:** `Zero_And_Spot_Curves.ipynb` (bootstrapping/pricing patterns).  \n",
    "**Concept reference:** `Carry_Concept__FTSE.pdf` (carry + rolldown intuition and diagnostics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf57a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- imports ---\n",
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79631f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- paths / constants ---\n",
    "NB_DIR = Path.cwd()  # assume notebook is run from its own directory\n",
    "DATA_EM_DIR = (NB_DIR / \"../../Data\").resolve()\n",
    "DATA_CLEAN_DIR = (NB_DIR / \"data_clean\").resolve()\n",
    "\n",
    "USD_NOTIONAL = 10_000_000.0\n",
    "BORROW_FRAC = 0.80               # 4/5 notional borrowed\n",
    "BORROW_SPREAD = 0.0050           # 50bp\n",
    "WEEK_DT = 1.0 / 52.0\n",
    "COUPON_FREQ = 4                  # quarterly\n",
    "FUNDING_CCY = \"GBP\"              # per assignment name \"FXCarryBasedOnGBP\"\n",
    "\n",
    "EM_CCYS = [\"TRY\", \"ZAR\", \"PKR\", \"BRL\", \"NGN\"]\n",
    "\n",
    "print(\"NB_DIR        :\", NB_DIR)\n",
    "print(\"DATA_EM_DIR   :\", DATA_EM_DIR, \"exists:\", DATA_EM_DIR.exists())\n",
    "print(\"DATA_CLEAN_DIR:\", DATA_CLEAN_DIR, \"exists:\", DATA_CLEAN_DIR.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3840e3c",
   "metadata": {},
   "source": [
    "## 1) Data exploration (required)\n",
    "\n",
    "We begin by inventorying inputs and inspecting their schema.  \n",
    "The code below is written to be **path-robust**: it searches for plausible filenames and prints what it finds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e300a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ls_tree(p: Path, max_items: int = 50) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        return pd.DataFrame({\"path\":[str(p)], \"exists\":[False]})\n",
    "    items = sorted([x for x in p.rglob(\"*\") if x.is_file()])\n",
    "    items = items[:max_items]\n",
    "    return pd.DataFrame({\n",
    "        \"relpath\":[str(x.relative_to(p)) for x in items],\n",
    "        \"suffix\":[x.suffix.lower() for x in items],\n",
    "        \"size_kb\":[round(x.stat().st_size/1024,2) for x in items],\n",
    "    })\n",
    "\n",
    "display(ls_tree(DATA_CLEAN_DIR, 200))\n",
    "display(ls_tree(DATA_EM_DIR, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddcf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_one(root: Path, patterns: Iterable[str]) -> Path:\n",
    "    pats = list(patterns)\n",
    "    hits = []\n",
    "    for pat in pats:\n",
    "        hits += list(root.rglob(pat))\n",
    "    hits = [h for h in hits if h.is_file()]\n",
    "    if not hits:\n",
    "        raise FileNotFoundError(f\"No files under {root} matching {pats}\")\n",
    "    hits = sorted(hits, key=lambda x: (len(str(x)), str(x)))\n",
    "    return hits[0]\n",
    "\n",
    "def read_table(path: Path) -> pd.DataFrame:\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    if suf == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if suf in {\".xlsx\", \".xls\"}:\n",
    "        return pd.read_excel(path)\n",
    "    raise ValueError(f\"Unsupported file type: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9f657",
   "metadata": {},
   "source": [
    "## 2) Load UK OIS (IUDSOIA)\n",
    "\n",
    "We need **short-tenor UK OIS** (daily), used as the weekly funding rate input:\n",
    "`r_borrow = OIS + 50bp`.  \n",
    "\n",
    "Expected columns vary; we standardize to:\n",
    "- `date` (datetime64[ns], tz-naive)\n",
    "- `ois` (decimal, e.g. 0.0425 = 4.25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6fe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Locate IUDSOIA\n",
    "iudsoia_path = find_one(DATA_CLEAN_DIR, patterns=[\"*IUDSOIA*.csv\", \"*IUDSOIA*.parquet\", \"*iudsoia*.csv\", \"*iudsoia*.parquet\"])\n",
    "print(\"IUDSOIA file:\", iudsoia_path)\n",
    "\n",
    "ois_raw = read_table(iudsoia_path)\n",
    "display(ois_raw.head())\n",
    "print(ois_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1b8559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_ois(df: pd.DataFrame) -> pd.Series:\n",
    "    d = df.copy()\n",
    "    date_col = next((c for c in d.columns if str(c).lower() in {\"date\",\"dt\",\"time\",\"observation_date\"}), None)\n",
    "    if date_col is None:\n",
    "        date_col = d.columns[0]\n",
    "    d[date_col] = pd.to_datetime(d[date_col])\n",
    "    d = d.sort_values(date_col)\n",
    "\n",
    "    rate_col = next((c for c in d.columns if str(c).lower() in {\"ois\",\"iudsoia\",\"value\",\"rate\"}), None)\n",
    "    if rate_col is None:\n",
    "        num_cols = [c for c in d.columns if c != date_col and pd.api.types.is_numeric_dtype(d[c])]\n",
    "        if not num_cols:\n",
    "            raise ValueError(\"Cannot infer OIS rate column.\")\n",
    "        rate_col = num_cols[0]\n",
    "\n",
    "    s = d.set_index(date_col)[rate_col].astype(float)\n",
    "    if s.dropna().median() > 1.0:\n",
    "        s = s / 100.0\n",
    "    s.name = \"ois\"\n",
    "    return s\n",
    "\n",
    "ois = standardize_ois(ois_raw)\n",
    "ois = ois[~ois.index.duplicated(keep=\"last\")].sort_index()\n",
    "display(ois.head())\n",
    "display(ois.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adebc6f",
   "metadata": {},
   "source": [
    "## 3) Load FX rates (Nasdaq EDI/CUR extracts)\n",
    "\n",
    "We need:\n",
    "- USD/GBP (funding conversion)\n",
    "- USD per EM currency (preferred), or the inverse (we will standardize)\n",
    "\n",
    "Exports differ; we support:\n",
    "- wide table: `date, GBP, TRY, ...`\n",
    "- long table: `date, code, value`\n",
    "\n",
    "We standardize to a wide DataFrame: `fx_usd_per[CCY] = USD per 1 unit of CCY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14e5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fx_path = find_one(DATA_CLEAN_DIR, patterns=[\"*EDI*CUR*.csv\", \"*edi*cur*.csv\", \"*fx*.csv\", \"*FX*.csv\", \"*currency*.csv\", \"*CUR*.csv\"])\n",
    "print(\"FX file:\", fx_path)\n",
    "\n",
    "fx_raw = read_table(fx_path)\n",
    "display(fx_raw.head())\n",
    "print(fx_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adcd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_fx_to_usd_per_ccy(df: pd.DataFrame, ccys: Iterable[str]) -> pd.DataFrame:\n",
    "    ccys = [c.upper() for c in ccys]\n",
    "    d = df.copy()\n",
    "\n",
    "    date_col = next((c for c in d.columns if str(c).lower() in {\"date\",\"dt\",\"time\",\"timestamp\"}), None)\n",
    "    if date_col is None:\n",
    "        date_col = d.columns[0]\n",
    "    d[date_col] = pd.to_datetime(d[date_col])\n",
    "\n",
    "    wide_cols = set(map(str.upper, d.columns))\n",
    "    if all(c in wide_cols for c in ccys):\n",
    "        out = d[[date_col] + ccys].copy()\n",
    "        out = out.rename(columns={date_col:\"date\"}).set_index(\"date\").sort_index()\n",
    "        out.columns = [c.upper() for c in out.columns]\n",
    "    else:\n",
    "        code_col = next((c for c in d.columns if str(c).lower() in {\"code\",\"ccy\",\"currency\",\"symbol\",\"cur\"}), None)\n",
    "        val_col  = next((c for c in d.columns if str(c).lower() in {\"value\",\"rate\",\"px\",\"price\",\"close\",\"mid\"}), None)\n",
    "        if code_col is None or val_col is None:\n",
    "            raise ValueError(\"Cannot infer FX long-form columns (need code and value).\")\n",
    "        out = (d[[date_col, code_col, val_col]]\n",
    "               .rename(columns={date_col:\"date\", code_col:\"ccy\", val_col:\"value\"}))\n",
    "        out[\"ccy\"] = out[\"ccy\"].astype(str).str.upper()\n",
    "        out = out[out[\"ccy\"].isin(ccys)]\n",
    "        out = out.pivot_table(index=\"date\", columns=\"ccy\", values=\"value\", aggfunc=\"last\").sort_index()\n",
    "\n",
    "    out = out.astype(float)\n",
    "\n",
    "    # Orientation heuristic using GBP: USD/GBP typically O(1), not O(10^1-10^2)\n",
    "    if \"GBP\" in out.columns:\n",
    "        med = out[\"GBP\"].dropna().median()\n",
    "        if med > 10:\n",
    "            out[\"GBP\"] = 1.0 / out[\"GBP\"]\n",
    "\n",
    "    return out\n",
    "\n",
    "FX_CCYS = [FUNDING_CCY] + EM_CCYS\n",
    "fx_usd_per = standardize_fx_to_usd_per_ccy(fx_raw, FX_CCYS)\n",
    "display(fx_usd_per.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c7bed",
   "metadata": {},
   "source": [
    "## 4) Load swap curves (Emerging Mkt YC)\n",
    "\n",
    "We need (at minimum) **1Y and 5Y** par swap rates for each EM currency (more tenors is better).\n",
    "\n",
    "We standardize to long form:\n",
    "- `date`, `ccy`, `tenor` (years), `par_rate` (decimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e37b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "em_yc_path = find_one(DATA_EM_DIR, patterns=[\"*Emerging*Mkt*YC*.csv\", \"*Emerging*Mkt*YC*.parquet\", \"*Mkt*YC*.csv\", \"*Mkt*YC*.parquet\", \"*swap*curve*.csv\", \"*swap*curve*.parquet\"])\n",
    "print(\"EM YC file:\", em_yc_path)\n",
    "\n",
    "em_raw = read_table(em_yc_path)\n",
    "display(em_raw.head())\n",
    "print(em_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51459a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def standardize_swap_curve_long(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "\n",
    "    date_col = next((c for c in d.columns if str(c).lower() in {\"date\",\"dt\",\"time\",\"asof\"}), None)\n",
    "    if date_col is None:\n",
    "        date_col = d.columns[0]\n",
    "    d[date_col] = pd.to_datetime(d[date_col])\n",
    "\n",
    "    ccy_col = next((c for c in d.columns if str(c).lower() in {\"ccy\",\"currency\",\"cur\",\"code\"}), None)\n",
    "    tenor_col = next((c for c in d.columns if str(c).lower() in {\"tenor\",\"mat\",\"maturity\",\"years\",\"t\"}), None)\n",
    "    rate_col  = next((c for c in d.columns if str(c).lower() in {\"rate\",\"swap\",\"par_rate\",\"y\",\"yield\",\"value\"}), None)\n",
    "\n",
    "    if ccy_col and tenor_col and rate_col:\n",
    "        out = d[[date_col, ccy_col, tenor_col, rate_col]].rename(columns={\n",
    "            date_col:\"date\", ccy_col:\"ccy\", tenor_col:\"tenor\", rate_col:\"par_rate\"\n",
    "        })\n",
    "    else:\n",
    "        out_rows = []\n",
    "        for col in d.columns:\n",
    "            if col == date_col:\n",
    "                continue\n",
    "            s = str(col).upper()\n",
    "            m = re.match(r\"(?P<ccy>[A-Z]{3})[_\\\\-\\\\s]?(?P<ten>\\\\d+(\\\\.\\\\d+)?)\\\\s*(Y|YR|YEAR|YEARS)?$\", s)\n",
    "            if not m:\n",
    "                m = re.match(r\"(?P<ccy>[A-Z]{3})(?P<ten>\\\\d+(\\\\.\\\\d+)?)(Y|YR|YEAR|YEARS)$\", s)\n",
    "            if not m:\n",
    "                continue\n",
    "            ccy = m.group(\"ccy\")\n",
    "            ten = float(m.group(\"ten\"))\n",
    "            out_rows.append(pd.DataFrame({\"date\": d[date_col], \"ccy\": ccy, \"tenor\": ten, \"par_rate\": d[col]}))\n",
    "        if not out_rows:\n",
    "            raise ValueError(\"Cannot infer swap curve structure (need long form or parsable wide columns).\")\n",
    "        out = pd.concat(out_rows, ignore_index=True)\n",
    "\n",
    "    out[\"ccy\"] = out[\"ccy\"].astype(str).str.upper()\n",
    "    out[\"tenor\"] = out[\"tenor\"].astype(float)\n",
    "    out[\"par_rate\"] = out[\"par_rate\"].astype(float)\n",
    "\n",
    "    if out[\"par_rate\"].dropna().median() > 1.0:\n",
    "        out[\"par_rate\"] = out[\"par_rate\"] / 100.0\n",
    "\n",
    "    out = out.dropna(subset=[\"date\",\"ccy\",\"tenor\",\"par_rate\"]).sort_values([\"ccy\",\"date\",\"tenor\"])\n",
    "    return out\n",
    "\n",
    "swap_long = standardize_swap_curve_long(em_raw)\n",
    "swap_long = swap_long[swap_long[\"ccy\"].isin(EM_CCYS + [FUNDING_CCY])]\n",
    "display(swap_long.head())\n",
    "print(\"ccys:\", swap_long[\"ccy\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff480c",
   "metadata": {},
   "source": [
    "## 5) Bootstrapping + bond pricing (generalized from Zero_And_Spot_Curves)\n",
    "\n",
    "We implement a **par-curve → zero-curve** bootstrap and a **quarterly coupon bond pricer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _interp_z(times: np.ndarray, z_tenors: np.ndarray, z_rates: np.ndarray) -> np.ndarray:\n",
    "    return np.interp(times, z_tenors, z_rates, left=z_rates[0], right=z_rates[-1])\n",
    "\n",
    "def zcb_from_par_curve(par: pd.Series, freq: int = COUPON_FREQ) -> pd.Series:\n",
    "    '''\n",
    "    Bootstrap continuous-compounded zero rates at the par curve's tenor pillars.\n",
    "    Input: par[tenor_years] = par coupon rate (decimal) for a par bond with coupon freq.\n",
    "    Output: zcb[tenor_years] = continuously-compounded zero rate at that tenor.\n",
    "    '''\n",
    "    par = par.dropna().sort_index()\n",
    "    tenors = par.index.to_numpy(dtype=float)\n",
    "    z = pd.Series(index=tenors, dtype=float)\n",
    "\n",
    "    delta = 1.0 / freq\n",
    "\n",
    "    for T in tenors:\n",
    "        c = float(par.loc[T])\n",
    "        times = np.arange(delta, T + 1e-12, delta)\n",
    "        if len(times) == 0:\n",
    "            z.loc[T] = np.nan\n",
    "            continue\n",
    "        times_pre = times[:-1]\n",
    "        if len(times_pre) == 0:\n",
    "            D_T = 1.0 / (1.0 + c*delta)\n",
    "        else:\n",
    "            known_t = z.dropna().index.to_numpy(dtype=float)\n",
    "            known_z = z.dropna().to_numpy(dtype=float)\n",
    "            if len(known_t) == 0:\n",
    "                z_pre = np.full_like(times_pre, c)\n",
    "            else:\n",
    "                z_pre = _interp_z(times_pre, known_t, known_z)\n",
    "            D_pre = np.exp(-z_pre * times_pre)\n",
    "            pv_cpn_pre = (c*delta) * D_pre.sum()\n",
    "            D_T = (1.0 - pv_cpn_pre) / (1.0 + c*delta)\n",
    "\n",
    "        D_T = float(np.clip(D_T, 1e-12, 1.0))\n",
    "        z.loc[T] = -np.log(D_T) / T\n",
    "\n",
    "    return z\n",
    "\n",
    "def bond_price_from_zcb(zcb: pd.Series, coupon_rate: float, tenor: float, freq: int = COUPON_FREQ) -> float:\n",
    "    '''\n",
    "    Clean PV per 1 notional for a fixed-rate bond with coupon_rate (annual, decimal),\n",
    "    coupons at freq, maturity tenor (years), discounted by zcb curve (cc zero rates).\n",
    "    '''\n",
    "    delta = 1.0 / freq\n",
    "    times = np.arange(delta, tenor + 1e-12, delta)\n",
    "    if len(times) == 0:\n",
    "        return 1.0\n",
    "    z_t = zcb.index.to_numpy(dtype=float)\n",
    "    z_r = zcb.to_numpy(dtype=float)\n",
    "    z_i = _interp_z(times, z_t, z_r)\n",
    "    D = np.exp(-z_i * times)\n",
    "    pv_cpn = (coupon_rate * delta) * D.sum()\n",
    "    pv_pri = D[-1]\n",
    "    return float(pv_cpn + pv_pri)\n",
    "\n",
    "def price_weekly_roll(par_curve_new: pd.Series, coupon_rate_orig: float, tenor_orig: float = 5.0,\n",
    "                      dt: float = WEEK_DT, freq: int = COUPON_FREQ) -> float:\n",
    "    rem = max(tenor_orig - dt, 1e-6)\n",
    "    zcb_new = zcb_from_par_curve(par_curve_new, freq=freq)\n",
    "    return bond_price_from_zcb(zcb_new, coupon_rate=coupon_rate_orig, tenor=rem, freq=freq)\n",
    "\n",
    "# sanity check\n",
    "_par = pd.Series({1.0:0.04, 2.0:0.042, 3.0:0.043, 5.0:0.045, 10.0:0.047})\n",
    "_zcb = zcb_from_par_curve(_par)\n",
    "p0 = bond_price_from_zcb(_zcb, coupon_rate=_par[5.0], tenor=5.0)\n",
    "print(\"Par bond price (should be ~1):\", p0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6bee1f",
   "metadata": {},
   "source": [
    "## 6) Curve panels + weekly alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1132005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def curve_panel(swap_long: pd.DataFrame, ccy: str) -> pd.DataFrame:\n",
    "    d = swap_long[swap_long[\"ccy\"] == ccy].copy()\n",
    "    if d.empty:\n",
    "        raise ValueError(f\"No curve data for {ccy}.\")\n",
    "    piv = d.pivot_table(index=\"date\", columns=\"tenor\", values=\"par_rate\", aggfunc=\"last\").sort_index()\n",
    "    piv.columns = piv.columns.astype(float)\n",
    "    piv = piv.sort_index(axis=1)\n",
    "    return piv\n",
    "\n",
    "curves: Dict[str, pd.DataFrame] = {ccy: curve_panel(swap_long, ccy) for ccy in sorted(set(swap_long[\"ccy\"]))}\n",
    "print(\"curve currencies:\", list(curves.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee13263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def choose_weekly_dates(idx: pd.DatetimeIndex, weekday: int = 2, tol_days: int = 2) -> pd.DatetimeIndex:\n",
    "    idx = pd.DatetimeIndex(idx).sort_values()\n",
    "    start = idx.min().normalize()\n",
    "    end = idx.max().normalize()\n",
    "    anchors = pd.date_range(start=start, end=end, freq=\"W-\" + [\"MON\",\"TUE\",\"WED\",\"THU\",\"FRI\",\"SAT\",\"SUN\"][weekday])\n",
    "\n",
    "    chosen = []\n",
    "    idx_set = set(idx)\n",
    "    for a in anchors:\n",
    "        window = [a + pd.Timedelta(days=k) for k in range(0, tol_days+1)] + \\\n",
    "                 [a - pd.Timedelta(days=k) for k in range(1, tol_days+1)]\n",
    "        pick = next((d for d in window if d in idx_set), None)\n",
    "        if pick is not None:\n",
    "            chosen.append(pick)\n",
    "    return pd.DatetimeIndex(sorted(set(chosen)))\n",
    "\n",
    "def align_weekly_inputs(curves: Dict[str, pd.DataFrame], ois: pd.Series, fx_usd_per: pd.DataFrame,\n",
    "                        weekday: int = 2, tol_days: int = 2):\n",
    "    idx = ois.index\n",
    "    for _,df in curves.items():\n",
    "        idx = idx.intersection(df.index)\n",
    "    idx = idx.intersection(fx_usd_per.index)\n",
    "\n",
    "    if len(idx) == 0:\n",
    "        raise ValueError(\"No overlapping dates across OIS, FX, and curves. Check parsing/sources.\")\n",
    "\n",
    "    wk = choose_weekly_dates(idx, weekday=weekday, tol_days=tol_days)\n",
    "\n",
    "    ois_d = ois.reindex(idx).ffill().loc[wk]\n",
    "    fx_d  = fx_usd_per.reindex(idx).ffill().loc[wk]\n",
    "    curves_w = {c: df.reindex(idx).ffill().loc[wk] for c,df in curves.items()}\n",
    "    return wk, curves_w, ois_d, fx_d\n",
    "\n",
    "wk_dates, curves_w, ois_w, fx_w = align_weekly_inputs(curves, ois, fx_usd_per)\n",
    "print(\"weekly obs:\", len(wk_dates), \"from\", wk_dates.min().date(), \"to\", wk_dates.max().date())\n",
    "display(fx_w.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6f7257",
   "metadata": {},
   "source": [
    "## 7) Weekly P&L per currency (core deliverable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_s5(curve_row: pd.Series) -> float:\n",
    "    ten = curve_row.index.to_numpy(dtype=float)\n",
    "    val = curve_row.to_numpy(dtype=float)\n",
    "    return float(np.interp(5.0, ten, val))\n",
    "\n",
    "def weekly_pnl_one_ccy(\n",
    "    lend_ccy: str,\n",
    "    fund_ccy: str,\n",
    "    curves_w: Dict[str, pd.DataFrame],\n",
    "    ois_w: pd.Series,\n",
    "    fx_w: pd.DataFrame,\n",
    "    notional_usd: float = USD_NOTIONAL,\n",
    "    borrow_frac: float = BORROW_FRAC,\n",
    "    borrow_spread: float = BORROW_SPREAD,\n",
    "    dt: float = WEEK_DT,\n",
    "    freq: int = COUPON_FREQ,\n",
    ") -> pd.DataFrame:\n",
    "    if fund_ccy not in fx_w.columns or lend_ccy not in fx_w.columns:\n",
    "        raise KeyError(f\"FX missing for {fund_ccy} or {lend_ccy}. Columns={list(fx_w.columns)}\")\n",
    "\n",
    "    fund_curve = curves_w.get(fund_ccy, None)\n",
    "    use_fund_curve = fund_curve is not None\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(fx_w.index)-1):\n",
    "        t0 = fx_w.index[i]\n",
    "        t1 = fx_w.index[i+1]\n",
    "\n",
    "        fx_f0, fx_f1 = float(fx_w.loc[t0, fund_ccy]), float(fx_w.loc[t1, fund_ccy])\n",
    "        fx_l0, fx_l1 = float(fx_w.loc[t0, lend_ccy]), float(fx_w.loc[t1, lend_ccy])\n",
    "\n",
    "        lend0 = curves_w[lend_ccy].loc[t0].dropna()\n",
    "        lend1 = curves_w[lend_ccy].loc[t1].dropna()\n",
    "        if len(lend0) < 2 or len(lend1) < 2:\n",
    "            continue\n",
    "\n",
    "        s5_l = get_s5(lend0)\n",
    "        if use_fund_curve:\n",
    "            fund0 = fund_curve.loc[t0].dropna()\n",
    "            s5_f = get_s5(fund0) if len(fund0) >= 2 else float(ois_w.loc[t0])\n",
    "        else:\n",
    "            s5_f = float(ois_w.loc[t0])\n",
    "\n",
    "        active = (s5_l - s5_f) >= 0.0050\n",
    "        if not active:\n",
    "            rows.append((t1, 0.0, 0.0, 0.0, 0.0, s5_l, s5_f, False))\n",
    "            continue\n",
    "\n",
    "        # lending leg\n",
    "        N_lend = notional_usd / fx_l0\n",
    "        p1 = price_weekly_roll(lend1, coupon_rate_orig=s5_l, tenor_orig=5.0, dt=dt, freq=freq)\n",
    "        cpn_accr_lend = N_lend * s5_l * dt\n",
    "        lend_value_usd_t1 = (N_lend * p1 + cpn_accr_lend) * fx_l1\n",
    "        lend_pnl = lend_value_usd_t1 - notional_usd\n",
    "\n",
    "        # funding leg\n",
    "        borrow_usd = borrow_frac * notional_usd\n",
    "        B_fund = borrow_usd / fx_f0\n",
    "        r_borrow = float(ois_w.loc[t0]) + borrow_spread\n",
    "        int_fund = B_fund * r_borrow * dt\n",
    "        repay_usd_t1 = (B_fund + int_fund) * fx_f1\n",
    "        fund_pnl = borrow_usd - repay_usd_t1\n",
    "\n",
    "        total_pnl = lend_pnl + fund_pnl\n",
    "        ret = total_pnl / notional_usd\n",
    "        rows.append((t1, total_pnl, ret, lend_pnl, fund_pnl, s5_l, s5_f, True))\n",
    "\n",
    "    out = pd.DataFrame(rows, columns=[\"date\",\"pnl_usd\",\"ret\",\"lend_pnl_usd\",\"fund_pnl_usd\",\"s5_lend\",\"s5_fund\",\"active\"]).set_index(\"date\")\n",
    "    return out\n",
    "\n",
    "pnl_panels = {ccy: weekly_pnl_one_ccy(ccy, FUNDING_CCY, curves_w, ois_w, fx_w) for ccy in EM_CCYS}\n",
    "{k: v.shape for k,v in pnl_panels.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18e2955",
   "metadata": {},
   "source": [
    "## 8) Performance analysis (required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def max_drawdown(x: pd.Series) -> float:\n",
    "    peak = x.cummax()\n",
    "    dd = (x / peak) - 1.0\n",
    "    return float(dd.min())\n",
    "\n",
    "def ann_sharpe_weekly(r: pd.Series) -> float:\n",
    "    r = r.dropna()\n",
    "    if len(r) < 10 or r.std(ddof=1) == 0:\n",
    "        return np.nan\n",
    "    return float(np.sqrt(52.0) * r.mean() / r.std(ddof=1))\n",
    "\n",
    "stats = []\n",
    "rets = {}\n",
    "for ccy, df in pnl_panels.items():\n",
    "    r = df[\"ret\"].fillna(0.0)\n",
    "    rets[ccy] = r\n",
    "    wealth = (1.0 + r).cumprod()\n",
    "    stats.append({\n",
    "        \"ccy\": ccy,\n",
    "        \"active_weeks\": int(df[\"active\"].sum()),\n",
    "        \"ann_sharpe\": ann_sharpe_weekly(r),\n",
    "        \"ann_mean\": float(52.0 * r.mean()),\n",
    "        \"ann_vol\": float(np.sqrt(52.0) * r.std(ddof=1)),\n",
    "        \"max_dd\": max_drawdown(wealth),\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(stats).sort_values(\"ann_sharpe\", ascending=False)\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738888d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rets_df = pd.DataFrame(rets).reindex(sorted(rets.keys()), axis=1)\n",
    "corr = rets_df.corr()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.imshow(corr.values, aspect=\"auto\")\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=45, ha=\"right\")\n",
    "plt.yticks(range(len(corr.index)), corr.index)\n",
    "plt.colorbar()\n",
    "plt.title(\"Correlation of weekly carry returns (by lending currency)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "active_df = pd.DataFrame({ccy: pnl_panels[ccy][\"active\"].astype(int) for ccy in EM_CCYS})\n",
    "ret_df = pd.DataFrame({ccy: pnl_panels[ccy][\"ret\"].fillna(0.0) for ccy in EM_CCYS})\n",
    "\n",
    "w = active_df.div(active_df.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "port_ret = (w * ret_df).sum(axis=1)\n",
    "port_wealth = (1.0 + port_ret).cumprod()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(port_wealth.index, port_wealth.values)\n",
    "plt.title(\"Equal-weight FX carry portfolio (weekly rebalanced; active-only weights)\")\n",
    "plt.ylabel(\"Wealth (start=1)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Portfolio ann Sharpe:\", ann_sharpe_weekly(port_ret))\n",
    "print(\"Portfolio max drawdown:\", max_drawdown(port_wealth))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92af9d8f",
   "metadata": {},
   "source": [
    "## 9) Robustness checks (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160fe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_sensitivity_weekday(weekday: int) -> dict:\n",
    "    wk_dates2, curves_w2, ois_w2, fx_w2 = align_weekly_inputs(curves, ois, fx_usd_per, weekday=weekday, tol_days=2)\n",
    "    pnl2 = {ccy: weekly_pnl_one_ccy(ccy, FUNDING_CCY, curves_w2, ois_w2, fx_w2) for ccy in EM_CCYS}\n",
    "    active2 = pd.DataFrame({ccy: pnl2[ccy][\"active\"].astype(int) for ccy in EM_CCYS})\n",
    "    ret2 = pd.DataFrame({ccy: pnl2[ccy][\"ret\"].fillna(0.0) for ccy in EM_CCYS})\n",
    "    w2 = active2.div(active2.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "    pr = (w2 * ret2).sum(axis=1)\n",
    "    wealth = (1.0 + pr).cumprod()\n",
    "    return {\"weekday\":weekday, \"ann_sharpe\":ann_sharpe_weekly(pr), \"max_dd\":max_drawdown(wealth), \"weeks\":len(pr)}\n",
    "\n",
    "pd.DataFrame([run_sensitivity_weekday(wd) for wd in [1,2,3]])  # Tue, Wed, Thu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fad1b",
   "metadata": {},
   "source": [
    "## 10) Export outputs for report generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc326408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUT_DIR = NB_DIR / \"outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "stats_df.to_csv(OUT_DIR / \"currency_stats.csv\", index=False)\n",
    "corr.to_csv(OUT_DIR / \"currency_corr.csv\")\n",
    "port_ret.to_frame(\"port_ret\").to_csv(OUT_DIR / \"portfolio_weekly_returns.csv\")\n",
    "\n",
    "print(\"Wrote outputs to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623d2af5",
   "metadata": {},
   "source": [
    "### Notebook checklist\n",
    "\n",
    "- [ ] Data files found under `../../Data/` and `./data_clean/`  \n",
    "- [ ] Weekly aligned dataset has non-trivial history across all currencies  \n",
    "- [ ] Per-currency weekly returns created  \n",
    "- [ ] Correlation matrix + drawdowns computed  \n",
    "- [ ] `./outputs/` contains tables for report generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
