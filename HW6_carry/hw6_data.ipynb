{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5451fc",
   "metadata": {},
   "source": [
    "# HW6 — Data Grab (BoE OIS + Nasdaq FX + Emerging Market Swap Curves)\n",
    "\n",
    "This notebook fetches the **required inputs** for *FX Carry Strategy*:\n",
    "\n",
    "- UK OIS short rate via **BoE IUDSOIA** (preferred, simplest)\n",
    "- Spot FX via **Nasdaq Data Link Tables API** `EDI/CUR` for: GBP, TRY, ZAR, PKR, BRL, NGN\n",
    "- Emerging-market swap curves from the **“Emerging Mkt YC”** file (class website download)\n",
    "\n",
    "Outputs are written to `data_clean/` as Parquet files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87d79a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTEBOOK_DIR: C:\\Users\\baile\\Box\\Winter26\\QTS\\HW_Assignment_Files\\HW6_carry\n",
      "RAW_DIR     : C:\\Users\\baile\\Box\\Winter26\\QTS\\HW_Assignment_Files\\HW6_carry\\data_raw\n",
      "CLEAN_DIR   : C:\\Users\\baile\\Box\\Winter26\\QTS\\HW_Assignment_Files\\HW6_carry\\data_clean\n",
      "NASDAQ key loaded: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import Any, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "NOTEBOOK_DIR = Path.cwd().resolve()\n",
    "RAW_DIR   = NOTEBOOK_DIR / \"data_raw\"\n",
    "CLEAN_DIR = NOTEBOOK_DIR / \"data_clean\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Load .env (Nasdaq API key; optional EMYC url)\n",
    "# -----------------------------\n",
    "def _load_dotenv(env_path: Path = NOTEBOOK_DIR / \".env\") -> None:\n",
    "    if not env_path.exists():\n",
    "        return\n",
    "    try:\n",
    "        from dotenv import load_dotenv\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Install python-dotenv: `pip install python-dotenv`\") from e\n",
    "    load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "_load_dotenv()\n",
    "\n",
    "# -----------------------------\n",
    "# HTTP session with retry\n",
    "# -----------------------------\n",
    "def _session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    retry = Retry(\n",
    "        total=8,\n",
    "        backoff_factor=0.5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=(\"GET\", \"HEAD\"),\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retry))\n",
    "    s.headers.update({\"User-Agent\": \"hw6-data-grab/1.0\"})\n",
    "    return s\n",
    "\n",
    "SESSION = _session()\n",
    "\n",
    "def _download(url: str, dst: Path, *, force: bool = False, timeout: int = 180) -> Path:\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists() and not force:\n",
    "        return dst\n",
    "    r = SESSION.get(url, stream=True, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    tmp = dst.with_suffix(dst.suffix + \".tmp\")\n",
    "    with tmp.open(\"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=1 << 20):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    tmp.replace(dst)\n",
    "    return dst\n",
    "\n",
    "print(\"NOTEBOOK_DIR:\", NOTEBOOK_DIR)\n",
    "print(\"RAW_DIR     :\", RAW_DIR)\n",
    "print(\"CLEAN_DIR   :\", CLEAN_DIR)\n",
    "print(\"NASDAQ key loaded:\", bool(os.getenv(\"NASDAQ_DATA_LINK_API_KEY\") or os.getenv(\"QUANDL_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94de891",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IUDSOIA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2026-02-06</th>\n",
       "      <td>0.037279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-02-09</th>\n",
       "      <td>0.037271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-02-10</th>\n",
       "      <td>0.037270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-02-11</th>\n",
       "      <td>0.037274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026-02-12</th>\n",
       "      <td>0.037272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             IUDSOIA\n",
       "DATE                \n",
       "2026-02-06  0.037279\n",
       "2026-02-09  0.037271\n",
       "2026-02-10  0.037270\n",
       "2026-02-11  0.037274\n",
       "2026-02-12  0.037272"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Bank of England: IUD series (CSV endpoint)\n",
    "# -----------------------------\n",
    "def _boe_date_str(ts: pd.Timestamp) -> str:\n",
    "    ts = pd.Timestamp(ts).normalize()\n",
    "    return f\"{ts.day:02d}/{ts.strftime('%b')}/{ts.year}\"\n",
    "\n",
    "def _find_csv_header_row(text: str) -> int:\n",
    "    lines = text.splitlines()\n",
    "    for i, ln in enumerate(lines):\n",
    "        s = ln.strip().strip(\"\\ufeff\")\n",
    "        if not s:\n",
    "            continue\n",
    "        if re.match(r'^\"?DATE\"?[,;]', s.upper()):\n",
    "            return i\n",
    "    return 0\n",
    "\n",
    "def fetch_boe_iud_series(\n",
    "    series_code: str,\n",
    "    start: str = \"1963-01-01\",\n",
    "    end: str | None = None,\n",
    "    *,\n",
    "    cache_dir: Path = RAW_DIR / \"boe\",\n",
    "    force: bool = False,\n",
    "    rate_unit: str = \"decimal\",  # \"decimal\" or \"percent\"\n",
    ") -> pd.DataFrame:\n",
    "    '''\n",
    "    Fetch BoE IADB series via CSV endpoint.\n",
    "\n",
    "    Returns: DataFrame indexed by date with one float column named `series_code`.\n",
    "    BoE rates are typically in percent; set rate_unit=\"decimal\" to divide by 100.\n",
    "    '''\n",
    "    end_ts = pd.Timestamp(end) if end else pd.Timestamp.today().normalize()\n",
    "    start_ts = pd.Timestamp(start).normalize()\n",
    "\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = cache_dir / f\"{series_code}_{start_ts.date()}_{end_ts.date()}_{rate_unit}.parquet\"\n",
    "    if out_path.exists() and not force:\n",
    "        return pd.read_parquet(out_path)\n",
    "\n",
    "    urls = [\n",
    "        \"https://www.bankofengland.co.uk/boeapps/database/_iadb-fromshowcolumns.asp\",\n",
    "        \"https://www.bankofengland.co.uk/boeapps/iadb/fromshowcolumns.asp\",\n",
    "    ]\n",
    "\n",
    "    params = {\n",
    "        \"csv.x\": \"yes\",\n",
    "        \"Datefrom\": _boe_date_str(start_ts),\n",
    "        \"Dateto\": _boe_date_str(end_ts),\n",
    "        \"SeriesCodes\": series_code,\n",
    "        \"CSVF\": \"CN\",       # try columnar first\n",
    "        \"UsingCodes\": \"Y\",\n",
    "        \"VPD\": \"Y\",\n",
    "    }\n",
    "\n",
    "    last_err: Exception | None = None\n",
    "    for url in urls:\n",
    "        for csvf in (\"CN\", \"TN\"):\n",
    "            params[\"CSVF\"] = csvf\n",
    "            try:\n",
    "                r = SESSION.get(url, params=params, timeout=120)\n",
    "                r.raise_for_status()\n",
    "                txt = r.content.decode(\"utf-8\", errors=\"replace\")\n",
    "                if \"<html\" in txt.lower():\n",
    "                    raise RuntimeError(f\"BoE returned HTML (not CSV): {url} CSVF={csvf}\")\n",
    "\n",
    "                skip = _find_csv_header_row(txt)\n",
    "                df = pd.read_csv(io.StringIO(txt), skiprows=skip)\n",
    "                df.columns = [c.strip().strip(\"\\ufeff\") for c in df.columns]\n",
    "\n",
    "                # Tabular layout: DATE + series column\n",
    "                if \"DATE\" in df.columns and series_code in df.columns:\n",
    "                    out = df[[\"DATE\", series_code]].copy()\n",
    "                    out[\"DATE\"] = pd.to_datetime(out[\"DATE\"], errors=\"coerce\", dayfirst=True)\n",
    "                    out[series_code] = pd.to_numeric(out[series_code], errors=\"coerce\")\n",
    "                    out = out.dropna(subset=[\"DATE\"]).drop_duplicates(subset=[\"DATE\"]).sort_values(\"DATE\")\n",
    "                    out = out.set_index(\"DATE\")\n",
    "                else:\n",
    "                    # Columnar layout: DATE + SERIES + VALUE\n",
    "                    lcols = {c.lower(): c for c in df.columns}\n",
    "                    date_col = lcols.get(\"date\") or next((c for c in df.columns if \"date\" in c.lower()), None)\n",
    "                    series_col = lcols.get(\"series\") or lcols.get(\"seriescode\") or lcols.get(\"series_code\")\n",
    "                    value_col = lcols.get(\"value\") or lcols.get(\"obs_value\") or lcols.get(\"rate\")\n",
    "                    if not (date_col and series_col and value_col):\n",
    "                        raise RuntimeError(f\"Unrecognized CSV layout: cols={df.columns.tolist()}\")\n",
    "                    out = df[[date_col, series_col, value_col]].copy()\n",
    "                    out = out[out[series_col].astype(str).str.strip().eq(series_code)]\n",
    "                    out[date_col] = pd.to_datetime(out[date_col], errors=\"coerce\", dayfirst=True)\n",
    "                    out[value_col] = pd.to_numeric(out[value_col], errors=\"coerce\")\n",
    "                    out = out.dropna(subset=[date_col]).drop_duplicates(subset=[date_col]).sort_values(date_col)\n",
    "                    out = out.set_index(date_col)[[value_col]].rename(columns={value_col: series_code})\n",
    "\n",
    "                if rate_unit == \"decimal\":\n",
    "                    out[series_code] = out[series_code] / 100.0\n",
    "\n",
    "                out.to_parquet(out_path)\n",
    "                return out\n",
    "\n",
    "            except Exception as e:\n",
    "                last_err = e\n",
    "                continue\n",
    "\n",
    "    raise RuntimeError(f\"Failed to fetch BoE series {series_code}. Last error: {last_err}\")\n",
    "\n",
    "# Required by assignment:\n",
    "sonia = fetch_boe_iud_series(\"IUDSOIA\", rate_unit=\"decimal\")\n",
    "sonia.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a2aedfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping BoE OIS curve archive (set USE_OIS_ARCHIVE=True if you need it).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# OPTIONAL: BoE OIS curve archive (oisddata.zip)\n",
    "# The assignment allows using this in place of IUDSOIA, but it is not required if IUDSOIA works.\n",
    "# This cell is safe-by-default: it will NOT run unless you set USE_OIS_ARCHIVE=True.\n",
    "# -----------------------------\n",
    "USE_OIS_ARCHIVE = False\n",
    "\n",
    "BOE_OIS_DAILY_ZIP = \"https://www.bankofengland.co.uk/-/media/boe/files/statistics/yield-curves/oisddata.zip\"\n",
    "\n",
    "_TENOR_RE = re.compile(r\"^(?P<num>\\d+(?:\\.\\d+)?)\\s*(?P<unit>D|W|M|Y|YR|YEARS?)$\", re.I)\n",
    "\n",
    "def _tenor_to_years(label: str) -> float | None:\n",
    "    s = str(label).strip().upper()\n",
    "    if s in {\"ON\", \"O/N\", \"OVERNIGHT\"}:\n",
    "        return 1.0 / 365.0\n",
    "    m = _TENOR_RE.match(s.replace(\" \", \"\"))\n",
    "    if not m:\n",
    "        # common patterns: '1W', '3M', '6M', '10Y'\n",
    "        m = re.match(r\"^(\\d+(?:\\.\\d+)?)(D|W|M|Y)$\", s)\n",
    "    if not m:\n",
    "        return None\n",
    "    num = float(m.group(1))\n",
    "    unit = m.group(2)\n",
    "    if unit == \"D\":\n",
    "        return num / 365.0\n",
    "    if unit == \"W\":\n",
    "        return (7.0 * num) / 365.0\n",
    "    if unit == \"M\":\n",
    "        return num / 12.0\n",
    "    if unit in {\"Y\", \"YR\"}:\n",
    "        return num\n",
    "    return None\n",
    "\n",
    "def _find_header_row(mat: pd.DataFrame) -> int | None:\n",
    "    # locate a row containing 'date' and multiple tenor-like labels\n",
    "    for i in range(min(len(mat), 60)):\n",
    "        row = mat.iloc[i].astype(str).str.strip().str.lower()\n",
    "        if (row == \"date\").any():\n",
    "            # require at least 3 tenor-ish labels in the row\n",
    "            hits = 0\n",
    "            for v in row.values:\n",
    "                if _tenor_to_years(v) is not None:\n",
    "                    hits += 1\n",
    "            if hits >= 3:\n",
    "                return i\n",
    "    return None\n",
    "\n",
    "def _parse_ois_sheet(xls: pd.ExcelFile, sheet: str) -> pd.DataFrame | None:\n",
    "    mat = pd.read_excel(xls, sheet_name=sheet, header=None)\n",
    "    hdr = _find_header_row(mat)\n",
    "    if hdr is None:\n",
    "        return None\n",
    "\n",
    "    header = mat.iloc[hdr].astype(str).str.strip()\n",
    "    data = mat.iloc[hdr+1:].copy()\n",
    "    data.columns = header\n",
    "\n",
    "    # normalize date col\n",
    "    date_col = next((c for c in data.columns if str(c).strip().lower() == \"date\"), None)\n",
    "    if date_col is None:\n",
    "        return None\n",
    "\n",
    "    data[date_col] = pd.to_datetime(data[date_col], errors=\"coerce\", dayfirst=True)\n",
    "    data = data.dropna(subset=[date_col])\n",
    "\n",
    "    # tenor columns\n",
    "    ten_cols = []\n",
    "    ten_map = {}\n",
    "    for c in data.columns:\n",
    "        if c == date_col:\n",
    "            continue\n",
    "        ty = _tenor_to_years(str(c))\n",
    "        if ty is not None:\n",
    "            ten_cols.append(c)\n",
    "            ten_map[c] = ty\n",
    "\n",
    "    if len(ten_cols) < 3:\n",
    "        return None\n",
    "\n",
    "    out = data[[date_col] + ten_cols].melt(id_vars=[date_col], var_name=\"tenor_label\", value_name=\"rate_pct\")\n",
    "    out[\"tenor_years\"] = out[\"tenor_label\"].map(ten_map).astype(float)\n",
    "    out[\"rate_pct\"] = pd.to_numeric(out[\"rate_pct\"], errors=\"coerce\")\n",
    "    out = out.dropna(subset=[\"rate_pct\", \"tenor_years\"])\n",
    "    out = out.rename(columns={date_col: \"date\"})\n",
    "    out[\"rate\"] = out[\"rate_pct\"] / 100.0  # decimal\n",
    "    out = out[[\"date\", \"tenor_years\", \"rate\"]].sort_values([\"date\", \"tenor_years\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def fetch_boe_ois_archive_tidy(*, force: bool = False) -> pd.DataFrame:\n",
    "    zip_path = _download(BOE_OIS_DAILY_ZIP, RAW_DIR / \"boe\" / \"oisddata.zip\", force=force)\n",
    "    tidy_frames: list[pd.DataFrame] = []\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        members = [n for n in z.namelist() if not n.endswith(\"/\")]\n",
    "        for name in sorted(members):\n",
    "            if \"__MACOSX\" in name or Path(name).name.startswith(\".\"):\n",
    "                continue\n",
    "            ext = Path(name).suffix.lower()\n",
    "            if ext not in {\".xlsx\", \".xls\"}:\n",
    "                continue\n",
    "            b = z.read(name)\n",
    "            xls = pd.ExcelFile(io.BytesIO(b))\n",
    "            for sh in xls.sheet_names:\n",
    "                parsed = _parse_ois_sheet(xls, sh)\n",
    "                if parsed is None or parsed.empty:\n",
    "                    continue\n",
    "                parsed[\"__source_file\"] = name\n",
    "                parsed[\"__sheet\"] = sh\n",
    "                tidy_frames.append(parsed)\n",
    "\n",
    "    if not tidy_frames:\n",
    "        raise RuntimeError(\"Could not locate any parseable OIS curve tables inside oisddata.zip.\")\n",
    "    out = pd.concat(tidy_frames, ignore_index=True).drop_duplicates(subset=[\"date\",\"tenor_years\",\"__source_file\",\"__sheet\"])\n",
    "    return out\n",
    "\n",
    "if USE_OIS_ARCHIVE:\n",
    "    ois_curve = fetch_boe_ois_archive_tidy()\n",
    "    ois_curve.to_parquet(CLEAN_DIR / \"boe_ois_curve_tidy.parquet\", index=False)\n",
    "    print(\"Saved:\", CLEAN_DIR / \"boe_ois_curve_tidy.parquet\", \"rows:\", len(ois_curve))\n",
    "    display(ois_curve.head())\n",
    "else:\n",
    "    print(\"Skipping BoE OIS curve archive (set USE_OIS_ARCHIVE=True if you need it).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2972fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Nasdaq Data Link Tables API utilities\n",
    "# -----------------------------\n",
    "def nasdaq_api_key() -> str:\n",
    "    key = os.getenv(\"NASDAQ_DATA_LINK_API_KEY\") or os.getenv(\"QUANDL_API_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\"Missing NASDAQ_DATA_LINK_API_KEY (or QUANDL_API_KEY). Put it in .env or your environment.\")\n",
    "    return key\n",
    "\n",
    "def nasdaq_datatable_metadata(datatable_code: str, *, api_key: str | None = None) -> dict[str, Any]:\n",
    "    api_key = api_key or nasdaq_api_key()\n",
    "    url = f\"https://data.nasdaq.com/api/v3/datatables/{datatable_code}/metadata.json\"\n",
    "    r = SESSION.get(url, params={\"api_key\": api_key}, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def nasdaq_datatable_fetch(\n",
    "    datatable_code: str,\n",
    "    params: dict[str, Any],\n",
    "    *,\n",
    "    api_key: str | None = None,\n",
    "    max_pages: int = 10_000,\n",
    ") -> pd.DataFrame:\n",
    "    api_key = api_key or nasdaq_api_key()\n",
    "    url = f\"https://data.nasdaq.com/api/v3/datatables/{datatable_code}.json\"\n",
    "\n",
    "    cursor: str | None = None\n",
    "    out: list[pd.DataFrame] = []\n",
    "\n",
    "    for _ in range(max_pages):\n",
    "        p = dict(params)\n",
    "        p[\"api_key\"] = api_key\n",
    "        if cursor:\n",
    "            p[\"qopts.cursor_id\"] = cursor\n",
    "\n",
    "        r = SESSION.get(url, params=p, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        js = r.json()\n",
    "\n",
    "        cols = [c[\"name\"] for c in js[\"datatable\"][\"columns\"]]\n",
    "        data = js[\"datatable\"][\"data\"]\n",
    "        out.append(pd.DataFrame(data, columns=cols))\n",
    "\n",
    "        cursor = js.get(\"meta\", {}).get(\"next_cursor_id\")\n",
    "        if not cursor:\n",
    "            break\n",
    "\n",
    "    return pd.concat(out, ignore_index=True) if out else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5016d725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved FX files to: C:\\Users\\baile\\Box\\Winter26\\QTS\\HW_Assignment_Files\\HW6_carry\\data_clean\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ccy</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56364</th>\n",
       "      <td>2026-02-12</td>\n",
       "      <td>NGN</td>\n",
       "      <td>1354.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56365</th>\n",
       "      <td>2026-02-13</td>\n",
       "      <td>NGN</td>\n",
       "      <td>1355.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56366</th>\n",
       "      <td>2026-02-14</td>\n",
       "      <td>NGN</td>\n",
       "      <td>1353.578275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56367</th>\n",
       "      <td>2026-02-15</td>\n",
       "      <td>NGN</td>\n",
       "      <td>1353.578275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56368</th>\n",
       "      <td>2026-02-16</td>\n",
       "      <td>NGN</td>\n",
       "      <td>1354.150000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  ccy        value\n",
       "56364 2026-02-12  NGN  1354.010000\n",
       "56365 2026-02-13  NGN  1355.580000\n",
       "56366 2026-02-14  NGN  1353.578275\n",
       "56367 2026-02-15  NGN  1353.578275\n",
       "56368 2026-02-16  NGN  1354.150000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# FX from EDI/CUR\n",
    "# -----------------------------\n",
    "def _guess_fx_columns(meta: dict[str, Any]) -> tuple[str, str, str]:\n",
    "    cols = [c[\"name\"] for c in meta[\"datatable\"][\"columns\"]]\n",
    "    lc = {c.lower(): c for c in cols}\n",
    "\n",
    "    date_col = lc.get(\"date\") or next((c for c in cols if \"date\" in c.lower()), None)\n",
    "    if not date_col:\n",
    "        raise RuntimeError(f\"Could not identify date column from columns={cols}\")\n",
    "\n",
    "    ccy_col = next((c for c in cols if c.lower() in {\"code\", \"currency\", \"iso_code\", \"ccy\", \"ticker\"}), None)\n",
    "    if not ccy_col:\n",
    "        ccy_col = next((c for c in cols if \"cur\" in c.lower() or \"code\" in c.lower() or \"tick\" in c.lower()), None)\n",
    "    if not ccy_col:\n",
    "        raise RuntimeError(f\"Could not identify currency column from columns={cols}\")\n",
    "\n",
    "    candidates = [c for c in cols if c not in {date_col, ccy_col}]\n",
    "    prefer = next((c for c in candidates if c.lower() in {\"rate\", \"value\", \"fx\", \"exchange_rate\"}), None)\n",
    "    value_col = prefer or (candidates[0] if candidates else None)\n",
    "    if not value_col:\n",
    "        raise RuntimeError(f\"Could not identify value column from columns={cols}\")\n",
    "\n",
    "    return date_col, ccy_col, value_col\n",
    "\n",
    "def fetch_fx_edi_cur(\n",
    "    currencies: Iterable[str],\n",
    "    start: str = \"1990-01-01\",\n",
    "    end: str | None = None,\n",
    "    *,\n",
    "    cache_dir: Path = RAW_DIR / \"nasdaq\",\n",
    "    force: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    end_ts = pd.Timestamp(end) if end else pd.Timestamp.today().normalize()\n",
    "    start_ts = pd.Timestamp(start).normalize()\n",
    "\n",
    "    out_path = cache_dir / f\"EDI_CUR_{start_ts.date()}_{end_ts.date()}_{'-'.join(currencies)}.parquet\"\n",
    "    if out_path.exists() and not force:\n",
    "        return pd.read_parquet(out_path)\n",
    "\n",
    "    meta = nasdaq_datatable_metadata(\"EDI/CUR\")\n",
    "    date_col, ccy_col, val_col = _guess_fx_columns(meta)\n",
    "\n",
    "    frames: list[pd.DataFrame] = []\n",
    "    for ccy in currencies:\n",
    "        params = {\n",
    "            f\"{date_col}.gte\": str(start_ts.date()),\n",
    "            f\"{date_col}.lte\": str(end_ts.date()),\n",
    "            ccy_col: ccy,\n",
    "            \"qopts.columns\": \",\".join([date_col, ccy_col, val_col]),\n",
    "        }\n",
    "        df = nasdaq_datatable_fetch(\"EDI/CUR\", params=params)\n",
    "        if df.empty:\n",
    "            continue\n",
    "        df = df.rename(columns={date_col: \"date\", ccy_col: \"ccy\", val_col: \"value\"})\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"date\", \"value\"]).sort_values(\"date\")\n",
    "        frames.append(df)\n",
    "\n",
    "    out = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[\"date\", \"ccy\", \"value\"])\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    return out\n",
    "\n",
    "FX_CODES = [\"GBP\", \"TRY\", \"ZAR\", \"PKR\", \"BRL\", \"NGN\"]\n",
    "fx_long = fetch_fx_edi_cur(FX_CODES, start=\"1990-01-01\")\n",
    "\n",
    "fx_wide = (\n",
    "    fx_long.pivot_table(index=\"date\", columns=\"ccy\", values=\"value\", aggfunc=\"last\")\n",
    "           .sort_index()\n",
    ")\n",
    "\n",
    "# We keep both interpretations because EDI/CUR is USD-based but can be quoted either way by vendor.\n",
    "fx_ccy_per_usd = fx_wide.copy()\n",
    "fx_usd_per_ccy = 1.0 / fx_wide.replace(0, np.nan)\n",
    "\n",
    "fx_long.to_parquet(CLEAN_DIR / \"nasdaq_edi_cur_fx_long.parquet\", index=False)\n",
    "fx_ccy_per_usd.to_parquet(CLEAN_DIR / \"nasdaq_edi_cur_ccy_per_usd_wide.parquet\")\n",
    "fx_usd_per_ccy.to_parquet(CLEAN_DIR / \"nasdaq_edi_cur_usd_per_ccy_wide.parquet\")\n",
    "\n",
    "print(\"Saved FX files to:\", CLEAN_DIR)\n",
    "display(fx_long.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18f64605",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Missing Emerging Mkt YC file. Download it from the class site and place it in ./data_raw/ (or set EMERGING_MKT_YC_URL in .env to a direct-download URL).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 226\u001b[39m\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported file type for Emerging Mkt YC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    225\u001b[39m \u001b[38;5;66;03m# Load + save\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m em_yc = \u001b[43mload_emerging_mkt_yc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    227\u001b[39m em_yc.to_parquet(CLEAN_DIR / \u001b[33m\"\u001b[39m\u001b[33memerging_mkt_swap_curves.parquet\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    229\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSaved:\u001b[39m\u001b[33m\"\u001b[39m, CLEAN_DIR / \u001b[33m\"\u001b[39m\u001b[33memerging_mkt_swap_curves.parquet\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrows:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(em_yc))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mload_emerging_mkt_yc\u001b[39m\u001b[34m(force_download)\u001b[39m\n\u001b[32m    162\u001b[39m     path = _find_emerging_yc_file()\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMissing Emerging Mkt YC file. Download it from the class site and place it in ./data_raw/ \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(or set EMERGING_MKT_YC_URL in .env to a direct-download URL).\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    170\u001b[39m suf = path.suffix.lower()\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suf \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m}:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Missing Emerging Mkt YC file. Download it from the class site and place it in ./data_raw/ (or set EMERGING_MKT_YC_URL in .env to a direct-download URL)."
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Emerging market swap curves (\"Emerging Mkt YC\" file from class website)\n",
    "#\n",
    "# Expected output: tidy long dataframe with columns:\n",
    "#   date, ccy, tenor_years, swap_rate\n",
    "# where swap_rate is in DECIMAL (e.g., 0.12 for 12%).\n",
    "#\n",
    "# How to provide the file:\n",
    "#   (A) Place it in ./data_raw/ (recommended), OR\n",
    "#   (B) Set EMERGING_MKT_YC_URL in .env to a direct-download URL.\n",
    "# -----------------------------\n",
    "EM_CCY = [\"TRY\", \"ZAR\", \"PKR\", \"BRL\", \"NGN\"]\n",
    "\n",
    "def _find_emerging_yc_file() -> Path | None:\n",
    "    pats = [\n",
    "        \"*Emerging*Mkt*YC*.*\",\n",
    "        \"*Emerging*YC*.*\",\n",
    "        \"*Emerging*Market*YC*.*\",\n",
    "        \"*Mkt*YC*.*\",\n",
    "    ]\n",
    "    search_dirs = [RAW_DIR, NOTEBOOK_DIR, NOTEBOOK_DIR.parent]\n",
    "    for d in search_dirs:\n",
    "        for pat in pats:\n",
    "            hits = sorted(d.glob(pat))\n",
    "            if hits:\n",
    "                return hits[0]\n",
    "    return None\n",
    "\n",
    "def _flatten_columns(cols) -> list[str]:\n",
    "    if isinstance(cols, pd.MultiIndex):\n",
    "        flat = []\n",
    "        for a, b in cols.to_list():\n",
    "            a = \"\" if a is None else str(a).strip()\n",
    "            b = \"\" if b is None else str(b).strip()\n",
    "            if a and b:\n",
    "                flat.append(f\"{a}_{b}\")\n",
    "            else:\n",
    "                flat.append(a or b)\n",
    "        return flat\n",
    "    return [str(c).strip() for c in cols]\n",
    "\n",
    "_CCY_TEN_RE = re.compile(\n",
    "    r\"^(?P<ccy>[A-Z]{3})[\\s_\\-]*(?P<ten>\\d+(?:\\.\\d+)?)(?P<unit>Y|YR|YEARS?|M|MO|MONTHS?)$\",\n",
    "    re.I\n",
    ")\n",
    "_TEN_CCY_RE = re.compile(\n",
    "    r\"^(?P<ten>\\d+(?:\\.\\d+)?)(?P<unit>Y|YR|YEARS?|M|MO|MONTHS?)[\\s_\\-]*(?P<ccy>[A-Z]{3})$\",\n",
    "    re.I\n",
    ")\n",
    "\n",
    "def _tenor_years_from(ten: float, unit: str) -> float:\n",
    "    u = unit.upper()\n",
    "    if u in {\"Y\", \"YR\", \"YEAR\", \"YEARS\"}:\n",
    "        return float(ten)\n",
    "    if u in {\"M\", \"MO\", \"MONTH\", \"MONTHS\"}:\n",
    "        return float(ten) / 12.0\n",
    "    raise ValueError(f\"Unsupported tenor unit: {unit}\")\n",
    "\n",
    "def _coerce_rate_to_decimal(s: pd.Series) -> pd.Series:\n",
    "    x = pd.to_numeric(s, errors=\"coerce\")\n",
    "    med = float(np.nanmedian(x.values)) if np.isfinite(x).any() else np.nan\n",
    "    # Heuristic: swap rates in emerging mkts are rarely < 1% in percent units; percent series typically > 1.\n",
    "    if np.isfinite(med) and med > 1.5:\n",
    "        return x / 100.0\n",
    "    return x\n",
    "\n",
    "def _tidy_from_wide(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = _flatten_columns(df.columns)\n",
    "\n",
    "    # locate date column\n",
    "    date_col = next((c for c in df.columns if c.lower() == \"date\"), None)\n",
    "    if date_col is None:\n",
    "        # common alternative: first column is date-like\n",
    "        c0 = df.columns[0]\n",
    "        maybe_date = pd.to_datetime(df[c0], errors=\"coerce\", dayfirst=True)\n",
    "        if maybe_date.notna().mean() >= 0.8:\n",
    "            date_col = c0\n",
    "        else:\n",
    "            raise RuntimeError(\"Could not locate date column in Emerging Mkt YC file.\")\n",
    "\n",
    "    df = df.rename(columns={date_col: \"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", dayfirst=True)\n",
    "    df = df.dropna(subset=[\"date\"]).sort_values(\"date\")\n",
    "\n",
    "    records = []\n",
    "    for c in df.columns:\n",
    "        if c == \"date\":\n",
    "            continue\n",
    "        col = str(c).strip()\n",
    "        m = _CCY_TEN_RE.match(col)\n",
    "        if not m:\n",
    "            m = _TEN_CCY_RE.match(col)\n",
    "        if not m:\n",
    "            continue\n",
    "\n",
    "        ccy = m.group(\"ccy\").upper()\n",
    "        ten = float(m.group(\"ten\"))\n",
    "        unit = m.group(\"unit\")\n",
    "        ty = _tenor_years_from(ten, unit)\n",
    "\n",
    "        if ccy not in EM_CCY:\n",
    "            continue\n",
    "\n",
    "        tmp = pd.DataFrame({\"date\": df[\"date\"].values, \"swap_rate\": _coerce_rate_to_decimal(df[c])})\n",
    "        tmp[\"ccy\"] = ccy\n",
    "        tmp[\"tenor_years\"] = ty\n",
    "        records.append(tmp)\n",
    "\n",
    "    if not records:\n",
    "        raise RuntimeError(\"Could not parse any {CCY,Tenor} columns from Emerging Mkt YC wide layout.\")\n",
    "\n",
    "    out = pd.concat(records, ignore_index=True)\n",
    "    out = out.dropna(subset=[\"swap_rate\"])\n",
    "    out = out[[\"date\", \"ccy\", \"tenor_years\", \"swap_rate\"]].sort_values([\"ccy\",\"date\",\"tenor_years\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def _tidy_from_long(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "    lcols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    date_col = lcols.get(\"date\") or next((c for c in df.columns if \"date\" in c.lower()), None)\n",
    "    ccy_col = lcols.get(\"ccy\") or lcols.get(\"currency\") or lcols.get(\"code\")\n",
    "    ten_col = lcols.get(\"tenor\") or lcols.get(\"maturity\") or lcols.get(\"term\")\n",
    "    rate_col = lcols.get(\"swap_rate\") or lcols.get(\"rate\") or lcols.get(\"value\")\n",
    "\n",
    "    if not (date_col and ccy_col and ten_col and rate_col):\n",
    "        raise RuntimeError(\"Emerging Mkt YC long layout not recognized (need date/ccy/tenor/rate).\")\n",
    "\n",
    "    out = df[[date_col, ccy_col, ten_col, rate_col]].rename(\n",
    "        columns={date_col: \"date\", ccy_col: \"ccy\", ten_col: \"tenor\", rate_col: \"swap_rate\"}\n",
    "    )\n",
    "    out[\"date\"] = pd.to_datetime(out[\"date\"], errors=\"coerce\", dayfirst=True)\n",
    "    out[\"ccy\"] = out[\"ccy\"].astype(str).str.upper().str.strip()\n",
    "\n",
    "    # tenor can be numeric years or labels like '5Y'\n",
    "    def _parse_ten(x: object) -> float | None:\n",
    "        if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "            return None\n",
    "        s = str(x).strip().upper()\n",
    "        if re.fullmatch(r\"\\d+(?:\\.\\d+)?\", s):\n",
    "            return float(s)\n",
    "        m = re.match(r\"^(\\d+(?:\\.\\d+)?)(Y|YR|YEARS?|M|MO|MONTHS?)$\", s)\n",
    "        if not m:\n",
    "            return None\n",
    "        return _tenor_years_from(float(m.group(1)), m.group(2))\n",
    "\n",
    "    out[\"tenor_years\"] = out[\"tenor\"].map(_parse_ten)\n",
    "    out[\"swap_rate\"] = _coerce_rate_to_decimal(out[\"swap_rate\"])\n",
    "    out = out.dropna(subset=[\"date\", \"ccy\", \"tenor_years\", \"swap_rate\"])\n",
    "    out = out[out[\"ccy\"].isin(EM_CCY)]\n",
    "    return out[[\"date\",\"ccy\",\"tenor_years\",\"swap_rate\"]].sort_values([\"ccy\",\"date\",\"tenor_years\"]).reset_index(drop=True)\n",
    "\n",
    "def load_emerging_mkt_yc(*, force_download: bool = False) -> pd.DataFrame:\n",
    "    url = os.getenv(\"EMERGING_MKT_YC_URL\")\n",
    "    if url:\n",
    "        dst = RAW_DIR / \"Emerging_Mkt_YC.xlsx\"\n",
    "        _download(url, dst, force=force_download)\n",
    "        path = dst\n",
    "    else:\n",
    "        path = _find_emerging_yc_file()\n",
    "\n",
    "    if path is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"Missing Emerging Mkt YC file. Download it from the class site and place it in ./data_raw/ \"\n",
    "            \"(or set EMERGING_MKT_YC_URL in .env to a direct-download URL).\"\n",
    "        )\n",
    "\n",
    "    suf = path.suffix.lower()\n",
    "    if suf in {\".parquet\"}:\n",
    "        df = pd.read_parquet(path)\n",
    "        # try both long and wide tidiers\n",
    "        try:\n",
    "            return _tidy_from_long(df)\n",
    "        except Exception:\n",
    "            return _tidy_from_wide(df)\n",
    "\n",
    "    if suf in {\".csv\", \".txt\"}:\n",
    "        df = pd.read_csv(path)\n",
    "        try:\n",
    "            return _tidy_from_long(df)\n",
    "        except Exception:\n",
    "            return _tidy_from_wide(df)\n",
    "\n",
    "    if suf in {\".xlsx\", \".xls\"}:\n",
    "        # first try multiindex header (common in financial spreadsheets)\n",
    "        try:\n",
    "            df2 = pd.read_excel(path, header=[0,1])\n",
    "            df2.columns = _flatten_columns(df2.columns)\n",
    "            try:\n",
    "                return _tidy_from_wide(df2)\n",
    "            except Exception:\n",
    "                pass\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # fall back: try each sheet\n",
    "        sheets = pd.read_excel(path, sheet_name=None, header=None)\n",
    "        for sh, mat in sheets.items():\n",
    "            # try to find a header row containing date and currency/tenor columns\n",
    "            # heuristic: scan first 50 rows for 'date'\n",
    "            hdr = None\n",
    "            for i in range(min(len(mat), 60)):\n",
    "                row = mat.iloc[i].astype(str).str.strip().str.lower()\n",
    "                if (row == \"date\").any():\n",
    "                    hdr = i\n",
    "                    break\n",
    "            if hdr is None:\n",
    "                continue\n",
    "            df = pd.read_excel(path, sheet_name=sh, header=hdr)\n",
    "            df.columns = _flatten_columns(df.columns)\n",
    "            try:\n",
    "                return _tidy_from_wide(df)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    return _tidy_from_long(df)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        raise RuntimeError(f\"Could not parse Emerging Mkt YC from: {path}\")\n",
    "\n",
    "    raise RuntimeError(f\"Unsupported file type for Emerging Mkt YC: {path}\")\n",
    "\n",
    "# Load + save\n",
    "em_yc = load_emerging_mkt_yc()\n",
    "em_yc.to_parquet(CLEAN_DIR / \"emerging_mkt_swap_curves.parquet\", index=False)\n",
    "\n",
    "print(\"Saved:\", CLEAN_DIR / \"emerging_mkt_swap_curves.parquet\", \"rows:\", len(em_yc))\n",
    "display(em_yc.head())\n",
    "\n",
    "# quick requirement check: do we have 1Y and 5Y in each currency?\n",
    "req_tenors = {1.0, 5.0}\n",
    "chk = (\n",
    "    em_yc.assign(tenor_round=em_yc[\"tenor_years\"].round(6))\n",
    "         .groupby(\"ccy\")[\"tenor_round\"]\n",
    "         .apply(lambda s: req_tenors.issubset(set(s.unique())))\n",
    ")\n",
    "print(\"Has both 1Y and 5Y per currency?\")\n",
    "display(chk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b4ebd9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in data_clean/:\n",
      " - boe_IUDSOIA_sonia.parquet\n",
      " - boe_ois_daily_archive_raw.parquet\n",
      " - gbpusd_usd_per_ccy.parquet\n",
      " - nasdaq_edi_cur_ccy_per_usd_wide.parquet\n",
      " - nasdaq_edi_cur_fx_long.parquet\n",
      " - nasdaq_edi_cur_usd_per_ccy_wide.parquet\n",
      " - usdgbp_ccy_per_usd.parquet\n",
      "\n",
      "Coverage checks:\n",
      "SONIA: 1997-01-02 → 2026-02-12 n= 7357\n",
      "FX: 2000-01-01 → 2026-02-16 n= 56369\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'em_yc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSONIA:\u001b[39m\u001b[33m\"\u001b[39m, sonia.index.min().date(), \u001b[33m\"\u001b[39m\u001b[33m→\u001b[39m\u001b[33m\"\u001b[39m, sonia.index.max().date(), \u001b[33m\"\u001b[39m\u001b[33mn=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(sonia))\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFX:\u001b[39m\u001b[33m\"\u001b[39m, fx_long[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].min().date(), \u001b[33m\"\u001b[39m\u001b[33m→\u001b[39m\u001b[33m\"\u001b[39m, fx_long[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].max().date(), \u001b[33m\"\u001b[39m\u001b[33mn=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(fx_long))\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEM YC:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mem_yc\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].min().date(), \u001b[33m\"\u001b[39m\u001b[33m→\u001b[39m\u001b[33m\"\u001b[39m, em_yc[\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m].max().date(), \u001b[33m\"\u001b[39m\u001b[33mn=\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(em_yc))\n",
      "\u001b[31mNameError\u001b[39m: name 'em_yc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Final: write required artifacts + sanity checks\n",
    "# -----------------------------\n",
    "sonia.to_parquet(CLEAN_DIR / \"boe_IUDSOIA_sonia.parquet\")\n",
    "\n",
    "# Also save a simple USD-vs-GBP series for convenience\n",
    "if \"GBP\" in fx_ccy_per_usd.columns:\n",
    "    usdgbp = fx_ccy_per_usd[[\"GBP\"]].rename(columns={\"GBP\": \"GBP_per_USD\"})\n",
    "    gbpusd = fx_usd_per_ccy[[\"GBP\"]].rename(columns={\"GBP\": \"USD_per_GBP\"})\n",
    "    usdgbp.to_parquet(CLEAN_DIR / \"usdgbp_ccy_per_usd.parquet\")\n",
    "    gbpusd.to_parquet(CLEAN_DIR / \"gbpusd_usd_per_ccy.parquet\")\n",
    "\n",
    "print(\"Files in data_clean/:\")\n",
    "for p in sorted(CLEAN_DIR.glob(\"*.parquet\")):\n",
    "    print(\" -\", p.name)\n",
    "\n",
    "print(\"\\nCoverage checks:\")\n",
    "print(\"SONIA:\", sonia.index.min().date(), \"→\", sonia.index.max().date(), \"n=\", len(sonia))\n",
    "print(\"FX:\", fx_long[\"date\"].min().date(), \"→\", fx_long[\"date\"].max().date(), \"n=\", len(fx_long))\n",
    "print(\"EM YC:\", em_yc[\"date\"].min().date(), \"→\", em_yc[\"date\"].max().date(), \"n=\", len(em_yc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
